person queri expans for the web paul - alexandru chirita l3s research center∗ appelstr. 9a 30167 hannov, germani chirita@l3s.de claudiu s. firan l3s research center appelstr. 9a 30167 hannov, germani firan@l3s.de wolfgang nejdl l3s research center appelstr. 9a 30167 hannov, germani nejdl@l3s.de abstract the inher ambigu of short keyword queri demand for  enhanc method for web retriev. in thi paper we propos to improv such web queri by expand them with term collect from each user"s person inform repositori, thu implicitli person the search output. we introduc five broad  techniqu for gener the addit queri keyword by analyz user data at increas granular level, rang from term and compound level analysi up to global co-occurr statist, as well as to us extern thesauri. our extens empir  analysi under four differ scenario show some of these approach to perform veri well, especi on ambigu queri, produc a veri strong increas in the qualiti of the output rank.  subsequ, we move thi person search framework on step further and propos to make the expans process adapt to  variou featur of each queri. a separ set of experi indic the adapt algorithm to bring an addit statist  signific improv over the best static expans approach. categori and subject descriptor h.3.3 [inform storag and retriev]: inform search and retriev; h.3.5 [inform storag and retriev]:  onlin inform servic-web-base servic gener term algorithm, experiment, measur 1. introduct the boom popular of search engin ha determin  simpl keyword search to becom the onli wide accept user  interfac for seek inform over the web. yet keyword queri ar inher ambigu. the queri canon book for exampl cover sever differ area of interest: religion, photographi, literatur, and music. clearli, on would prefer search output to be align with user"s topic(s) of interest, rather than displai a select of popular url from each categori. studi have shown that more than 80% of the user would prefer to receiv such person search result [33] instead of the current gener on. queri expans assist the user in formul a better queri, by append addit keyword to the initi search request in order to encapsul her interest therein, as well as to focu the web search output accordingli. it ha been shown to perform veri well over larg data set, especi with short input queri (see for exampl [19, 3]). thi is exactli the web search scenario! in thi paper we propos to enhanc web queri reformul by exploit the user"s person inform repositori (pir), i.e., the person collect of text document, email, cach web page, etc. sever advantag aris when move web search  person down to the desktop level (note that by desktop we refer to pir, and we us the two term interchang). first is of cours the qualiti of person: the local desktop is a rich repositori of inform, accur describ most, if not all  interest of the user. second, as all profil inform is store and exploit local, on the person machin, anoth veri import benefit is privaci. search engin should not be abl to know about a person"s interest, i.e., thei should not be abl to connect a  specif person with the queri she issu, or wors, with the output url she click within the search interfac1 (see volokh [35] for a discuss on privaci issu relat to person web search). our algorithm expand web queri with keyword extract from user"s pir, thu implicitli person the search output. after a discuss of previou work in section 2, we first  investig the analysi of local desktop queri context in section 3.1.1. we propos sever keyword, express, and summari base  techniqu for determin expans term from those person  document match the web queri best. in section 3.1.2 we move our analysi to the global desktop collect and investig expans base on co-occurr metric and extern thesauri. the  experi present in section 3.2 show mani of these approach to perform veri well, especi on ambigu queri, produc ndcg [15] improv of up to 51.28%. in section 4 we move thi algorithm framework further and propos to make the  expans process adapt to the clariti level of the queri. thi yield an addit improv of 8.47% over the previous identifi best algorithm. we conclud and discuss further work in section 5. 1 search engin can map queri at least to ip address, for exampl by us cooki and mine the queri log. howev, by move the user profil at the desktop level we ensur such inform is not explicitli associ to a particular user and store on the search engin side. 2. previou work thi paper bring togeth two ir area: search person and automat queri expans. there exist a vast amount of algorithm for both domain. howev, not much ha been done specif aim at combin them. in thi section we thu present a separ analysi, first introduc some approach to person search, as thi repres the main goal of our research, and then discuss sever queri expans techniqu and their relationship to our algorithm. 2.1 person search person search compris two major compon: (1) user profil, and (2) the actual search algorithm. thi section split the relev background accord to the focu of each articl into either on of these element. approach focus on the user profil. sugiyama et al. [32] analyz surf behavior and gener user profil as featur (term) of the visit page. upon issu a new queri, the search result were rank base on the similar between each url and the user profil. qiu and cho [26] us machin learn on the past click histori of the user in order to determin topic prefer vector and then appli topic-sensit pagerank [13]. user  profil base on brows histori ha the advantag of be rather easi to obtain and process. thi is probabl why it is also emploi by sever industri search engin (e.g., yahoo! myweb2 ).  howev, it is definit not suffici for gather a thorough insight into user"s interest. more, it requir to store all person  inform at the server side, which rais signific privaci concern. onli two other approach enhanc web search us desktop data, yet both us differ core idea: (1) teevan et al. [34]  modifi the queri term weight from the bm25 weight scheme to incorpor user interest as captur by their desktop index; (2) in chirita et al. [6], we focus on re-rank the web search  output accord to the cosin distanc between each url and a set of desktop term describ user"s interest. moreov, none of these investig the adapt applic of person. approach focus on the person algorithm.  effect build the person aspect directli into  pagerank [25] (i.e., by bias it on a target set of page) ha  receiv much attent recent. haveliwala [13] comput a  topicori pagerank, in which 16 pagerank vector bias on each of the main topic of the open directori were initi calcul off-line, and then combin at run-time base on the similar  between the user queri and each of the 16 topic. more recent, nie et al. [24] modifi the idea by distribut the pagerank of a page across the topic it contain in order to gener topic  orient rank. jeh and widom [16] propos an algorithm that avoid the massiv resourc need for store on person pagerank vector (ppv) per user by precomput ppv onli for a small set of page and then appli linear combin. as the comput of ppv for larger set of page wa still quit  expens, sever solut have been investig, the most import on be those of fogara and racz [12], and sarlo et al. [30], the latter us round and count-min sketch in order to fastli obtain accur enough approxim of the person score. 2.2 automat queri expans automat queri expans aim at deriv a better formul of the user queri in order to enhanc retriev. it is base on  exploit variou social or collect specif characterist in order to gener addit term, which ar append to the origin  in2 http://myweb2.search.yahoo.com put keyword befor identifi the match document return as output. in thi section we survei some of the repres queri expans work group accord to the sourc emploi to gener addit term: (1) relev feedback, (2)  collect base co-occurr statist, and (3) thesauru inform. some other approach ar also address in the end of the section. relev feedback techniqu. the main idea of relev feedback (rf) is that us inform can be extract from the relev document return for the initi queri. first approach were manual [28] in the sens that the user wa the on choos the relev result, and then variou method were appli to  extract new term, relat to the queri and the select document. efthimiadi [11] present a comprehens literatur review and propos sever simpl method to extract such new keyword base on term frequenc, document frequenc, etc. we us some of these as inspir for our desktop specif techniqu. chang and hsu [5] ask user to choos relev cluster, instead of  document, thu reduc the amount of interact necessari. rf ha also been shown to be effect automat by consid the top rank document as relev [37] (thi is known as pseudo rf). lam and jone [21] us summar to extract  inform sentenc from the top-rank document, and append them to the user queri. carpineto et al. [4] maxim the diverg  between the languag model defin by the top retriev document and that defin by the entir collect. final, yu et al. [38] select the expans term from vision-base segment of web page in order to cope with the multipl topic resid therein. co-occurr base techniqu. term highli co-occur with the issu keyword have been shown to increas precis when append to the queri [17]. mani statist measur have been develop to best assess term relationship level, either  analyz entir document [27], lexic affin relationship [3] (i.e., pair of close relat word which contain exactli on of the  initi queri term), etc. we have also investig three such  approach in order to identifi queri relev keyword from the rich, yet rather complex person inform repositori. thesauru base techniqu. a broadli explor method is to expand the user queri with new term, whose mean is close relat to the input keyword. such relationship ar usual  extract from larg scale thesauri, as wordnet [23], in which  variou set of synonym, hypernym, etc. ar predefin. just as for the co-occurr method, initi experi with thi approach were controversi, either report improv, or even  reduct in output qualiti [36]. recent, as the experiment  collect grew larger, and as the emploi algorithm becam more complex, better result have been obtain [31, 18, 22]. we also us wordnet base expans term. howev, we base thi  process on analyz the desktop level relationship between the  origin queri and the propos new keyword. other techniqu. there ar mani other attempt to extract expans term. though orthogon to our approach, two work ar veri relev for the web environ: cui et al. [8]  gener word correl util the probabl for queri term to appear in each document, as comput over the search engin log. kraft and zien [19] show that anchor text is veri similar to user queri, and thu exploit it to acquir addit keyword. 3. queri expans us desktop data desktop data repres a veri rich repositori of profil  inform. howev, thi inform come in a veri  unstructur wai, cover document which ar highli divers in  format, content, and even languag characterist. in thi section we first tackl thi problem by propos sever lexic analysi  algorithm which exploit user"s pir to extract keyword expans term at variou granular, rang from term frequenc within  desktop document up to util global co-occurr statist over the person inform repositori. then, in the second part of the section we empir analyz the perform of each approach. 3.1 algorithm thi section present the five gener approach for  analyz user"s desktop data in order to provid expans term for web search. in the propos algorithm we gradual increas the amount of person inform util. thu, in the first part we investig three local analysi techniqu focus onli on those desktop document match user"s queri best. we append to the web queri the most relev term, compound, and sentenc  summari from these document. in the second part of the section we move toward a global desktop analysi, propos to investig term co-occurr, as well as thesauri, in the expans process. 3.1.1 expand with local desktop analysi local desktop analysi is relat to enhanc pseudo  relev feedback to gener queri expans keyword from the pir best hit for user"s web queri, rather than from the top rank web search result. we distinguish three granular level for thi process and we investig each of them separ. term and document frequenc. as the simplest possibl  measur, tf and df have the advantag of be veri fast to comput. previou experi with small data set have show them to yield veri good result [11]. we thu independ associ a score with each term, base on each of the two statist. the tf base on is obtain by multipli the actual frequenc of a term with a posit score descend as the term first appear closer to the end of the document. thi is necessari especi for longer document, becaus more inform term tend to appear toward their begin [10]. the complet tf base keyword extract formula is as follow: termscor = 1 2 + 1 2 · nrword − po nrword ! · log(1 + tf) (1) where nrword is the total number of term in the document and po is the posit of the first appear of the term; tf  repres the frequenc of each term in the desktop document match user"s web queri. the identif of suitabl expans term is even simpler when us df: given the set of top-k relev desktop  document, gener their snippet as focus on the origin search request. thi queri orient is necessari, sinc the df score ar comput at the level of the entir pir and would produc too noisi suggest otherwis. onc the set of candid term ha been identifi, the select proce by order them accord to the df score thei ar associ with. ti ar resolv us the correspond tf score. note that a hybrid tfxidf approach is not necessarili effici, sinc on desktop term might have a high df on the desktop, while be quit rare in the web. for exampl, the term  pagerank would be quit frequent on the desktop of an ir scientist, thu achiev a low score with tfxidf. howev, as it is rather rare in the web, it would make a good resolut of the queri  toward the correct topic. lexic compound. anick and tipirneni [2] defin the  lexic dispers hypothesi, accord to which an express"s  lexic dispers (i.e., the number of differ compound it appear in within a document or group of document) can be us to  automat identifi kei concept over the input document set. although sever possibl compound express ar avail, it ha been shown that simpl approach base on noun analysi ar almost as good as highli complex part-of-speech pattern identif  algorithm [1]. we thu inspect the match desktop document for all their lexic compound of the follow form: { adject? noun+ } all such compound could be easili gener off-line, at index time, for all the document in the local repositori. moreov, onc identifi, thei can be further sort depend on their dispers within each document in order to facilit fast retriev of the most frequent compound at run-time. sentenc select. thi techniqu build upon sentenc  orient document summar: first, the set of relev desktop document is identifi; then, a summari contain their most import sentenc is gener as output. sentenc select is the most comprehens local analysi approach, as it produc the most detail expans (i.e., sentenc). it downsid is that,  unlik with the first two algorithm, it output cannot be store  effici, and consequ it cannot be comput off-line. we  gener sentenc base summari by rank the document sentenc accord to their salienc score, as follow [21]: sentencescor = sw2 tw + ps + tq2 nq the first term is the ratio between the squar amount of signific word within the sentenc and the total number of word therein. a word is signific in a document if it frequenc is abov a  threshold as follow: tf > ms = v ` x 7 − 0.1 ∗ (25 − ns) , if ns < 25 7 , if ns ∈ [25, 40] 7 + 0.1 ∗ (ns − 40) , if ns > 40 with ns be the total number of sentenc in the document (see [21] for detail). the second term is a posit score set to (avg(ns) − sentenceindex)/avg2 (ns) for the first ten  sentenc, and to 0 otherwis, avg(ns) be the averag number of sentenc over all desktop item. thi wai, short document such as email ar not affect, which is correct, sinc thei usual do not contain a summari in the veri begin. howev, as longer document usual do includ overal descript sentenc in the begin [10], these sentenc ar more like to be relev. the final term bias the summari toward the queri. it is the ratio  between the squar number of queri term present in the sentenc and the total number of term from the queri. it is base on the belief that the more queri term contain in a sentenc, the more like will that sentenc convei inform highli relat to the queri. 3.1.2 expand with global desktop analysi in contrast to the previous present approach, global analysi reli on inform from across the entir person desktop to infer the new relev queri term. in thi section we propos two such techniqu, name term co-occurr statist, and filter the output of an extern thesauru. term co-occurr statist. for each term, we can easili comput off-line those term co-occur with it most frequent in a given collect (i.e., pir in our case), and then exploit thi inform at run-time in order to infer keyword highli  correl with the user queri. our gener co-occurr base queri expans algorithm is as follow: algorithm 3.1.2.1. co-occurr base keyword similar search. off-line comput: 1: filter potenti keyword k with df ∈ [10, . . . , 20% · n] 2: for each keyword ki 3: for each keyword kj 4: comput scki,kj , the similar coeffici of (ki, kj) on-line comput: 1: let s be the set of keyword, potenti similar to an input express e. 2: for each keyword k of e: 3: s ← s ∪ tsc(k), where tsc(k) contain the top-k term most similar to k 4: for each term t of s: 5a: let score(t) ← q k∈e(0.01 + sct,k) 5b: let score(t) ← #desktophit(e|t) 6: select top-k term of s with the highest score. the off-line comput need an initi trim phase (step 1) for optim purpos. in addit, we also restrict the  algorithm to comput co-occurr level across noun onli, as thei contain by far the largest amount of conceptu inform, and as thi approach reduc the size of the co-occurr matrix consider. dure the run-time phase, have the term most correl with each particular queri keyword alreadi identifi, on more oper is necessari, name calcul the correl of everi output term with the entir queri. two approach ar possibl: (1) us a product of the correl between the term and all keyword in the origin express (step 5a), or (2)  simpli count the number of document in which the propos term co-occur with the entir user queri (step 5b). we consid the follow formula for similar coeffici [17]: • cosin similar, defin as: cs = dfx,y pdfx · dfy (2) • mutual inform, defin as: mi = log n · dfx,y dfx · dfy (3) • likelihood ratio, defin in the paragraph below. dfx is the document frequenc of term x, and dfx,y is the  number of document contain both x and y. to further increas the qualiti of the gener score we limit the latter indic to  cooccurr within a window of w term. we set w to be the same as the maximum amount of expans keyword desir. dun"s likelihood ratio λ [9] is a co-occurr base  metric similar to χ2 . it start by attempt to reject the null  hypothesi, accord to which two term a and b would appear in text independ from each other. thi mean that p(a b) = p(a¬b) = p(a), where p(a¬b) is the probabl that term a is not follow by term b. consequ, the test for independ of a and b can be perform by look if the distribut of a given that b is present is the same as the distribut of a given that b is not present. of cours, in realiti we know these term ar not independ in text, and we onli us the statist metric to highlight term which ar frequent appear togeth. we  compar the two binomi process by us likelihood ratio of their associ hypothes. first, let us defin the likelihood ratio for on hypothesi: λ = maxω∈Ω0 h(ω; k) maxω∈Ω h(ω; k) (4) where ω is a point in the paramet space Ω, Ω0 is the particular hypothesi be test, and k is a point in the space of observ k. if we assum that two binomi distribut have the same underli paramet, i.e., {(p1, p2) | p1 = p2}, we can write: λ = maxp h(p, p; k1, k2, n1, n2) maxp1,p2 h(p1, p2; k1, k2, n1, n2) (5) where h(p1, p2; k1, k2, n1, n2) = pk1 1 · (1 − p1)(n1−k1) ·  n1 k1 ¡ · pk2 2 · (1 − p2)(n2−k2) ·  n2 k2 ¡ . sinc the maxima ar obtain with p1 = k1 n1 , p2 = k2 n2 , and p = k1+k2 n1+n2 , we have: λ = maxp l(p, k1, n1)l(p, k2, n2) maxp1,p2 l(p1, k1, n1)l(p2, k2, n2) (6) where l(p, k, n) = pk · (1 − p)n−k . take the logarithm of the likelihood, we obtain: −2 · log λ = 2 · [log l(p1, k1, n1) + log l(p2, k2, n2) − log l(p, k1, n1) − log l(p, k2, n2)] where log l(p, k, n) = k · log p + (n − k) · log(1 − p). final, if we write o11 = p(a b), o12 = p(¬a b), o21 = p(a ¬b), and o22 = p(¬a¬b), then the co-occurr likelihood of term a and b becom: −2 · log λ = 2 · [o11 · log p1 + o12 · log (1 − p1) + o21 · log p2 + o22 · log (1 − p2) − (o11 + o21) · log p − (o12 + o22) · log (1 − p)] where p1 = k1 n1 = o11 o11+o12 , p2 = k2 n2 = o21 o21+o22 , and p = k1+k2 n1+n2 thesauru base expans. larg scale thesauri encapsul global knowledg about term relationship. thu, we first identifi the set of term close relat to each queri keyword, and then we calcul the desktop co-occurr level of each of these  possibl expans term with the entir initi search request. in the end, those suggest with the highest frequenc ar kept. the algorithm is as follow: algorithm 3.1.2.2. filter thesauru base queri expans. 1: for each keyword k of an input queri q: 2: select the follow set of relat term us wordnet: 2a: syn: all synonym 2b: sub: all sub-concept resid on level below k 2c: super: all super-concept resid on level abov k 3: for each set si of the abov mention set: 4: for each term t of si: 5: search the pir with (q|t), i.e., the origin queri, as expand with t 6: let h be the number of hit of the abov search (i.e., the co-occur level of t with q) 7: return top-k term as order by their h valu. we observ three type of term relationship (step 2a-2c): (1) synonym, (2) sub-concept, name hyponym (i.e., sub-class) and meronym (i.e., sub-part), and (3) super-concept, name hypernym (i.e., super-class) and holonym (i.e., super-part). as thei repres quit differ type of associ, we  investig them separ. we limit the output expans set (step 7) to contain onli term appear at least t time on the desktop, in order to avoid noisi suggest, with t = min( n docspertop , mindoc). we set docspertop = 2, 500, and mindoc = 5, the latter on cope with the case of small pir. 3.2 experi 3.2.1 experiment setup we evalu our algorithm with 18 subject (ph.d. and  postdoc. student in differ area of comput scienc and  educ). first, thei instal our lucen base search engin3 and 3 clearli, if on had alreadi instal a desktop search applic, then thi overhead would not be present. index all their local store content: file within user select path, email, and web cach. without loss of gener, we  focus the experi on singl-user machin. then, thei chose 4 queri relat to their everydai activ, as follow: • on veri frequent altavista queri, as extract from the top 2% queri most issu to the search engin within a 7.2  million entri log from octob 2001. in order to connect such a queri to each user"s interest, we ad an off-line  preprocess phase: we gener the most frequent search  request and then randomli select a queri with at least 10 hit on each subject"s desktop. to further ensur a real life scenario, user were allow to reject the propos queri and ask for a new on, if thei consid it total outsid their interest area. • on randomli select log queri, filter us the same  procedur as abov. • on self-select specif queri, which thei thought to have onli on mean. • on self-select ambigu queri, which thei thought to have at least three mean. the averag queri length were 2.0 and 2.3 term for the log queri, as well as 2.9 and 1.8 for the self-select on. even though our algorithm ar mainli intend to enhanc search when us ambigu queri keyword, we chose to investig their  perform on a wide span of queri type, in order to see how thei perform in all situat. the log queri evalu real life request, in contrast to the self-select on, which target rather the  identif of top and bottom perform. note that the former on were somewhat farther awai from each subject"s interest, thu  be also more difficult to person on. to gain an insight into the relationship between each queri type and user interest, we ask each person to rate the queri itself with a score of 1 to 5, have the follow interpret: (1) never heard of it, (2) do not know it, but heard of it, (3) know it partial, (4) know it well, (5) major  interest. the obtain grade were 3.11 for the top log queri, 3.72 for the randomli select on, 4.45 for the self-select specif on, and 4.39 for the self-select ambigu on. for each queri, we collect the top-5 url gener by 20 version of the algorithm4 present in section 3.1. these result were then shuffl into on set contain usual between 70 and 90 url. thu, each subject had to assess about 325 document for all four queri, be neither awar of the algorithm, nor of the rank of each assess url. overal, 72 queri were issu and over 6,000 url were evalu dure the experi. for each of these url, the tester had to give a rate rang from 0 to 2, divid the relev result in two categori, (1) relev and (2) highli relev. final, the qualiti of each rank wa assess us the normal version of discount cumul gain (dcg) [15]. dcg is a rich measur, as it give more weight to highli rank document, while also incorpor differ  relev level by give them differ gain valu: dcg(i) = & g(1) , if i = 1 dcg(i − 1) + g(i)/log(i) , otherwis. we us g(i) = 1 for relev result, and g(i) = 2 for highli  relev on. as queri have more relev output document will have a higher dcg, we also normal it valu to a score between 0 (the worst possibl dcg given the rate) and 1 (the best  possibl dcg given the rate) to facilit averag over queri. all result were test for statist signific us t-test. 4 note that all desktop level part of our algorithm were perform with lucen us it predefin search and rank function. algorithm specif aspect. the main paramet of our  algorithm is the number of gener expans keyword. for thi experi we set it to 4 term for all techniqu, leav an  analysi at thi level for a subsequ investig. in order to optim the run-time comput speed, we chose to limit the number of output keyword per desktop document to the number of  expans keyword desir (i.e., four). for all algorithm we also  investig bigger limit. thi allow us to observ that the lexic compound method would perform better if onli at most on compound per document were select. we therefor chose to experi with thi new approach as well. for all other  techniqu, consid less than four term per document did not seem to consist yield ani addit qualit gain. we label the algorithm we evalu as follow: 0. googl: the actual googl queri output, as return by the googl api; 1. tf, df: term and document frequenc; 2. lc, lc[o]: regular and optim (by consid onli on top compound per document) lexic compound; 3. ss: sentenc select; 4. tc[cs], tc[mi], tc[lr]: term co-occurr statist us respect cosin similar, mutual inform, and likelihood ratio as similar coeffici; 5. wn[syn], wn[sub], wn[sup]: wordnet base  expans with synonym, sub-concept, and super-concept,  respect. except for the thesauru base expans, in all case we also  investig the perform of our algorithm when exploit onli the web browser cach to repres user"s person inform. thi is motiv by the fact that other person document such as for exampl email ar known to have a somewhat differ languag than that resid on the world wide web [34]. howev, as thi approach perform visibl poorer than us the entir desktop data, we omit it from the subsequ analysi. 3.2.2 result log queri. we evalu all variant of our algorithm us ndcg. for log queri, the best perform wa achiev with tf, lc[o], and tc[lr]. the improv thei brought were up to 5.2% for top queri (p = 0.14) and 13.8% for randomli  select queri (p = 0.01, statist signific), both obtain with lc[o]. a summari of all result is depict in tabl 1. both tf and lc[o] yield veri good result, indic that simpl keyword and express orient approach might be  suffici for the desktop base queri expans task. lc[o] wa much better than lc, amelior it qualiti with up to 25.8% in the case of randomli select log queri, improv which wa also  signific with p = 0.04. thu, a select of compound span over sever desktop document is more inform about user"s interest than the gener approach, in which there is no restrict on the number of compound produc from everi person item. the more complex desktop orient approach, name  sentenc select and all term co-occurr base algorithm, show a rather averag perform, with no visibl  improv, except for tc[lr]. also, the thesauru base expans usual produc veri few suggest, possibl becaus of the mani technic queri emploi by our subject. we observ howev that expand with sub-concept is veri good for  everydai life term (e.g., car), wherea the us of super-concept is valuabl for compound have at least on term with low  technic (e.g., document cluster). as expect, the synonym base expans perform gener well, though in some veri algorithm ndcg signif. ndcg signif. top vs. googl random vs. googl googl 0.42 -  0.40tf 0.43 p = 0.32 0.43 p = 0.04 df 0.17 -  0.23lc 0.39 -  0.36lc[o] 0.44 p = 0.14 0.45 p = 0.01 ss 0.33 -  0.36tc[cs] 0.37 -  0.35tc[mi] 0.40 -  0.36tc[lr] 0.41 - 0.42 p = 0.06 wn[syn] 0.42 -  0.38wn[sub] 0.28 -  0.33wn[sup] 0.26 -  0.26tabl 1: normal discount cumul gain at the first 5 result when search for top (left) and random (right) log queri. algorithm ndcg signif. ndcg signif. clear vs. googl ambigu vs. googl googl 0.71 -  0.39tf 0.66 - 0.52 p 0.01 df 0.37 -  0.31lc 0.65 - 0.54 p 0.01 lc[o] 0.69 - 0.59 p 0.01 ss 0.56 - 0.52 p 0.01 tc[cs] 0.60 - 0.50 p = 0.01 tc[mi] 0.60 - 0.47 p = 0.02 tc[lr] 0.56 - 0.47 p = 0.03 wn[syn] 0.70 -  0.36wn[sub] 0.46 -  0.32wn[sup] 0.51 -  0.29tabl 2: normal discount cumul gain at the first 5 result when search for user select clear (left) and  ambigu (right) queri. technic case it yield rather gener suggest. final, we notic googl to be veri optim for some top frequent queri. howev, even within thi harder scenario, some of our  person algorithm produc statist signific improv over regular search (i.e., tf and lc[o]). self-select queri. the ndcg valu obtain with  selfselect queri ar depict in tabl 2. while our algorithm did not enhanc googl for the clear search task, thei did produc strong improv of up to 52.9% (which were of cours also highli signific with p 0.01) when util with ambigu queri. in fact, almost all our algorithm result in statist signific improv over googl for thi queri type. in gener, the rel differ between our algorithm were similar to those observ for the log base queri. as in the  previou analysi, the simpl desktop base term frequenc and  lexic compound metric perform best. nevertheless, a veri good outcom wa also obtain for desktop base sentenc select and all term co-occurr metric. there were no visibl  differ between the behavior of the three differ approach to  cooccurr calcul. final, for the case of clear queri, we notic that fewer expans term than 4 might be less noisi and thu help in bring further improv. we thu pursu thi idea with the adapt algorithm present in the next section. 4. introduc adapt in the previou section we have investig the behavior of each techniqu when ad a fix number of keyword to the user queri. howev, an optim person queri expans  algorithm should automat adapt itself to variou aspect of each queri, as well as to the particular of the person us it. in thi section we discuss the factor influenc the behavior of our  expans algorithm, which might be us as input for the adapt process. then, in the second part we present some initi  experi with on of them, name queri clariti. 4.1 adapt factor sever indic could assist the algorithm to automat tune the number of expans term. we start by discuss  adapt by analyz the queri clariti level. then, we briefli introduc an approach to model the gener queri formul process in  order to tailor the search algorithm automat, and discuss some other possibl factor that might be of us for thi task. queri clariti. the interest for analyz queri difficulti ha increas onli recent, and there ar not mani paper address thi topic. yet it ha been long known that queri disambigu ha a high potenti of improv retriev effect for low recal search with veri short queri [20], which is exactli our target scenario. also, the success of ir system clearli vari across differ topic. we thu propos to us an estim number express the calcul level of queri clariti in order to  automat tweak the amount of person fed into the algorithm. the follow metric ar avail: • the queri length is express simpli by the number of word in the user queri. the solut is rather ineffici, as report by he and ouni [14]. • the queri scope relat to the idf of the entir queri, as in: c1 = log( #documentsincollect #hit(queri) ) (7) thi metric perform well when us with document  collect cover a singl topic, but poor otherwis [7, 14]. • the queri clariti [7] seem to be the best, as well as the most appli techniqu so far. it measur the diverg between the languag model associ to the user queri and the languag model associ to the collect. in a  simplifi version (i.e., without smooth over the term which ar not present in the queri), it can be express as follow: c2 =  w∈queri pml(w|queri) · log pml(w|queri) pcoll(w) (8) where pml(w|queri) is the probabl of the word w within the submit queri, and pcoll(w) is the probabl of w within the entir collect of document. other solut exist, but we think thei ar too computation expens for the huge amount of data that need to be process within web applic. we thu decid to investig onli c1 and c2. first, we analyz their perform over a larg set of queri and split their clariti predict in three categori: • small scope / clear queri: c1 ∈ [0, 12], c2 ∈ [4, ∞). • medium scope / semi-ambigu queri: c1 ∈ [12, 17), c2 ∈ [2.5, 4). • larg scope / ambigu queri: c1 ∈ [17, ∞), c2 ∈ [0, 2.5]. in order to limit the amount of experi, we analyz onli the result produc when emploi c1 for the pir and c2 for the web. as algorithm basi we us lc[o], i.e., optim lexic compound, which wa clearli the win method in the previou analysi. as manual investig show it to slightli overfit the expans term for clear queri, we util a substitut for thi particular case. two candid were consid: (1) tf, i.e., the second best approach, and (2) wn[syn], as we observ that it first and second expans term were often veri good. desktop scope web clariti no. of term algorithm larg ambigu 4 lc[o] larg semi-ambig. 3 lc[o] larg clear 2 lc[o] medium ambigu 3 lc[o] medium semi-ambig. 2 lc[o] medium clear 1 tf / wn[syn] small ambigu 2 tf / wn[syn] small semi-ambig. 1 tf / wn[syn] small clear  0tabl 3: adapt person queri expans. given the algorithm and clariti measur, we implement the adapt procedur by tailor the amount of expans term ad to the origin queri, as a function of it ambigu in the web, as well as within user"s pir. note that the ambigu level is relat to the number of document cover a certain queri. thu, to some extent, it ha differ mean on the web and within pir. while a queri deem ambigu on a larg collect such as the web will veri like inde have a larg number of mean, thi mai not be the case for the desktop. take for exampl the queri pagerank. if the user is a link analysi expert, mani of her document might match thi term, and thu the queri would be classifi as ambigu. howev, when analyz against the web, thi is definit a clear queri. consequ, we emploi more addit term, when the queri wa more ambigu in the web, but also on the desktop. put anoth wai, queri deem clear on the desktop were inher not well cover within user"s pir, and thu had fewer keyword append to them. the number of expans term we util for each combin of scope and clariti level is depict in tabl 3. queri formul process. interact queri expans ha a high potenti for enhanc search [29]. we believ that model it underli process would be veri help in produc  qualit adapt web search algorithm. for exampl, when the user is ad a new term to her previous issu queri, she is basic reformul her origin request. thu, the newli ad term ar more like to convei inform about her search goal. for a gener, non person retriev engin, thi could correspond to give more weight to these new keyword. within our  person scenario, the gener expans can similarli be bias toward these term. nevertheless, more investig ar  necessari in order to solv the challeng pose by thi approach. other featur. the idea of adapt the retriev process to variou aspect of the queri, of the user itself, and even of the  emploi algorithm ha receiv onli littl attent in the literatur. onli some approach have been investig, usual indirectli. there exist studi of queri behavior at differ time of dai, or of the topic span by the queri of variou class of user, etc. howev, thei gener do not discuss how these featur can be actual incorpor in the search process itself and thei have almost never been relat to the task of web person. 4.2 experi we us exactli the same experiment setup as for our  previou analysi, with two log-base queri and two self-select on (all differ from befor, in order to make sure there is no bia on the new approach), evalu with ndcg over the top-5  result output by each algorithm. the newli propos adapt  person queri expans algorithm ar denot as a[lco/tf] for the approach us tf with the clear desktop queri, and as a[lco/wn] when wn[syn] wa util instead of tf. the overal result were at least similar, or better than googl for all kind of log queri (see tabl 4). for top frequent queri, algorithm ndcg signif. ndcg signif. top vs. googl random vs. googl googl 0.51 -  0.45tf 0.51 - 0.48 p = 0.04 lc[o] 0.53 p = 0.09 0.52 p < 0.01 wn[syn] 0.51 -  0.45a[lco/tf] 0.56 p < 0.01 0.49 p = 0.04 a[lco/wn] 0.55 p = 0.01  0.44tabl 4: normal discount cumul gain at the first 5 result when us our adapt person search algorithm on top (left) and random (right) log queri. algorithm ndcg signif. ndcg signif. clear vs. googl ambigu vs. googl googl 0.81 -  0.46tf 0.76 - 0.54 p = 0.03 lc[o] 0.77 - 0.59 p 0.01 wn[syn] 0.79 -  0.44a[lco/tf] 0.81 - 0.64 p 0.01 a[lco/wn] 0.81 - 0.63 p 0.01 tabl 5: normal discount cumul gain at the first 5 result when us our adapt person search algorithm on user select clear (left) and ambigu (right) queri. both adapt algorithm, a[lco/tf] and a[lco/wn], improv with 10.8% and 7.9% respect, both differ be also  statist signific with p ≤ 0.01. thei also achiev an  improv of up to 6.62% over the best perform static  algorithm, lc[o] (p = 0.07). for randomli select queri, even though a[lco/tf] yield significantli better result than googl (p = 0.04), both adapt approach fall behind the static  algorithm. the major reason seem to be the imperfect select of the number of expans term, as a function of queri clariti. thu, more experi ar need in order to determin the  optim number of gener expans keyword, as a function of the queri ambigu level. the analysi of the self-select queri show that adapt can bring even further improv into web search  person (see tabl 5). for ambigu queri, the score given to googl search ar enhanc by 40.6% through a[lco/tf] and by 35.2% through a[lco/wn], both strongli signific with p 0.01. adapt also bring anoth 8.9% improv over the static person of lc[o] (p = 0.05). even for clear queri, the newli propos flexibl algorithm perform slightli better, improv with 0.4% and 1.0% respect. all result ar depict graphic in figur 1. we notic that a[lco/tf] is the overal best algorithm, perform better than googl for all type of queri, either extract from the search  engin log, or self-select. the experi present in thi section confirm clearli that adapt is a necessari further step to take in web search person. 5. conclus and further work in thi paper we propos to expand web search queri by  exploit the user"s person inform repositori in order to  automat extract addit keyword relat both to the queri itself and to user"s interest, person the search output. in thi context, the paper includ the follow contribut: • we propos five techniqu for determin expans term from person document. each of them produc  addit queri keyword by analyz user"s desktop at  increas granular level, rang from term and express level analysi up to global co-occurr statist and  extern thesauri. figur 1: rel ndcg gain (in %) for each algorithm  overal, as well as separ per queri categori. • we provid a thorough empir analysi of sever  variant of our approach, under four differ scenario. we show some of these approach to perform veri well,  produc ndcg improv of up to 51.28%. • we move thi person search framework further and propos to make the expans process adapt to featur of each queri, a strong focu be put on it clariti level. • within a separ set of experi, we show our adapt algorithm to provid an addit improv of 8.47% over the previous identifi best approach. we ar current perform investig on the depend between variou queri featur and the optim number of  expans term. we ar also analyz other type of approach to identifi queri expans suggest, such as appli latent  semant analysi on the desktop data. final, we ar design a set of more complex combin of these metric in order to provid enhanc adapt to our algorithm. 6. acknowledg we thank ricardo baeza-yate, vassili plachoura, carlo castillo and vanessa murdock from yahoo! for the interest discuss about the experiment setup and the algorithm we present. we ar grate to fabrizio silvestri from cnr and to ronni lempel from ibm for provid us the altavista queri log. final, we thank our colleagu from l3s for particip in the time consum experi we perform, as well as to the  european commiss for the fund support (project nepomuk, 6th framework programm, ist contract no. 027705). 7. refer [1] j. allan and h. raghavan. us part-of-speech pattern to reduc queri ambigu. in proc. of the 25th intl. acm sigir conf. on research and develop in inform retriev, 2002. [2] p. g. anick and s. tipirneni. the paraphras search assist: terminolog feedback for iter inform seek. in proc. of the 22nd intl. acm sigir conf. on research and develop in inform retriev, 1999. [3] d. carmel, e. farchi, y. petruschka, and a. soffer. automat queri wefin us lexic affin with maxim inform gain. in proc. of the 25th intl. acm sigir conf. on research and develop in inform retriev, page 283-290, 2002. [4] c. carpineto, r. de mori, g. romano, and b. bigi. an inform-theoret approach to automat queri expans. acm toi, 19(1):1-27, 2001. [5] c.-h. chang and c.-c. hsu. integr queri expans and conceptu relev feedback for person web inform retriev. in proc. of the 7th intl. conf. on world wide web, 1998. [6] p. a. chirita, c. firan, and w. nejdl. summar local context to person global web search. in proc. of the 15th intl. cikm conf. on inform and knowledg manag, 2006. [7] s. cronen-townsend, y. zhou, and w. b. croft. predict queri perform. in proc. of the 25th intl. acm sigir conf. on research and develop in inform retriev, 2002. [8] h. cui, j.-r. wen, j.-y. nie, and w.-y. ma. probabilist queri expans us queri log. in proc. of the 11th intl. conf. on world wide web, 2002. [9] t. dun. accur method for the statist of surpris and coincid. comput linguist, 19:61-74, 1993. [10] h. p. edmundson. new method in automat extract. journal of the acm, 16(2):264-285, 1969. [11] e. n. efthimiadi. user choic: a new yardstick for the evalu of rank algorithm for interact queri expans. inform process and manag, 31(4):605-620, 1995. [12] d. fogara and b. racz. scale link base similar search. in proc. of the 14th intl. world wide web conf., 2005. [13] t. haveliwala. topic-sensit pagerank. in proc. of the 11th intl. world wide web conf., honolulu, hawaii, mai 2002. [14] b. he and i. ouni. infer queri perform us pre-retriev predictor. in proc. of the 11th intl. spire conf. on string process and inform retriev, 2004. [15] k. j¨arvelin and j. keklinen. ir evalu method for retriev highli relev document. in proc. of the 23th intl. acm sigir conf. on research and develop in inform retriev, 2000. [16] g. jeh and j. widom. scale person web search. in proc. of the 12th intl. world wide web confer, 2003. [17] m.-c. kim and k.-s. choi. a comparison of colloc-base similar measur in queri expans. inf. proc. and mgmt., 35(1):19-30, 1999. [18] s.-b. kim, h.-c. seo, and h.-c. rim. inform retriev us word sens: root sens tag approach. in proc. of the 27th intl. acm sigir conf. on research and develop in inform retriev, 2004. [19] r. kraft and j. zien. mine anchor text for queri refin. in proc. of the 13th intl. conf. on world wide web, 2004. [20] r. krovetz and w. b. croft. lexic ambigu and inform retriev. acm tran. inf. syst., 10(2), 1992. [21] a. m. lam-adesina and g. j. f. jone. appli summar techniqu for term select in relev feedback. in proc. of the 24th intl. acm sigir conf. on research and develop in inform retriev, 2001. [22] s. liu, f. liu, c. yu, and w. meng. an effect approach to document retriev via util wordnet and recogn phrase. in proc. of the 27th intl. acm sigir conf. on research and develop in inform retriev, 2004. [23] g. miller. wordnet: an electron lexic databas. commun of the acm, 38(11):39-41, 1995. [24] l. nie, b. davison, and x. qi. topic link analysi for web search. in proc. of the 29th intl. acm sigir conf. on re. and develop in inf. retr., 2006. [25] l. page, s. brin, r. motwani, and t. winograd. the pagerank citat rank: bring order to the web. technic report, stanford univ., 1998. [26] f. qiu and j. cho. automat indentif of user interest for person search. in proc. of the 15th intl. www conf., 2006. [27] y. qiu and h.-p. frei. concept base queri expans. in proc. of the 16th intl. acm sigir conf. on research and develop in inf. retr., 1993. [28] j. rocchio. relev feedback in inform retriev. the smart retriev system: experi in automat document process, page 313-323, 1971. [29] i. ruthven. re-examin the potenti effect of interact queri expans. in proc. of the 26th intl. acm sigir conf., 2003. [30] t. sarlo, a. a. benczur, k. csalogani, d. fogara, and b. racz. to random or not to random: space optim summari for hyperlink analysi. in proc. of the 15th intl. www conf., 2006. [31] c. shah and w. b. croft. evalu high accuraci retriev techniqu. in proc. of the 27th intl. acm sigir conf. on research and develop in inform retriev, page 2-9, 2004. [32] k. sugiyama, k. hatano, and m. yoshikawa. adapt web search base on user profil construct without ani effort from user. in proc. of the 13th intl. world wide web conf., 2004. [33] d. sullivan. the older you ar, the more you want person search, 2004. http://searchenginewatch.com/searchdai/articl.php/3385131. [34] j. teevan, s. dumai, and e. horvitz. person search via autom analysi of interest and activ. in proc. of the 28th intl. acm sigir conf. on research and develop in inform retriev, 2005. [35] e. volokh. person and privaci. commun. acm, 43(8), 2000. [36] e. m. voorhe. queri expans us lexic-semant relat. in proc. of the 17th intl. acm sigir conf. on re. and develop in inf. retr., 1994. [37] j. xu and w. b. croft. queri expans us local and global document analysi. in proc. of the 19th intl. acm sigir conf. on research and develop in inform retriev, 1996. [38] s. yu, d. cai, j.-r. wen, and w.-y. ma. improv pseudo-relev feedback in web inform retriev us web page segment. in proc. of the 12th intl. conf. on world wide web, 2003. 