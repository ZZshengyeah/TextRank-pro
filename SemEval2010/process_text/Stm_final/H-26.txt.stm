a support vector method for optim averag precis yisong yue cornel univers ithaca, ny, usa yyue@cs.cornel.edu thoma finlei cornel univers ithaca, ny, usa tomf@cs.cornel.edu filip radlinski cornel univers ithaca, ny, usa filip@cs.cornel.edu thorsten joachim cornel univers ithaca, ny, usa tj@cs.cornel.edu abstract machin learn is commonli us to improv rank  retriev system. due to comput difficulti, few  learn techniqu have been develop to directli optim for mean averag precis (map), despit it widespread us in evalu such system. exist approach  optim map either do not find a global optim solut, or ar computation expens. in contrast, we present a gener svm learn algorithm that effici find a global optim solut to a straightforward relax of map. we evalu our approach us the trec 9 and trec 10 web track corpora (wt10g), compar against svm optim for accuraci and rocarea. in most case we show our method to produc statist signific  improv in map score. categori and subject descriptor h.3.3 [inform search and retriev]: retriev model gener term algorithm, theori, experiment 1. introduct state of the art inform retriev system commonli us machin learn techniqu to learn rank function. howev, most current approach do not optim for the evalu measur most often us, name mean averag precis (map). instead, current algorithm tend to take on of two  gener approach. the first approach is to learn a model that estim the probabl of a document be relev given a queri (e.g., [18, 14]). if solv effect, the rank with best map perform can easili be deriv from the  probabl of relev. howev, achiev high map onli requir find a good order of the document. as a  result, find good probabl requir solv a more  difficult problem than necessari, like requir more train data to achiev the same map perform. the second common approach is to learn a function that maxim a surrog measur. perform measur  optim includ accuraci [17, 15], rocarea [1, 5, 10, 11, 13, 21] or modif of rocarea [4], and ndcg [2, 3]. learn a model to optim for such measur might result in suboptim map perform. in fact, although some previou system have obtain good map perform, it is known that neither achiev optim accuraci nor  rocarea can guarante optim map perform[7]. in thi paper, we present a gener approach for learn rank function that maxim map perform.  specif, we present an svm algorithm that global optim a hing-loss relax of map. thi approach simplifi the process of obtain rank function with high map perform by avoid addit intermedi step and heurist. the new algorithm also make it conceptu just as easi to optim svm for map as wa previous possibl onli for accuraci and rocarea. in contrast to recent work directli optim for map perform by metzler & croft [16] and caruana et al. [6], our techniqu is computation effici while find a global optim solut. like [6, 16], our method learn a linear model, but is much more effici in practic and, unlik [16], can handl mani thousand of featur. we now describ the algorithm in detail and provid proof of correct. follow thi, we provid an analysi of  run time. we finish with empir result from experi on the trec 9 and trec 10 web track corpu. we have also develop a softwar packag implement our  algorithm that is avail for public us1 . 2. the learn problem follow the standard machin learn setup, our goal is to learn a function h : x → y between an input space x (all possibl queri) and output space y (rank over a corpu). in order to quantifi the qualiti of a predict, ˆy = h(x), we will consid a loss function ∆ : y × y → . ∆(y, ˆy) quantifi the penalti for make predict ˆy if the correct output is y. the loss function allow us to  incorpor specif perform measur, which we will exploit 1 http://svmrank.yisongyu.com for optim map. we restrict ourselv to the supervis learn scenario, where input/output pair (x, y) ar  avail for train and ar assum to come from some fix distribut p(x, y). the goal is to find a function h such that the risk (i.e., expect loss), r∆ p (h) = z x×y ∆(y, h(x))dp(x, y), is minim. of cours, p(x, y) is unknown. but given a finit set of train pair, s = {(xi, yi) ∈ x × y : i = 1, . . . , n}, the perform of h on s can be measur by the empir risk, r∆ s (h) = 1 n nx i=1 ∆(yi, h(xi)). in the case of learn a rank retriev function, x  denot a space of queri, and y the space of (possibl weak) rank over some corpu of document c = {d1, . . . ,d|c|}. we can defin averag precis loss as ∆map(y, ˆy) = 1 − map(rank(y), rank(ˆy)), where rank(y) is a vector of the rank valu of each  document in c. for exampl, for a corpu of two document, {d1, d2}, with d1 have higher rank than d2, rank(y ) = (1, 0). we assum true rank have two rank valu, where relev document have rank valu 1 and non-relev  document rank valu 0. we further assum that all predict rank ar complet rank (no ti). let p = rank(y) and ˆp = rank(ˆy). the averag precis score is defin as map(p, ˆp) = 1 rel x j:pj =1 prec@j, where rel = |{i : pi = 1}| is the number of relev  document, and prec@j is the percentag of relev document in the top j document in predict rank ˆy. map is the mean of the averag precis score of a group of queri. 2.1 map vs rocarea most learn algorithm optim for accuraci or  rocarea. while optim for these measur might achiev good map perform, we us two simpl exampl to show it can also be suboptim in term of map. rocarea assign equal penalti to each misord of a relev/non-relev pair. in contrast, map assign greater penalti to misord higher up in the predict rank. us our notat, rocarea can be defin as roc(p, ˆp) = 1 rel · (|c| − rel) x i:pi=1 x j:pj =0 1[ˆpi>ˆpj ], where p is the true (weak) rank, ˆp is the predict  rank, and 1[b] is the indic function condit on b. doc id 1 2 3 4 5 6 7 8 p 1 0 0 0 0 1 1 0 rank(h1(x)) 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 tabl 1: toi exampl and model suppos we have a hypothesi space with onli two  hypothesi function, h1 and h2, as shown in tabl 1. these two hypothes predict a rank for queri x over a corpu of eight document. hypothesi map rocarea h1(x) 0.59 0.47 h2(x) 0.51 0.53 tabl 2: perform of toi model tabl 2 show the map and rocarea score of h1 and h2. here, a learn method which optim for  rocarea would choos h2 sinc that result in a higher  rocarea score, but thi yield a suboptim map score. 2.2 map vs accuraci us a veri similar exampl, we now demonstr how optim for accuraci might result in suboptim map. model which optim for accuraci ar not directli  concern with the rank. instead, thei learn a threshold such that document score higher than the threshold can be classifi as relev and document score lower as  nonrelev. doc id 1 2 3 4 5 6 7 8 9 10 11 p 1 0 0 0 0 1 1 1 1 0 0 rank(h1(x)) 11 10 9 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 9 10 11 tabl 3: toi exampl and model we consid again a hypothesi space with two  hypothes. tabl 3 show the predict of the two hypothes on a singl queri x. hypothesi map best acc. h1(q) 0.70 0.64 h2(q) 0.64 0.73 tabl 4: perform of toi model tabl 4 show the map and best accuraci score of h1(q) and h2(q). the best accuraci refer to the highest  achiev accuraci on that rank when consid all  possibl threshold. for instanc, with h1(q), a threshold  between document 1 and 2 give 4 error (document 6-9  incorrectli classifi as non-relev), yield an accuraci of 0.64. similarli, with h2(q), a threshold between document 5 and 6 give 3 error (document 10-11 incorrectli  classifi as relev, and document 1 as non-relev), yield an accuraci of 0.73. a learn method which optim for accuraci would choos h2 sinc that result in a higher accuraci score, but thi yield a suboptim map score. 3. optim averag precis we build upon the approach us by [13] for  optim rocarea. unlik rocarea, howev, map doe not decompos linearli in the exampl and requir a  substanti extend algorithm, which we describ in thi section. recal that the true rank is a weak rank with two rank valu (relev and non-relev). let cx and c¯x  denot the set of relev and non-relev document of c for queri x, respect. we focu on function which ar parametr by a weight vector w, and thu wish to find w to minim the empir risk, r∆ s (w) ≡ r∆ s (h(·; w)). our approach is to learn a discrimin function f : x × y → over input-output pair. given queri x, we can deriv a predict by find the rank y that maxim the discrimin function: h(x; w) = argmax y∈y f(x, y; w). (1) we assum f to be linear in some combin featur  represent of input and output Ψ(x, y) ∈ rn , i.e., f(x, y; w) = wt Ψ(x, y). (2) the combin featur function we us is Ψ(x, y) = 1 |cx| · |c¯x| x i:di∈cx x j:dj ∈c¯x [yij (φ(x, di) − φ(x, dj))] , where φ : x × c → n is a featur map function from a queri/document pair to a point in n dimension space2 . we repres rank as a matrix of pairwis order, y ⊂ {−1, 0, +1}|c|×|c| . for ani y ∈ y, yij = +1 if di is rank ahead of dj, and yij = −1 if dj is rank ahead of di, and yij = 0 if di and dj have equal rank. we consid onli matric which correspond to valid rank (i.e, obei antisymmetri and transit). intuit, Ψ is a  summat over the vector differ of all relev/non-relev document pair. sinc we assum predict rank to be complet rank, yij is either +1 or −1 (never 0). given a learn weight vector w, predict a rank (i.e. solv equat (1)) given queri x reduc to pick each yij to maxim wt Ψ(x, y). as is also discuss in [13], thi is attain by sort the document by wt φ(x, d) in descend order. we will discuss later the choic of φ we us for our experi. 3.1 structur svm the abov formul is veri similar to learn a  straightforward linear model while train on the pairwis  differ of relev/non-relev document pair. mani svm-base approach optim over these pairwis  differ (e.g., [5, 10, 13, 4]), although these method do not optim for map dure train. previous, it wa not clear how to incorpor non-linear multivari loss  function such as map loss directli into global optim problem such as svm train. we now present a method base on structur svm [19] to address thi problem. we us the structur svm formul, present in  optim problem 1, to learn a w ∈ rn . optim problem 1. (structur svm) min w,ξ≥0 1 2 w 2 + c n nx i=1 ξi (3) s.t. ∀i, ∀y ∈ y \ yi : wt Ψ(xi, yi) ≥ wt Ψ(xi, y) + ∆(yi, y) − ξi (4) the object function to be minim (3) is a tradeoff between model complex, w 2 , and a hing loss relax of map loss, p ξi. as is usual in svm train, c is a 2 for exampl, on dimens might be the number of time the queri word appear in the document. algorithm 1 cut plane algorithm for solv op 1 within toler . 1: input: (x1, y1), . . . , (xn, yn), c, 2: wi ← ∅ for all i = 1, . . . , n 3: repeat 4: for i = 1, . . . , n do 5: h(y; w) ≡ ∆(yi, y) + wt Ψ(xi, y) − wt Ψ(xi, yi) 6: comput ˆy = argmaxi∈y h(y; w) 7: comput ξi = max{0, maxi∈wi h(y; w)} 8: if h(ˆy; w) > ξi + then 9: wi ← wi ∪ {ˆy} 10: w ← optim (3) over w = s i wi 11: end if 12: end for 13: until no wi ha chang dure iter paramet that control thi tradeoff and can be tune to achiev good perform in differ train task. for each (xi, yi) in the train set, a set of constraint of the form in equat (4) is ad to the optim problem. note that wt Ψ(x, y) is exactli our discrimin function f(x, y; w) (see equat (2)). dure predict, our model choos the rank which maxim the  discrimin (1). if the discrimin valu for an incorrect rank y is greater than for the true rank yi (e.g., f(xi, y; w) > f(xi, yi; w)), then correspond slack variabl, ξi, must be at least ∆(yi, y) for that constraint to be satisfi.  therefor, the sum of slack, p ξi, upper bound the map loss. thi is state formal in proposit 1. proposit 1. let ξ∗ (w) be the optim solut of the slack variabl for op 1 for a given weight vector w. then 1 n pn i=1 ξi is an upper bound on the empir risk r∆ s (w). (see [19] for proof) proposit 1 show that op 1 learn a rank function that optim an upper bound on map error on the  train set. unfortun there is a problem: a constraint is requir for everi possibl wrong output y, and the  number of possibl wrong output is exponenti in the size of c. fortun, we mai emploi algorithm 1 to solv op 1. algorithm 1 is a cut plane algorithm, iter  introduc constraint until we have solv the origin problem within a desir toler [19]. the algorithm start with no constraint, and iter find for each exampl (xi, yi) the output ˆy associ with the most violat constraint. if the correspond constraint is violat by more than we introduc ˆy into the work set wi of activ constraint for exampl i, and re-solv (3) us the updat w. it can be shown that algorithm 1"s outer loop is guarante to halt within a polynomi number of iter for ani desir precis . theorem 1. let ¯r = maxi maxi Ψ(xi, yi) − Ψ(xi, y) , ¯∆ = maxi maxi ∆(yi, y), and for ani > 0, algorithm 1 termin after ad at most max  2n ¯∆ , 8c ¯∆ ¯r2 2 ff constraint to the work set w. (see [19] for proof) howev, within the inner loop of thi algorithm we have to comput argmaxi∈y h(y; w), where h(y; w) = ∆(yi, y) + wt Ψ(xi, y) − wt Ψ(xi, yi), or equival, argmax y∈y ∆(yi, y) + wt Ψ(xi, y), sinc wt Ψ(xi, yi) is constant with respect to y. though close relat to the classif procedur, thi ha the substanti complic that we must contend with the  addit ∆(yi, y) term. without the abil to effici find the most violat constraint (i.e., solv argmaxi∈y h(y, w)), the constraint gener procedur is not tractabl. 3.2 find the most violat constraint us op 1 and optim to rocarea loss (∆roc), the problem of find the most violat constraint, or solv argmaxi∈y h(y, w) (henceforth argmax h), is address in [13]. solv argmax h for ∆map is more difficult. thi is primarili becaus rocarea decompos nice into a sum of score comput independ on each rel  order of a relev/non-relev document pair. map, on the other hand, doe not decompos in the same wai as  rocarea. the main algorithm contribut of thi paper is an effici method for solv argmax h for ∆map. on us properti of ∆map is that it is invari to  swap two document with equal relev. for exampl, if document da and db ar both relev, then swap the posit of da and db in ani rank doe not affect ∆map. by extens, ∆map is invari to ani arbitrari  permut of the relev document amongst themselv and of the non-relev document amongst themselv. howev, thi reshuﬄing will affect the discrimin score, wt Ψ(x, y). thi lead us to observ 1. observ 1. consid rank which ar constrain by fix the relev at each posit in the rank (e.g., the 3rd document in the rank must be relev). everi rank which satisfi the same set of constraint will have the same ∆map. if the relev document ar sort by wt φ(x, d) in descend order, and the non-relev  document ar likewis sort by wt φ(x, d), then the  interleav of the two sort list which satisfi the constraint will maxim h for that constrain set of rank. observ 1 impli that in the rank which  maxim h, the relev document will be sort by wt φ(x, d), and the non-relev document will also be sort likewis. by first sort the relev and non-relev document, the problem is simplifi to find the optim interleav of two sort list. for the rest of our discuss, we assum that the relev document and non-relev document ar both sort by descend wt φ(x, d). for conveni, we also refer to relev document as {dx 1 , . . . dx |cx|} = cx , and non-relev document as {d¯x 1 , . . . d¯x |c¯x|} = c¯x . we defin δj(i1, i2), with i1 < i2, as the chang in h from when the highest rank relev document rank after d¯x j is dx i1 to when it is dx i2 . for i2 = i1 + 1, we have δj(i, i + 1) = 1 |cx| „ j j + i − j − 1 j + i − 1 « − 2 · (sx i − s¯x j ), (5) where si = wt φ(x, di). the first term in (5) is the chang in ∆map when the ith relev document ha j non-relev document rank befor it, as oppos to j −1. the second term is the chang in the discrimin score, wt Ψ(x, y), when yij chang from +1 to −1. . . . , dx i , d¯x j , dx i+1, . . . . . . , d¯x j , dx i , dx i+1, . . . figur 1: exampl for δj(i, i + 1) figur 1 give a conceptu exampl for δj(i, i + 1). the bottom rank differ from the top onli where d¯x j slide up on rank. the differ in the valu of h for these two rank is exactli δj(i, i + 1). for ani i1 < i2, we can then defin δj(i1, i2) as δj(i1, i2) = i2−1 x k=i1 δj(k, k + 1), (6) or equival, δj(i1, i2) = i2−1 x k=i1 » 1 |cx| „ j j + k − j − 1 j + k − 1 « − 2 · (sx k − s¯x j )  . let o1, . . . , o|c¯x| encod the posit of the non-relev document, where dx oj is the highest rank relev  document rank after the jth non-relev document. due to observ 1, thi encod uniqu identifi a complet rank. we can recov the rank as yij = 8 >>>< >>>: 0 if i = j sign(si − sj) if di, dj equal relev sign(oj − i − 0.5) if di = dx i , dj = d¯x j sign(j − oi + 0.5) if di = d¯x i , dj = dx j . (7) we can now reformul h into a new object function, h (o1, . . . , o|c¯x||w) = h(¯y|w) + |c¯x | x k=1 δk(ok, |cx | + 1), where ¯y is the true (weak) rank. conceptu h start with a perfect rank ¯y, and add the chang in h when each success non-relev document slide up the rank. we can then reformul the argmax h problem as argmax h = argmax o1,...,o|c¯x| |c¯x | x k=1 δk(ok, |cx | + 1) (8) s.t. o1 ≤ . . . ≤ o|c¯x|. (9) algorithm 2 describ the algorithm us to solv  equat (8). conceptu, algorithm 2 start with a perfect rank. then for each success non-relev document, the algorithm modifi the solut by slide that  document up the rank to local maxim h while keep the posit of the other non-relev document constant. 3.2.1 proof of correct algorithm 2 is greedi in the sens that it find the best posit of each non-relev document independ from the other non-relev document. in other word, the  algorithm maxim h for each non-relev document, d¯x j , algorithm 2 find the most violat constraint (argmax h) for algorithm 1 with ∆map 1: input: w, cx , c¯x 2: sort cx and c¯x in descend order of wt φ(x, d) 3: sx i ← wt φ(x, dx i ), i = 1, . . . , |cx | 4: s¯x i ← wt φ(x, d¯x i ), i = 1, . . . , |c¯x | 5: for j = 1, . . . , |c¯x | do 6: optj ← argmaxk δj(k, |cx | + 1) 7: end for 8: encod ˆy accord to (7) 9: return ˆy without consid the posit of the other non-relev document, and thu ignor the constraint of (9). in order for the solut to be feasibl, the jth non-relev document must be rank after the first j − 1 non-relev document, thu satisfi opt1 ≤ opt2 ≤ . . . ≤ opt|c¯x|. (10) if the solut is feasibl, the it clearli solv (8). therefor, it suffic to prove that algorithm 2 satisfi (10). we first prove that δj(·, ·) is monoton decreas in j. lemma 1. for ani 1 ≤ i1 < i2 ≤ |cx | + 1 and 1 ≤ j < |c¯x |, it must be the case that δj+1(i1, i2) ≤ δj(i1, i2). proof. recal from (6) that both δj(i1, i2) and δj+1(i1, i2) ar summat of i2 − i1 term. we will show that each term in the summat of δj+1(i1, i2) is no greater than the correspond term in δj(i1, i2), or δj+1(k, k + 1) ≤ δj(k, k + 1) for k = i1, . . . , i2 − 1. each term in δj(k, k +1) and δj+1(k, k +1) can be further decompos into two part (see (5)). we will show that each part of δj+1(k, k + 1) is no greater than the correspond part in δj(k, k + 1). in other word, we will show that both j + 1 j + k + 1 − j j + k ≤ j j + k − j − 1 j + k − 1 (11) and −2 · (sx k − s¯x j+1) ≤ −2 · (sx k − s¯x j ) (12) ar true for the aforement valu of j and k. it is easi to see that (11) is true by observ that for ani two posit integ 1 ≤ a < b, a + 1 b + 1 − a b ≤ a b − a − 1 b − 1 , and choos a = j and b = j + k. the second inequ (12) hold becaus algorithm 2 first sort d¯x in descend order of s¯x , impli s¯x j+1 ≤ s¯x j . thu we see that each term in δj+1 is no greater than the correspond term in δj, which complet the proof. the result of lemma 1 lead directli to our main  correct result: theorem 2. in algorithm 2, the comput valu of optj satisfi (10), impli that the solut return by algorithm 2 is feasibl and thu optim. proof. we will prove that optj ≤ optj+1 hold for ani 1 ≤ j < |c¯x |, thu impli (10). sinc algorithm 2 comput optj as optj = argmax k δj(k, |cx | + 1), (13) then by definit of δj (6), for ani 1 ≤ i < optj, δj(i, optj) = δj(i, |cx | + 1) − δj(optj, |cx | + 1) < 0. us lemma 1, we know that δj+1(i, optj) ≤ δj(i, optj) < 0, which impli that for ani 1 ≤ i < optj, δj+1(i, |cx | + 1) − δj+1(optj, |cx | + 1) < 0. suppos for contradict that optj+1 < optj. then δj+1(optj+1, |cx | + 1) < δj+1(optj, |cx | + 1), which contradict (13). therefor, it must be the case that optj ≤ optj+1, which complet the proof. 3.2.2 run time the run time of algorithm 2 can be split into two part. the first part is the sort by wt φ(x, d), which  requir o(n log n) time, where n = |cx | + |c¯x |. the second part comput each optj, which requir o(|cx | · |c¯x |) time. though in the worst case thi is o(n2 ), the number of  relev document, |cx |, is often veri small (e.g., constant with respect to n), in which case the run time for the second part is simpli o(n). for most real-world dataset, algorithm 2 is domin by the sort and ha complex o(n log n). algorithm 1 is guarante to halt in a polynomi  number of iter [19], and each iter run algorithm 2. virtual all well-perform model were train in a  reason amount of time (usual less than on hour). onc train is complet, make predict on queri x  us the result hypothesi h(x|w) requir onli sort by wt φ(x, d). we develop our softwar us a python interfac3 to svmstruct , sinc the python languag greatli simplifi the code process. to improv perform, it is advis to us the standard c implement4 of svmstruct . 4. experi setup the main goal of our experi is to evalu whether directli optim map lead to improv map  perform compar to convent svm method that  optim a substitut loss such as accuraci or rocarea. we empir evalu our method us two set of trec web track queri, on each from trec 9 and trec 10 (topic 451-500 and 501-550), both of which us the wt10g corpu. for each queri, trec provid the relev  judgment of the document. we gener our featur us the score of exist retriev function on these queri. while our method is agnost to the mean of the  featur, we chose to us exist retriev function as a simpl yet effect wai of acquir us featur. as such, our 3 http://www.cs.cornel.edu/~tomf/svmpython/ 4 http://svmlight.joachim.org/svm_struct.html dataset base func featur trec 9 indri 15 750 trec 10 indri 15 750 trec 9 submiss 53 2650 trec 10 submiss 18 900 tabl 5: dataset statist experi essenti test our method"s abil to re-rank the highli rank document (e.g., re-combin the score of the retriev function) to improv map. we compar our method against the best retriev  function train on (henceforth base function), as well as against previous propos svm method. compar with the best base function test our method"s abil to learn a  us combin. compar with previou svm method allow us to test whether optim directli for map (as oppos to accuraci or rocarea) achiev a higher map score in practic. the rest of thi section describ the base function and the featur gener method in detail. 4.1 choos retriev function we chose two set of base function for our experi. for the first set, we gener three indic over the wt10g corpu us indri5 . the first index wa gener us default set, the second us porter-stem, and the last us porter-stem and indri"s default stopword. for both trec 9 and trec 10, we us the  descript portion of each queri and score the document us five of indri"s built-in retriev method, which ar cosin similar, tfidf, okapi, languag model with dirichlet prior, and languag model with jelinek-mercer prior. all paramet were kept as their default. we comput the score of these five retriev method over the three indic, give 15 base function in total. for each queri, we consid the score of document found in the union of the top 1000 document of each base function. for our second set of base function, we us score from the trec 9 [8] and trec 10 [9] web track submiss. we us onli the non-manual, non-short submiss from both year. for trec 9 and trec 10, there were 53 and 18 such submiss, respect. a typic submiss  contain score of it top 1000 document. b ca wt φ(x,d) f(d|x) figur 2: exampl featur bin 4.2 gener featur in order to gener input exampl for our method, a concret instanti of φ must be provid. for each  doc5 http://www.lemurproject.org trec 9 trec 10 model map w/l map w/l svm∆ map 0.242 -  0.236best func. 0.204 39/11 ** 0.181 37/13 ** 2nd best 0.199 38/12 ** 0.174 43/7 ** 3rd best 0.188 34/16 ** 0.174 38/12 ** tabl 6: comparison with indri function ument d score by a set of retriev function f on queri x, we gener the featur as a vector φ(x, d) = 1[f(d|x)>k] : ∀f ∈ f, ∀k ∈ kf , where f(d|x) denot the score that retriev function f  assign to document d for queri x, and each kf is a set of real valu. from a high level, we ar express the score of each retriev function us |kf | + 1 bin. sinc we ar us linear kernel, on can think of the learn problem as find a good piecewis-constant  combin of the score of the retriev function. figur 2 show an exampl of our featur map method. in thi exampl we have a singl featur f = {f}. here, kf = {a, b, c}, and the weight vector is w = wa, wb, wc . for ani document d and queri x, we have wt φ(x, d) = 8 >>< >>: 0 if f(d|x) < a wa if a ≤ f(d|x) < b wa + wb if b ≤ f(d|x) < c wa + wb + wc if c ≤ f(d|x) . thi is express qualit in figur 2, where wa and wb ar posit, and wc is neg. we ran our main experi us four choic of f: the set of aforement indri retriev function for trec 9 and trec 10, and the web track submiss for trec 9 and trec 10. for each f and each function f ∈ f, we chose 50 valu for kf which ar reason space and captur the sensit region of f. us the four choic of f, we gener four dataset for our main experi. tabl 5 contain statist of the gener dataset. there ar mani wai to gener featur, and we ar not advoc our method over other. thi wa simpli an effici mean to normal the output of differ function and allow for a more express model. 5. experi for each dataset in tabl 5, we perform 50 trial. for each trial, we train on 10 randomli select queri, and  select anoth 5 queri at random for a valid set.  model were train us a wide rang of c valu. the model which perform best on the valid set wa select and test on the remain 35 queri. all queri were select to be in the train, valid and test set the same number of time. us thi setup, we perform the same experi while us our method (svm∆ map), an svm optim for rocarea (svm∆ roc) [13], and a convent classif svm (svmacc) [20]. all svm method us a linear kernel. we report the averag perform of all model over the 50 trial. 5.1 comparison with base function in analyz our result, the first question to answer is, can svm∆ map learn a model which outperform the best base trec 9 trec 10 model map w/l map w/l svm∆ map 0.290 -  0.287best func. 0.280 28/22 0.283 29/21 2nd best 0.269 30/20 0.251 36/14 ** 3rd best 0.266 30/20 0.233 36/14 ** tabl 7: comparison with trec submiss trec 9 trec 10 model map w/l map w/l svm∆ map 0.284 -  0.288best func. 0.280 27/23 0.283 31/19 2nd best 0.269 30/20 0.251 36/14 ** 3rd best 0.266 30/20 0.233 35/15 ** tabl 8: comparison with trec subm. (w/o best) function? tabl 6 present the comparison of svm∆ map with the best indri base function. each column group contain the macro-averag map perform of svm∆ map or a base function. the w/l column show the number of queri where svm∆ map achiev a higher map score. signific test were perform us the two-tail wilcoxon sign rank test. two star indic a signific level of 0.95. all tabl displai our experiment result ar structur ident. here, we find that svm∆ map significantli  outperform the best base function. tabl 7 show the comparison when train on trec  submiss. while achiev a higher map score than the best base function, the perform differ between svm∆ map the base function is not signific. given that mani of these submiss us score function which ar carefulli craft to achiev high map, it is possibl that the best perform submiss us techniqu which subsum the techniqu of the other submiss. as a result, svm∆ map would not be abl to learn a hypothesi which can  significantli out-perform the best submiss. henc, we ran the same experi us a modifi dataset where the featur comput us the best  submiss were remov. tabl 8 show the result (note that we ar still compar against the best submiss though we ar not us it for train). notic that while the  perform of svm∆ map degrad slightli, the perform wa still compar with that of the best submiss. 5.2 comparison w/ previou svm method the next question to answer is, doe svm∆ map produc higher map score than previou svm method? tabl 9 and 10 present the result of svm∆ map, svm∆ roc, and svmacc when train on the indri retriev function and trec  submiss, respect. tabl 11 contain the correspond result when train on the trec submiss without the best submiss. to start with, our result indic that svmacc wa not competit with svm∆ map and svm∆ roc, and at time  underperform dramat. as such, we tri sever  approach to improv the perform of svmacc. 5.2.1 altern svmacc method on issu which mai caus svmacc to underperform is the sever imbal between relev and non-relev  doctrec 9 trec 10 model map w/l map w/l svm∆ map 0.242 -  0.236svm∆ roc 0.237 29/21 0.234 24/26 svmacc 0.147 47/3 ** 0.155 47/3 ** svmacc2 0.219 39/11 ** 0.207 43/7 ** svmacc3 0.113 49/1 ** 0.153 45/5 ** svmacc4 0.155 48/2 ** 0.155 48/2 ** tabl 9: train on indri function trec 9 trec 10 model map w/l map w/l svm∆ map 0.290 -  0.287svm∆ roc 0.282 29/21 0.278 35/15 ** svmacc 0.213 49/1 ** 0.222 49/1 ** svmacc2 0.270 34/16 ** 0.261 42/8 ** svmacc3 0.133 50/0 ** 0.182 46/4 ** svmacc4 0.233 47/3 ** 0.238 46/4 ** tabl 10: train on trec submiss ument. the vast major of the document ar not  relev. svmacc2 address thi problem by assign more penalti to fals neg error. for each dataset, the ratio of the fals neg to fals posit penalti is equal to the ratio of the number non-relev and relev document in that dataset. tabl 9, 10 and 11 indic that svmacc2 still perform significantli wors than svm∆ map. anoth possibl issu is that svmacc attempt to find just on discrimin threshold b that is queri-invari. it mai be that differ queri requir differ valu of b. have the learn method try to find a good b valu (when on doe not exist) mai be detriment. we took two approach to address thi issu. the first method, svmacc3, convert the retriev function score into percentil. for exampl, for document d, queri q and  retriev function f, if the score f(d|q) is in the top 90% of the score f(·|q) for queri q, then the convert score is f (d|q) = 0.9. each kf contain 50 evenli space valu between 0 and 1. tabl 9, 10 and 11 show that the  perform of svmacc3 wa also not competit with svm∆ map. the second method, svmacc4, normal the score given by f for each queri. for exampl, assum for queri q that f output score in the rang 0.2 to 0.7. then for document d, if f(d|q) = 0.6, the convert score would be f (d|q) = (0.6 − 0.2)/(0.7 − 0.2) = 0.8. each kf contain 50 evenli space valu between 0 and 1. again, tabl 9, 10 and 11 show that svmacc4 wa not competit with svm∆ map 5.2.2 map vs rocarea svm∆ roc perform much better than svmacc in our  experi. when train on indri retriev function (see tabl 9), the perform of svm∆ roc wa slight, though not significantli, wors than the perform of svm∆ map. howev, tabl 10 show that svm∆ map did significantli  outperform svm∆ roc when train on the trec submiss. tabl 11 show the perform of the model when train on the trec submiss with the best submiss remov. the perform of most model degrad by a small amount, with svm∆ map still have the best perform. trec 9 trec 10 model map w/l map w/l svm∆ map 0.284 -  0.288svm∆ roc 0.274 31/19 ** 0.272 38/12 ** svmacc 0.215 49/1 ** 0.211 50/0 ** svmacc2 0.267 35/15 ** 0.258 44/6 ** svmacc3 0.133 50/0 ** 0.174 46/4 ** svmacc4 0.228 46/4 ** 0.234 45/5 ** tabl 11: train on trec subm. (w/o best) 6. conclus and futur work we have present an svm method that directli  optim map. it provid a principl approach and avoid difficult to control heurist. we formul the  optim problem and present an algorithm which provabl find the solut in polynomi time. we have shown  empir that our method is gener superior to or  competit with convent svm method. our new method make it conceptu just as easi to optim svm for map as wa previous possibl onli for accuraci and rocarea. the comput cost for train is veri reason in practic. sinc other method typic requir tune multipl heurist, we also expect to train fewer model befor find on which achiev good perform. the learn framework us by our method is fairli  gener. a natur extens of thi framework would be to develop method to optim for other import ir  measur, such as normal discount cumul gain [2, 3, 4, 12] and mean reciproc rank. 7. acknowledg thi work wa fund under nsf award ii-0412894, nsf career award 0237381, and a gift from yahoo!  research. the third author wa also partli support by a microsoft research fellowship. 8. refer [1] b. t. bartel, g. w. cottrel, and r. k. belew. automat combin of multipl rank retriev system. in proceed of the acm confer on research and develop in inform retriev (sigir), 1994. [2] c. burg, t. shake, e. renshaw, a. lazier, m. deed, n. hamilton, and g. hullend. learn to rank us gradient descent. in proceed of the intern confer on machin learn (icml), 2005. [3] c. j. c. burg, r. ragno, and q. le. learn to rank with non-smooth cost function. in proceed of the intern confer on advanc in neural inform process system (nip), 2006. [4] y. cao, j. xu, t.-y. liu, h. li, y. huang, and h.-w. hon. adapt rank svm to document retriev. in proceed of the acm confer on research and develop in inform retriev (sigir), 2006. [5] b. carterett and d. petkova. learn a rank from pairwis prefer. in proceed of the acm confer on research and develop in inform retriev (sigir), 2006. [6] r. caruana, a. niculescu-mizil, g. crew, and a. ksike. ensembl select from librari of model. in proceed of the intern confer on machin learn (icml), 2004. [7] j. davi and m. goadrich. the relationship between precis-recal and roc curv. in proceed of the intern confer on machin learn (icml), 2006. [8] d. hawk. overview of the trec-9 web track. in proceed of trec-2000, 2000. [9] d. hawk and n. craswel. overview of the trec-2001 web track. in proceed of trec-2001, nov. 2001. [10] r. herbrich, t. graepel, and k. obermay. larg margin rank boundari for ordin regress. advanc in larg margin classifi, 2000. [11] a. herschtal and b. raskutti. optimis area under the roc curv us gradient descent. in proceed of the intern confer on machin learn (icml), 2004. [12] k. jarvelin and j. kekalainen. ir evalu method for retriev highli relev document. in proceed of the acm confer on research and develop in inform retriev (sigir), 2000. [13] t. joachim. a support vector method for multivari perform measur. in proceed of the intern confer on machin learn (icml), page 377-384, new york, ny, usa, 2005. acm press. [14] j. lafferti and c. zhai. document languag model, queri model, and risk minim for inform retriev. in proceed of the acm confer on research and develop in inform retriev (sigir), page 111-119, 2001. [15] y. lin, y. lee, and g. wahba. support vector machin for classif in nonstandard situat. machin learn, 46:191-202, 2002. [16] d. metzler and w. b. croft. a markov random field model for term depend. in proceed of the 28th annual intern acm sigir confer on research and develop in inform retriev, page 472-479, 2005. [17] k. morik, p. brockhausen, and t. joachim. combin statist learn with a knowledg-base approach. in proceed of the intern confer on machin learn, 1999. [18] s. robertson. the probabl rank principl in ir. journal of document. journal of document, 33(4):294-304, 1977. [19] i. tsochantaridi, t. hofmann, t. joachim, and y. altun. larg margin method for structur and interdepend output variabl. journal of machin learn research (jmlr), 6(sep):1453-1484, 2005. [20] v. vapnik. statist learn theori. wilei and son inc., 1998. [21] l. yan, r. dodier, m. mozer, and r. wolniewicz. optim classifi perform via approxim to the wilcoxon-mann-witnei statist. in proceed of the intern confer on machin learn (icml), 2003. 