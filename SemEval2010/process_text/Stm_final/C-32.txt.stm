buddycach: high-perform object storag for collabor strong-consist applic in a wan âˆ— magnu e. bjornsson and liuba shrira depart of comput scienc brandei univers waltham, ma 02454-9110 {magnu, liuba}@cs.brandei.edu abstract collabor applic provid a share work  environ for group of network client collabor on a  common task. thei requir strong consist for share  persist data and effici access to fine-grain object. these properti ar difficult to provid in wide-area network  becaus of high network latenc. buddycach is a new transact cach approach that improv the latenc of access to share persist object for collabor strong-consist applic in  high-latenc network environ. the challeng is to improv perform while provid the correct and avail properti of a transact cach protocol in the presenc of node failur and slow peer. we have implement a buddycach prototyp and  evalu it perform. analyt result, confirm by measur of the buddycach prototyp us the  multius 007 benchmark indic that for typic internet  latenc, e.g. rang from 40 to 80 millisecond round trip time to the storag server, peer us buddycach can reduc by up to 50% the latenc of access to share object compar to access the remot server directli. categori and subject descriptor c.2.4 [comput system organ]: distribut system gener term design, perform 1. introduct improv in network connect erod the  distinct between local and wide-area comput and,  increasingli, user expect their work environ to follow them wherev thei go. nevertheless, distribut applic mai perform poorli in wide-area network environ.  network bandwidth problem will improv in the forese  futur, but improv in network latenc is fundament limit. buddycach is a new object cach techniqu that address the network latenc problem for collabor  applic in wide-area network environ. collabor applic provid a share work  environ for group of network user collabor on a  common task, for exampl a team of engin jointli  overse a construct project. strong-consist collabor applic, for exampl cad system, us client/server transact object storag system to ensur consist  access to share persist data. up to now howev, user have rare consid run consist network storag system over wide-area network as perform would be unaccept [24]. for transact storag system, the high cost of wide-area network interact to maintain data consist is the main cost limit the perform and therefor, in wide-area network environ, collabor applic have been adapt to us weaker consist storag system [22]. adapt an applic to us weak consist storag system requir signific effort sinc the applic need to be rewritten to deal with a  differ storag system semant. if share persist object could be access with low-latenc, a new field of distribut strong-consist applic could be open. cooper web cach [10, 11, 15] is a well-known  approach to reduc client interact with a server by  allow on client to obtain miss object from a anoth client instead of the server. collabor applic seem a  particularli good match to benefit from thi approach sinc on of the hard problem, name determin what object ar cach where, becom easi in small group typic of  collabor set. howev, cooper web cach  techniqu do not provid two import properti need by collabor applic, strong consist and effici 26 access to fine-grain object. cooper object cach system [2] provid these properti. howev, thei reli on interact with the server to provid fine-grain cach  coher that avoid the problem of fals share when access to unrel object appear to conflict becaus thei occur on the same physic page. interact with the server  increas latenc. the contribut of thi work is extend cooper cach techniqu to provid strong consist and effici access to fine-grain object in wide-area  environ. consid a team of engin emploi by a construct compani overse a remot project and work in a shed at the construct site. the engin us a collabor cad applic to revis and updat complex project  design document. the share document ar store in  transact repositori server at the compani home site. the engin us workstat run repositori client. the workstat ar interconnect by a fast local ethernet but the network connect to the home repositori server is slow. to improv access latenc, client fetch object from repositori server and cach and access them local. a coher protocol ensur that client cach remain  consist when object ar modifi. the perform problem face the collabor applic is coordin with the server consist access to share object. with buddycach, a group of close-by collabor client, connect to storag repositori via a high-latenc link, can avoid interact with the server if need object, updat or coher inform ar avail in some client in the group. buddycach present two main technic challeng. on challeng is how to provid effici access to share  finegrain object in the collabor group without impos perform overhead on the entir cach system. the other challeng is to support fine-grain cach coher in the presenc of slow and fail node. buddycach us a redirect approach similar to on us in cooper web cach system [11]. a  redirector server, interpos between the client and the remot server, run on the same network as the collabor group and, when possibl, replac the function of the remot server. if the client request can not be serv local, the redirector forward it to a remot server. when on of the client in the group fetch a share object from the  repositori, the object is like to be need by other client.  buddycach redirect subsequ request for thi object to the cach client. similarli, when a client creat or modifi a share object, the new data is like to be of potenti interest to all group member. buddycach us  redirect to support peer updat, a lightweight applic-level multicast techniqu that provid group member with  consist access to the new data commit within the  collabor group without impos extra overhead outsid the group. nevertheless, in a transact system, redirect  interfer with share object avail. solo commit, is a valid techniqu us by buddycach to avoid the  undesir client depend that reduc object avail when some client node in the group ar slow, or client fail independ. a salient featur of solo commit is  support fine-grain valid us inexpens coars-grain coher inform. sinc redirect support the perform benefit of  reduc interact with the server but introduc extra  process cost due to avail mechan and request  forward, thi rais the question is the cure wors than the diseas? we design and implement a buddycach prototyp and studi it perform benefit and cost us analyt model and system measur. we compar the storag system perform with and without buddycach and consid how the cost-benefit balanc is affect by network latenc. analyt result, support by measur base on the multi-user 007 benchmark, indic that for typic  internet latenc buddycach provid signific perform benefit, e.g. for latenc rang from 40 to 80 millisecond round trip time, client us the buddycach can reduc by up to 50% the latenc of access to share object  compar to the client access the repositori directli. these strong perform gain could make transact object storag system more attract for collabor  applic in wide-area environ. 2. relat work cooper cach techniqu [20, 16, 13, 2, 28] provid access to client cach to avoid high disk access latenc in an environ where server and client run on a fast local area network. these techniqu us the server to provid redirect and do not consid issu of high network  latenc. multiprocessor system and distribut share memori system [14, 4, 17, 18, 5] us fine-grain coher techniqu to avoid the perform penalti of fals share but do not address issu of avail when node fail. cooper web cach techniqu, (e.g. [11, 15])  investig issu of maintain a directori of object cach in nearbi proxi cach in wide-area environ, us  distribut directori protocol for track cach chang. thi work doe not consid issu of consist concurr  updat to share fine-grain object. cheriton and li propos mmo [12] a hybrid web  coher protocol that combin invalid with updat us multicast deliveri channel and receiv-reliabl  protocol, exploit local in a wai similar to buddycach. thi multicast transport level solut is gear to the singl writer semant of web object. in contrast, buddycach us applic level multicast and a sender-reliabl  coher protocol to provid similar access latenc  improv for transact object. applic level multicast solut in a middl-ware system wa describ by  pendaraki, shi and verma in [27]. the schema support small multi-sender group appropri for collabor  applic and consid coher issu in the presenc of  failur but doe not support strong consist or fine-grain share. yin, alvisi, dahlin and lin [32, 31] present a  hierarch wan cach coher scheme. the protocol us leas to provid fault-toler call-back and take  advantag of nearbi cach to reduc the cost of leas extens. the studi us simul to investig latenc and fault toler issu in hierarch avoid-base coher scheme. in contrast, our work us implement and analysi to evalu the cost and benefit of redirect and fine grain updat in an optimist system.  anderson, eastham and vahdat in webf [29] present a global file system coher protocol that allow client to choos 27 on per file basi between receiv updat or invalid. updat and invalid ar multicast on separ  channel and client subscrib to on of the channel. the  protocol exploit applic specif method e.g. last-writer-win polici for broadcast applic, to deal with concurr updat but is limit to file system. mazier studi a bandwidth save techniqu [24] to  detect and avoid repeat file fragment transfer across a wan when fragment ar avail in a local cach. buddycach provid similar bandwidth improv when object ar avail in the group cach. 3. buddycach high network latenc impos perform penalti for transact applic access share persist  object in wide-area network environ. thi section  describ the buddycach approach for reduc the network latenc penalti in collabor applic and explain the main design decis. we consid a system in which a distribut transact object repositori store object in highli reliabl server, perhap outsourc in data-center connect via  high-bandwidth reliabl network. collabor client interconnect via a fast local network, connect via high-latenc, possibl satellit, link to the server at the data-center to access share persist object. the server provid disk storag for the persist object. a persist object is own by a singl server. object mai be small (order of 100 byte for program languag object [23]). to amort the cost of disk and network transfer object ar group into physic page. to improv object access latenc, client fetch the object from the server and cach and access them local. a  transact cach coher protocol run at client and server to ensur that client cach remain consist when object ar modifi. the perform problem face the  collabor client group is the high latenc of coordin  consist access to the share object. buddycach architectur is base on a request  redirect server, interpos between the client and the remot server. the interpos server (the redirector) run on the same network as the collabor group and, when possibl, replac the function of the remot server. if the client  request can be serv local, the interact with the server is avoid. if the client request can not be serv local,  redirector forward it to a remot server. redirect approach ha been us to improv the perform of web cach protocol. buddycach redirector support the correct, avail and fault-toler properti of transact cach protocol [19]. the correct properti ensur  onecopi serializ of the object commit by the client transact. the avail and fault-toler properti ensur that a crash or slow client doe not disrupt ani other client"s access to persist object. the three type of client server interact in a  transact cach protocol ar the commit of a transact, the fetch of an object miss in a client cach, and the exchang of cach coher inform. buddycach avoid  interact with the server when a miss object, or cach  coher inform need by a client is avail within the collabor group. the redirector alwai interact with the server at commit time becaus onli storag server  provid transact durabl in a wai that ensur commit client redirector client client buddi group client redirector client client buddi group server figur 1: buddycach. data remain avail in the presenc of client or redirector failur. figur 1 show the overal buddycach  architectur. 3.1 cach coher the redirector maintain a directori of page cach at each client to provid cooper cach [20, 16, 13, 2, 28], redirect a client fetch request to anoth client that cach the request object. in addit, redirector manag cach coher. sever effici transact cach coher protocol [19] exist for persist object storag system. protocol make differ choic in granular of data transfer and  granular of cach consist. the current best-perform protocol us page granular transfer when client fetch miss object from a server and object granular  coher to avoid fals (page-level) conflict. the  transact cach taxonomi [19] propos by carei, franklin and livni classifi the coher protocol into two main categori accord to whether a protocol avoid or detect access to stale object in the client cach. the buddycach approach could be appli to both categori with differ perform cost and benefit in each categori. we chose to investig buddycach in the context of occ [3], the current best perform detect-base  protocol. we chose occ becaus it is simpl, perform well in high-latenc network, ha been implement and we had access to the implement. we ar investig  buddycach with psaa [33], the best perform  avoidancebas protocol. below we outlin the occ protocol [3]. the occ protocol us object-level coher. when a client  request a miss object, the server transfer the contain page. transact can read and updat local cach  object without server intervent. howev, befor a  transact commit it must be valid; the server must make sure the valid transact ha not read a stale version of some object that wa updat by a successfulli  commit or valid transact. if valid fail, the  transact is abort. to reduc the number and cost of abort, 28 helper request a:p fetch ppeer fetch p page p redirector figur 2: peer fetch a server send background object invalid messag to client cach the contain page. when client receiv invalid thei remov stale object from the cach and send background acknowledg to let server know about thi. sinc invalid remov stale object from the client cach, invalid acknowledg indic to the server that a client with no outstand invalid ha read  upto-date object. an unacknowledg invalid indic a stale object mai have been access in the client cach. the valid procedur at the server abort a client  transact if a client read an object while an invalid is outstand. the acknowledg invalid mechan support  object-level cach coher without object-base directori or per-object version number. avoid per-object  overhead is veri import to reduc perform penalti [3] of manag mani small object, sinc typic object ar small. an import buddycach design goal is to  maintain thi benefit. sinc in buddycach a page can be fetch into a client cach without server intervent (as illustr in figur 2), cach directori at the server keep track of page cach in each collabor group rather than each client. redirector keep track of page cach in each client in a group. server send to the redirector invalid for page cach in the entir group. the redirector propag invalid from server to affect client. when all affect client  acknowledg invalid, redirector can propag the group  acknowledg to the server. 3.2 light-weight peer updat when on of the client in the collabor group creat or modifi share object, the copi cach by ani other client becom stale but the new data is like to be of  potenti interest to the group member. the goal in buddycach is to provid group member with effici and consist access to updat commit within the group without  impos extra overhead on other part of the storag system. the two possibl approach to deal with stale data ar cach invalid and cach updat. cach coher studi in web system (e.g. [7]) dsm system (e.g. [5]), and transact object system (e.g. [19]) compar the benefit of updat and invalid. the studi show the commit client server redirector x2. store x 6. updat x 3. commit x 4. commit ok 5. commit ok1. commit x figur 3: peer updat. benefit ar strongli workload-depend. in gener,  invalid-base coher protocol ar effici sinc  invalid ar small, batch and piggyback on other messag. moreov, invalid protocol match the  current hardwar trend for increas client cach size. larger cach ar like to contain much more data than is activ us. updat-base protocol that propag updat to low-interest object in a wide-area network would be  wast. nevertheless, invalid-base coher protocol can perform poorli in high-latenc network [12] if the  object"s new valu is like to be of interest to anoth group member. with an invalid-base protocol, on  member"s updat will invalid anoth member"s cach copi, caus the latter to perform a high-latenc fetch of the new valu from the server. buddycach circumv thi well-known bandwidth vs. latenc trade-off impos by updat and invalid  protocol in wide-area network environ. it avoid the latenc penalti of invalid by us the redirector to retain and propag updat commit by on client to other client within the group. thi avoid the bandwidth penalti of updat becaus server propag invalid to the redirector. as far as we know, thi us of local multicast in buddycach redirector is new and ha not been us in earlier cach system. the peer updat work as follow. an updat commit  request from a client arriv at the redirector contain the object updat. redirector retain the updat and  propag the request to the coordin server. after the  transact commit, the coordin server send a commit repli to the redirector of the commit client group. the  redirector forward the repli to the commit client, and also propag the retain commit updat to the client cach the modifi page (see figur 3). sinc the group outsid the buddycach propag invalid, there is no extra overhead outsid the commit group. 3.3 solo commit in the occ protocol, client acknowledg server  invalid (or updat) to indic remov of stale data. the straightforward group acknowledg protocol where redirector collect and propag a collect  acknowledg29 redirector commit ok abort client 1 client 2 server commit (p(x)) commit (p(x)) ok + inv(p(x)) inv(p(x)) commit(p(x)) commit(p(x)) ack(p(x)) ack(p(x)) figur 4: valid with slow peer ment to the server, interfer with the avail properti of the transact cach protocol [19] sinc a client that is slow to acknowledg an invalid or ha fail can  delai a group acknowledg and prevent anoth client in the group from commit a transact. e.g. an engin that commit a repeat revis to the same share design object (and therefor hold the latest version of the object) mai need to abort if the group acknowledg ha not propag to the server. consid a situat depict in figur 4 where client1 commit a transact t that read the latest version of an object x on page p recent modifi by client1. if the commit request for t reach the server befor the collect acknowledg from client2 for the last modif of x arriv at the server, the occ valid procedur  consid x to be stale and abort t (becaus, as explain abov, an invalid unacknowledg by a client, act as  indic to the server that the cach object valu is stale at the client). note that while invalid ar not requir for the  correct of the occ protocol, thei ar veri import for the perform sinc thei reduc the perform  penalti of abort and fals share. the asynchron  invalid ar an import part of the reason occ ha  competit perform with psaa [33], the best perform avoid-base protocol [3]. nevertheless, sinc invalid ar sent and process asynchron, invalid process mai be arbitrarili delai at a client. leas-base scheme (time-out base) have been propos to improv the avail of  hierarch callback-base coher protocol [32] but the  asynchron natur of invalid make the leas-base  approach inappropri for asynchron invalid. the solo commit valid protocol allow a client with up-to-date object to commit a transact even if the group acknowledg is delai due to slow or crash peer. the protocol requir client to includ extra inform with the transact read set in the commit messag, to indic to the server the object read by the transact ar up-to-date. object version number could provid a simpl wai to track up-to-date object but, as mention abov,  maintain per object version number impos unaccept high overhead (in disk storag, i/o cost and directori size) on the entir object system when object ar small [23].  instead, solo commit us coars-grain page version number to identifi fine-grain object version. a page version number is increment at a server when at transact that modifi object on the page commit. updat commit by a  singl transact and correspond invalid ar therefor uniqu identifi by the modifi page version number. page version number ar propag to client in fetch repli, commit repli and with invalid, and client includ page version number in commit request sent to the server. if a transact fail valid due to miss group acknowledg, the server check page version number of the object in the transact read set and allow the transact to commit if the client ha read from the latest page version. the page version number enabl independ commit but page version check onli detect page-level conflict. to detect object-level conflict and avoid the problem of fals share we need the acknowledg invalid. section 4 describ the detail of the implement of solo commit support for fine-grain share. 3.4 group configur the buddycach architectur support multipl  concurr peer group. potenti, it mai be faster to access data cach in anoth peer group than to access a remot server. in such case extend buddycach protocol to support multi-level peer cach could be worthwhil. we have not pursu thi possibl for sever reason. in web cach workload, simpli increas the  popul of client in a proxi cach often increas the  overal cach hit rate [30]. in buddycach applic,  howev, we expect share to result mainli from explicit client interact and collabor, suggest that inter-group fetch is unlik to occur. moreov, measur from multi-level web cach system [9] indic that a  multilevel system mai not be advantag unless the network connect between the peer group is veri fast. we ar primarili interest in environ where close  collabor peer have fast close-rang connect, but the  connect between peer group mai be slow. as a result, we decid that support for inter-group fetch in buddycach is not a high prioriti right now. to support heterogen resourc-rich and resourc-poor peer, the buddycach redirector can be configur to run either in on of the peer node or, when avail, in a  separ node within the site infrastructur. moreov, in a resourc-rich infrastructur node, the redirector can be  configur as a stand-by peer cach to receiv page fetch by other peer, emul a central cach somewhat similar to a region web proxi cach. from the buddycach cach coher protocol point of view, howev, such a stand-by peer cach is equival to a regular peer cach and therefor we do not consid thi case separ in the discuss in thi paper. 4. implement in thi section we provid the detail of the buddycach implement. we have implement buddycach in the thor client/server object-orient databas [23]. thor  support high perform access to distribut object and therefor provid a good test platform to investig  buddycach perform. 30 4.1 base storag system thor server provid persist storag for object and client cach copi of these object. applic run at the client and interact with the system by make call on method of cach object. all method call occur within atom transact. client commun with server to fetch page or to commit a transact. the server have a disk for store persist object, a stabl transact log, and volatil memori. the disk is organ as a collect of page which ar the unit of disk access. the stabl log hold commit inform and object modif for commit transact. the server memori contain cach directori and a recover modifi object cach call the mob. the directori keep track of which page ar cach by which client. the mob hold recent modifi object that have not yet been written back to their page on disk. as mob fill up, a background process propag modifi object to the disk [21, 26]. 4.2 base cach coher transact ar serial us optimist concurr control occ [3] describ in section 3.1. we provid some of the relev occ protocol implement detail. the client keep track of object that ar read and modifi by it transact; it send thi inform, along with new copi of modifi object, to the server when it tri to commit the transact. the server determin whether the commit is possibl, us a two-phase commit protocol if the  transact us object at multipl server. if the transact commit, the new copi of modifi object ar append to the log and also insert in the mob. the mob is  recover, i.e. if the server crash, the mob is reconstruct at recoveri by scan the log. sinc object ar not lock befor be us, a  transact commit can caus cach to contain obsolet object. server will abort a transact that us obsolet object. howev, to reduc the probabl of abort, server notifi client when their object becom obsolet by send them invalid messag; a server us it directori and the inform about the commit transact to determin what invalid messag to send. invalid messag ar small becaus thei simpli identifi obsolet object.  furthermor, thei ar sent in the background, batch and  piggyback on other messag. when a client receiv an invalid messag, it remov obsolet object from it cach and abort the current  transact if it us them. the client continu to retain page contain invalid object; these page ar now  incomplet with hole in place of the invalid object.  perform invalid on an object basi mean that fals share doe not caus unnecessari abort; keep  incomplet page in the client cach mean that fals share doe not lead to unnecessari cach miss. client acknowledg invalid to indic remov of stale data as explain in section 3.1. invalid messag prevent some abort, and acceler those that must happen - thu wast less work and oï¬„oad detect of abort from server to client. when a transact abort, it client restor the cach copi of modifi object to the state thei had befor the transact start; thi is possibl becaus a client make a copi of an object the first time it is modifi by a  transact. 4.3 redirect the redirector run on the same local network as the peer group, in on of the peer node, or in a special node within the infrastructur. it maintain a directori of page  avail in the peer group and provid fast central fetch redirect (see figur 2) between the peer cach. to  improv perform, client inform the redirector when thei evict page or object by piggyback that inform on messag sent to the redirector. to ensur up-to-date object ar fetch from the group cach the redirector track the statu of the page. a cach page is either complet in which case it contain consist valu for all the object, or incomplet, in which case some of the object on a page ar mark invalid. onli complet page ar us by the peer fetch. the protocol for  maintain page statu when page ar updat and invalid is describ in section 4.4. when a client request ha to be process at the server, e.g., a complet request page is unavail in the peer group or a peer need to commit a transact, the redirector act as a server proxi: it forward the request to the server, and then forward the repli back to the client. in addit, in respons to invalid sent by a server, the redirector distribut the updat or invalid inform to client cach the modifi page and, after all client acknowledg, propag the group acknowledg back to the server (see figur 3). the redirector-server protocol is, in effect, the client-server protocol us in the base thor storag system, where the combin peer group cach is plai the role of a singl client cach in the base system. 4.4 peer updat the peer updat is implement as follow. an updat commit request from a client arriv at the redirector  contain the object updat. redirector retain the updat and propag the request to the coordin server. after a transact commit, us a two phase commit if need, the coordin server send a commit repli to the redirector of the commit client group. the redirector forward the repli to the commit client. it wait for the invalid to arriv to propag correspond retain (commit) updat to the client cach the modifi page (see  figur 3.) particip server that ar home to object modifi by the transact gener object invalid for each cach group that cach page contain the modifi object  (includ the commit group). the invalid ar sent lazili to the redirector to ensur that all the client in the group cach the modifi object get rid of the stale data. in cach group other than the commit group,  redirector propag the invalid to all the client cach the modifi page, collect the client acknowledg and after complet the collect, propag collect  acknowledg back to the server. within the commit client group, the arriv  invalid ar not propag. instead, updat ar sent to client cach those object" page, the updat ar acknowledg by the client, and the collect acknowledg is  propag to the server. an invalid render a cach page unavail for peer fetch chang the statu of a complet page p into an  incomplet. in contrast, an updat of a complet page  preserv the complet page statu. as shown by studi of the 31 fragment reconstruct [2], such updat propag allow to avoid the perform penalti of fals share. that is, when client within a group modifi differ object on the same page, the page retain it complet statu and remain avail for peer fetch. therefor, the effect of peer updat is similar to eager fragment reconstruct [2]. we have also consid the possibl of allow a peer to fetch an incomplet page (with invalid object mark accordingli) but decid against thi possibl becaus of the extra complex involv in track invalid object. 4.5 vcach the solo commit valid protocol allow client with up-to-date object to commit independ of slower (or fail) group member. as explain in section 3.3, the solo commit protocol allow a transact t to pass valid if extra coher inform suppli by the client indic that transact t ha read up-to-date object. client us page version number to provid thi extra coher  inform. that is, a client includ the page version number correspond to each object in the read object set sent in the commit request to the server. sinc a uniqu page  version number correspond to each commit object updat, the page version number associ with an object allow the valid procedur at the server to check if the client transact ha read up-to-date object. the us of coars-grain page version to identifi object version avoid the high penalti of maintain persist object version for small object, but requir an extra  protocol at the client to maintain the map from a cach  object to the identifi page version (objecttovers). the main implement issu is concern with maintain thi map effici. at the server side, when modif commit, server associ page version number with the invalid. at valid time, if an unacknowledg invalid is  pend for an object x read by a transact t, the valid procedur check if the version number for x in t"s read set match the version number for highest pend invalid for x, in which case the object valu is current, otherwis t fail valid. we note again that the page version number-base check, and the invalid acknowledg-base check ar  complimentari in the solo commit valid and both ar need. the page version number check allow the valid to  proce befor invalid acknowledg arriv but by itself a page version number check detect page-level conflict and is not suffici to support fine-grain coher without the object-level invalid. we now describ how the client manag the map  objecttovers. the client maintain a page version number for each cach page. the version number satisfi the  follow invari v p about the state of object on a page: if a cach page p ha a version number v, then the valu of an object o on a cach page p is either invalid or it  reflect at least the modif commit by transact preced the transact that set p"s version number to v. new object valu and new page version number arriv when a client fetch a page or when a commit repli or  invalid arriv for thi page. the new object valu modifi the page and, therefor, the page version number need to be updat to maintain the invari v p. a page version number that arriv when a client fetch a page, replac object version x 8 redirector server 1client 1 com(p(x,6),q(y,9)) com(p(x,6),q(y,9)) ok(p(x,8),q(y,10)) ok(p(x,8),q(y,10)) inv(q(s,11)) inv(q(s,11)) inv(p(r,7) inv(p(r,7) server 2 figur 5: reorder invalid the page version number for thi page. such an updat preserv the invari v p. similarli, an in-sequenc page version number arriv at the client in a commit or  invalid messag advanc the version number for the entir cach page, without violat v p. howev, invalid or updat and their correspond page version number can also arriv at the client out of sequenc, in which case updat the page version number could violat v p. for exampl, a commit repli for a transact that updat  object x on page p in server s1, and object y on page q in server s2, mai deliv a new version number for p from the transact coordin s1 befor an invalid gener for an earlier transact that ha modifi object r on page p arriv from s1 (as shown in figur 5). the cach updat protocol ensur that the valu of ani object o in a cach page p reflect the updat or  invalid with the highest observ version number. that is, obsolet updat or invalid receiv out of sequenc do not affect the valu of an object. to maintain the objecttovers map and the  invari v p in the presenc of out-of-sequenc arriv of page version number, the client manag a small version number cach vcach that maintain the map from an object into it correspond page version number for all reorder  version number updat until a complet page version number sequenc is assembl. when the miss version number for the page arriv and complet a sequenc, the version number for the entir page is advanc. the objecttovers map, includ the vcach and page version number, is us at transact commit time to provid version number for the read object set as follow. if the read object ha an entri in the vcach, it version number is equal to the highest version number in the vcach for thi object. if the object is not present in the vcach, it version number is equal the version number of it contain cach page. figur 6 show the objecttovers map in the client cach, includ the page version number for page and the vcach. client can limit vcach size as need sinc re-fetch a page remov all reorder page version number from the vcach. howev, we expect version number reorder to be uncommon and therefor expect the vcach to be veri small. 5. buddycach failov a client group contain multipl client node and a  redi32 versionpageobject version vcach client cach client page cach figur 6: objecttovers map with vcach rector that can fail independ. the goal of the failov protocol is to reconfigur the buddycach in the case of a node failur, so that the failur of on node doe not disrupt other client from access share object. moreov, the failur of the redirector should allow unaffect client to keep their cach intact. we have design a failov protocol for buddycach but have not implement it yet. the appendix outlin the protocol. 6. perform evalu buddycach redirect support the perform  benefit of avoid commun with the server but  introduc extra process cost due to avail mechan and request forward. is the cure wors then the  diseas? to answer the question, we have implement a  buddycach prototyp for the occ protocol and conduct  experi to analyz the perform benefit and cost over a rang of network latenc. 6.1 analysi the perform benefit of peer fetch and peer updat ar due to avoid server interact. thi section present a simpl analyt perform model for thi benefit. the avoid server interact correspond to differ type of client cach miss. these can be cold miss, invalid miss and capac miss. our analysi focus on cold miss and invalid miss, sinc the benefit of avoid capac miss can be deriv from the cold miss.  moreov, technolog trend indic that memori and storag capac will continu to grow and therefor a typic  buddycach configur is like not to be cach limit. the client cach miss ar determin by sever  variabl, includ the workload and the cach configur. our analysi tri, as much as possibl, to separ these variabl so thei can be control in the valid  experi. to studi the benefit of avoid cold miss, we consid cold cach perform in a read-onli workload (no  invalid miss). we expect peer fetch to improv the latenc cost for client cold cach miss by fetch object from nearbi cach. we evalu how the redirect cost affect thi benefit by compar and analyz the perform of an applic run in a storag system with  buddycach and without (call base). to studi the benefit of avoid invalid miss, we consid hot cach perform in a workload with  modif (with no cold miss). in hot cach we expect buddycach to provid two complementari benefit, both of which reduc the latenc of access to share modifi  object. peer updat let a client access an object modifi by a nearbi collabor peer without the delai impos by invalid-onli protocol. in group where peer share a read-onli interest in the modifi object, peer fetch allow a client to access a modifi object as soon as a collabor peer ha it, which avoid the delai of server fetch without the high cost impos by the updat-onli protocol. technolog trend indic that both benefit will remain import in the forese futur. the trend toward  increas in avail network bandwidth decreas the cost of the updat-onli protocol. howev, the trend toward increasingli larg cach, that ar updat when cach  object ar modifi, make invalid-base protocol more attract. to evalu these two benefit we consid the  perform of an applic run without buddycach with an applic run buddycach in two configur. on, where a peer in the group modifi the object, and anoth where the object ar modifi by a peer outsid the group. peer updat can also avoid invalid miss due to fals-share, introduc when multipl peer updat  differ object on the same page concurr. we do not  analyz thi benefit (demonstr by earlier work [2]) becaus our benchmark do not allow us to control object layout, and also becaus thi benefit can be deriv given the cach hit rate and workload content. 6.1.1 the model the model consid how the time to complet an  execut with and without buddycach is affect by  invalid miss and cold miss. consid k client run concurr access uniformli a share set of n page in buddycach (bc) and base. let tfetch(s), tredirect(s), tcommit(s), and tcomput(s) be the time it take a client to, respect, fetch from server, peer fetch, commit a transact and comput in a transact, in a system s, where s is either a system with buddycach (bc) or without (base). for simplic, our model assum the fetch and commit time ar constant. in gener thei mai vari with the server load, e.g. thei depend on the total number of client in the system. the number of miss avoid by peer fetch depend on k, the number of client in the buddycach, and on the client co-interest in the share data. in a specif buddycach  execut it is model by the variabl r, defin as a number of fetch arriv at the redirector for a given version of page p (i.e. until an object on the page is invalid). consid an execut with cold miss. a client start with a cold cach and run read-onli workload until it  access all n page while commit l transact. we  assum there ar no capac miss, i.e. the client cach is larg enough to hold n page. in bc, r cold miss for page p reach the redirector. the first of the miss fetch p from the server, and the subsequ r âˆ’ 1 miss ar  redirect. sinc each client access the entir share set r = k. let tcold(base) and tcold(bc) be the time it take to complet the l transact in base and bc. 33 tcold(base) = n âˆ— tfetch(base) +(tcomput + tcommit(base)) âˆ— l (1) tcold(bc) = n âˆ— 1 k âˆ— tfetch(bc) + (1 âˆ’ 1 k ) âˆ— tredirect +(tcomput + tcommit(bc)) âˆ— l (2) consid next an execut with invalid miss. a client start with a hot cach contain the work set of n page. we focu on a simpl case where on client (writer) run a workload with modif, and the other client (reader) run a read-onli workload. in a group contain the writer (bcw ), peer updat elimin all invalid miss. in a group contain onli reader (bcr), dure a steadi state execut with uniform updat, a client transact ha missinv  invalid miss. consid the sequenc of r client miss on page p that arriv at the redirector in bcr between two consequ invalid of page p. the first miss goe to the server, and the r âˆ’ 1 subsequ miss ar redirect. unlik with cold miss, r â‰¤ k becaus the second  invalid disabl redirect for p until the next miss on p caus a server fetch. assum uniform access, a client invalid miss ha a chanc of 1/r to be the first miss (result in server fetch), and a chanc of (1 âˆ’ 1/r) to be redirect. let tinval(base), tinval(bcr) and tinval(bcw ) be the time it take to complet a singl transact in the base, bcr and bcw system. tinval(base) = missinv âˆ— tfetch(base) +tcomput + tcommit(base) (3) tinval(bcr) = missinv âˆ— ( 1 r âˆ— tfetch(bcr) +(1 âˆ’ 1 r ) âˆ— tredirect(bcr)) +tcomput + tcommit(bcr) (4) tinval(bcw ) = tcomput + tcommit(bcw ) (5) in the experi describ below, we measur the  paramet n, r, missinv, tfetch(s), tredirect(s), tcommit(s), and tcomput(s). we comput the complet time deriv us the abov model and deriv the benefit. we then valid the model by compar the deriv valu to the complet time and benefit measur directli in the  experi. 6.2 experiment setup befor present our result we describ our experiment setup. we us two system in our experi. the base system run thor distribut object storag system [23] with client connect directli to the server. the buddi system run our implement of buddycach prototyp in thor, support peer fetch, peer updat, and solo commit, but not the failov. our workload ar base on the multi-user oo7  benchmark [8]; thi benchmark is intend to captur the  characterist of mani differ multi-user cad/cam/case  applic, but doe not model ani specif applic. we us oo7 becaus it is a standard benchmark for measur object storag system perform. the oo7 databas  contain a tree of assembl object with leav point to three composit part chosen randomli from among 500 such  object. each composit part contain a graph of atom part link by connect object; each atom part ha 3  outgo connect. we us a medium databas that ha 200 atom part per composit part. the multi-user databas alloc for each client a privat modul consist of on tree of assembl object, and add an extra share modul that scale proportion to the number of client. we expect a typic buddycach configur not to be cach limit and therefor focu on workload where the object in the client work set fit in the cach. sinc the goal of our studi is to evalu how effect our  techniqu deal with access to share object, in our studi we limit client access to share data onli. thi allow us to studi the effect our techniqu have on cold cach and cach consist miss and isol as much as possibl the effect of cach capac miss. to keep the length of our experi reason, we us small cach. the oo7 benchmark gener databas  modul of predefin size. in our implement of oo7, the privat modul size is about 38mb. to make sure that the entir work set fit into the cach we us a singl privat modul and choos a cach size of 40mb for each client. the oo7 databas is gener with modul for 3 client, onli on of which is us in our experi as we explain abov. the object in the databas ar cluster in 8k page, which ar also the unit of transfer in the fetch request. we consid two type of transact workload in our analysi, read-onli and read-write. in oo7 benchmark, read-onli transact us the t1 travers that perform a depth-first travers of entir composit part graph. write transact us the t2b travers that is ident to t1 except that it modifi all the atom part in a singl  composit. a singl transact includ on travers and there is no sleep time between transact. both read-onli and read-write transact alwai work with data from the same modul. client run read-write transact don"t  modifi in everi transact, instead thei have a 50% probabl of run read-onli transact. the databas wa store by a server on a 40gb ibm 7200rpm hard drive, with a 8.5 averag seek time and 40 mb/sec data transfer rate. in base system client  connect directli to the databas. in buddi system client  connect to the redirector that connect to the databas. we run the experi with 1-10 client in base, and on or two 1-10 client group in buddi. the server, the client and the redirector ran on a 850mhz intel pentium iii  processor base pc, 512mb of memori, and linux red hat 6.2. thei were connect by a 100mb/s ethernet. the server wa configur with a 50mb cach (of which 6mb were us for the modifi object buffer), the client had a 40mb cach. the experi ran in utah experiment testb emulab.net [1]. 34 latenc [ms] base buddi 3 group 5 group 3 group 5 group fetch 1.3 1.4 2.4 2.6 commit 2.5 5.5 2.4 5.7 tabl 1: commit and server fetch oper latenc [ms] peerfetch 1.8 - 5.5 âˆ’alerthelp 0.3 - 4.6 âˆ’copyunswizzl 0.24 âˆ’crossredirector 0.16 tabl 2: peer fetch 6.3 basic cost thi section analyz the basic cost of the request in the buddi system dure the oo7 run. 6.3.1 redirect fetch and commit request in the buddycach cross the redirector, a cost not incur in the base system. for a request redirect to the server (server fetch) the extra cost of redirect includ a local request from the client to  redirector on the wai to and from the server. we evalu thi latenc overhead indirectli by compar the measur  latenc of the buddi system server fetch or commit request with the measur latenc of the correspond request in the base system. tabl 1 show the latenc for the commit and server fetch request in the base and buddi system for 3 client and 5 client group in a fast local area network. all the number were comput by averag measur request latenc over 1000 request. the measur show that the redirect cost of cross the redirector in not veri high even in a local area network. the commit cost increas with the number of client sinc commit ar process sequenti. the fetch cost doe not increas as much becaus the server cach reduc thi cost. in a larg system with mani group, howev, the server cach becom less effici. to evalu the overhead of the peer fetch, we measur the peer fetch latenc (peerfetch) at the request client and break down it compon cost. in peer fetch, the cost of the redirect includ, in addit to the local network request cost, the cpu process latenc of cross the redirector and cross the helper, the latter includ the time to process the help request and the time to copi, and unswizzl the request page. we directli measur the time to copi and unswizzl the request page at the helper, (copyunswizzl), and time the cross time us a null cross request. tabl 2 summar the latenc that allow us to break down the peer fetch cost. crossredirector, includ the cpu latenc of cross the redirector plu a local network round-trip and is measur by time a round-trip null request issu by a client to the redirector. alerthelp, includ the time for the helper to notic the request plu a network  roundtrip, and is measur by time a round-trip null request issu from an auxiliari client to the helper client. the local network latenc is fix and less than 0.1 ms. the alerthelp latenc which includ the elaps time from the help request arriv until the start of help request process is highli variabl and therefor contribut to the high variabl of the peerfetch time. thi is becaus the client in buddi system is current singl thread and therefor onli start process a help request when block wait for a fetch- or commit repli. thi overhead is not inher to the buddycach architectur and could be  mitig by a multi-thread implement in a system with pre-emptiv schedul. 6.3.2 version cach the solo commit allow a fast client modifi an object to commit independ of a slow peer. the solo  commit mechan introduc extra process at the server at transact valid time, and extra process at the client at transact commit time and at updat or  invalid process time. the server side overhead ar minim and consist of a page version number updat at commit time, and a version number comparison at transact valid time. the version cach ha an entri onli when invalid or updat arriv out of order. thi mai happen when a  transact access object in multipl server. our experi run in a singl server system and therefor, the commit time overhead of version cach manag at the client doe not contribut in the result present in the section below. to gaug these client side overhead in a multipl server  system, we instrument the version cach implement to run with a workload trace that includ reorder  invalid and time the basic oper. the extra client commit time process includ a version cach lookup oper for each object read by the  transact at commit request prepar time, and a version cach insert oper for each object updat by a  transact at commit repli process time, but onli if the  updat page is miss some earlier invalid or updat. it is import that the extra commit time cost ar kept to a minimum sinc client is synchron wait for the commit complet. the measur show that in the worst case, when a larg number of invalid arriv out of order, and about half of the object modifi by t2a (200 object) resid on reorder page, the cost of updat the version cach is 0.6 ms. the invalid time cost ar  compar, but sinc invalid and updat ar process in the background thi cost is less import for the overal perform. we ar current work on optim the version cach implement to further reduc these cost. 6.4 overal perform thi section examin the perform gain seen by an applic run oo7 benchmark with a buddycach in a wide area network. 6.4.1 cold miss to evalu the perform gain from avoid cold miss we compar the cold cach perform of oo7  benchmark run read-onli workload in the buddi and base system. we deriv the time by time the execut of the system in the local area network environ and  substitut 40 ms and 80 ms delai for the request cross the redirector and the server to estim the perform in the wide-area-network. figur 7 and 8 show the overal time to complet 1000 cold cach transact. the number were 35 0 5 0 100 150 200 250 base buddi base buddi base buddi 3 client 5 client 10 client [ms] cpu commit server fetch peer fetch figur 7: breakdown for cold read-onli 40ms rtt 0 5 0 100 150 200 250 300 350 400 base buddi base buddi base buddi 3 client 5 client 10 client [ms] cpu commit server fetch peer fetch figur 8: breakdown for cold read-onli 80ms rtt obtain by averag the overal time of each client in the group. the result show that in a 40 ms network buddi  system reduc significantli the overal time compar to the base system, provid a 39% improv in a three client group, 46% improv in the five client group and 56% improv in the ten client case. the overal time includ time spent perform client comput, direct fetch request, peer fetch, and commit request. in the three client group, buddi and base incur almost the same commit cost and therefor the entir perform benefit of buddi is due to peer fetch avoid direct fetch. in the five and ten client group the server fetch cost for individu client decreas becaus with more client fault in a fix size share modul into buddycach, each client need to perform less server fetch. figur 8 show the overal time and cost break down in the 80 ms network. the buddycach provid similar  perform improv as with the 40ms network. higher network latenc increas the rel perform  advantag provid by peer fetch rel to direct fetch but thi benefit is offset by the increas commit time. figur 9 show the rel latenc improv provid by buddycach (comput as the overal measur time differ between buddi and base rel to base) as a -10% 0% 10% 20% 30% 40% 50% 60% 70% 1 5 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 100 latenc [ms] 3 client 3 client (perf model) 5 client 5 client (perf model) 10 client 10 fe (perf model) figur 9: cold miss benefit 0 2 0 4 0 6 0 8 0 100 120 140 base buddi reader buddi writer [ms] cpu commit server fetch peer fetch figur 10: breakdown for hot read-write 40ms rtt function of network latenc, with a fix server load. the cost of the extra mechan domin buddycach benefit when network latenc is low. at typic internet latenc 20ms-60ms the benefit increas with latenc and level off around 60ms with signific (up to 62% for ten client) improv. figur 9 includ both the measur improv and the improv deriv us the analyt model.remark, the analyt result predict the measur improv veri close, albeit be somewhat higher than the  empir valu. the main reason why the simplifi model work well is it captur the domin perform compon, network latenc cost. 6.4.2 invalid miss to evalu the perform benefit provid by  buddycach due to avoid invalid miss, we compar the hot cach perform of the base system with two  differ buddi system configur. on of the buddi system configur repres a collabor peer group  modifi share object (writer group), the other repres a group where the peer share a read-onli interest in the  modifi object (reader group) and the writer resid outsid the buddycach group. in each of the three system, a singl client run a  readwrit workload (writer) and three other client run read-onli workload (reader). buddi system with on group  contain36 0 5 0 100 150 200 250 300 base buddi reader buddi writer [ms] cpu commit server fetch peer fetch figur 11: breakdown for hot read-write 80ms rtt ing a singl reader and anoth group contain two reader and on writer model the writer group. buddi system with on group contain a singl writer and anoth group  run three reader model the reader group. in base, on writer and three reader access the server directli. thi simpl configur is suffici to show the impact of  buddycach techniqu. figur 10 and 11 show the overal time to complet 1000 hot cach oo7 read-onli transact. we obtain the  number by run 2000 transact to filter out cold miss and then time the next 1000 transact. here again, the report number ar deriv from the local area network experi result. the result show that the buddycach reduc  significantli the complet time compar to the base system. in a 40 ms network, the overal time in the writer group improv by 62% compar to base. thi benefit is due to peer updat that avoid all miss due to updat. the overal time in the reader group improv by 30% and is due to peer fetch that allow a client to access an  invalid object at the cost of a local fetch avoid the delai of fetch from the server. the latter is an import  benefit becaus it show that on workload with updat, peer fetch allow an invalid-base protocol to provid some of the benefit of updat-base protocol. note that the perform benefit deliv by the peer fetch in the reader group is approxim 50% less than the perform benefit deliv by peer updat in the writer group. thi differ is similar in 80ms network. figur 12 show the rel latenc improv  provid by buddycach in buddi reader and buddi writer configur (comput as the overal time differ  between buddyread and base rel to base, and buddi writer and base rel to base) in a hot cach experi as a function of increas network latenc, for fix server load. the peer updat benefit domin overhead in writer configur even in low-latenc network (peer updat  incur minim overhead) and offer signific 44-64%  improv for entir latenc rang. the figur includ both the measur improv and the improv deriv us the analyt model. as in cold cach experi, here the analyt result  predict the measur improv close. the differ is -10% 0% 10% 20% 30% 40% 50% 60% 70% 1 5 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 100 latenc [ms] benefit[%] buddi reader buddi reader (perf model) buddi writer buddi writer (perf model) figur 12: invalid miss benefit minim in the "writer group", and somewhat higher in the "reader group" (consist with the result in the cold cach experi). as in cold cach case, the reason why the  simplifi analyt model work well is becaus it captur the cost of network latenc, the domin perform cost. 7. conclus collabor applic provid a share work  environ for group of network client collabor on a  common task. thei requir strong consist for share  persist data and effici access to fine-grain object. these properti ar difficult to provid in wide-area network  becaus of high network latenc. thi paper describ buddycach, a new transact  cooper cach [20, 16, 13, 2, 28] techniqu that improv the latenc of access to share persist object for  collabor strong-consist applic in high-latenc  network environ. the techniqu improv perform yet provid strong correct and avail properti in the presenc of node failur and slow client. buddycach us redirect to fetch miss object  directli from group member cach, and to support peer  updat, a new lightweight applic-level multicast  techniqu that give group member consist access to the new data commit within the collabor group without  impos extra overhead outsid the group. redirect,  howev, can interfer with object avail. solo commit, is a new valid techniqu that allow a client in a group to commit independ of slow or fail peer. it  provid fine-grain valid us inexpens coars-grain version inform. we have design and implement buddycach  prototyp in thor distribut transact object storag  system [23] and evalu the benefit and cost of the system over a rang of network latenc. analyt result,  support by the system measur us the multi-user 007 benchmark indic, that for typic internet latenc  buddycach provid signific perform benefit, e.g. for latenc rang from 40 to 80 millisecond round trip time, client us the buddycach can reduc by up to 50% the latenc of access to share object compar to the client access the repositori directli. the main contribut of the paper ar: 1. extend cooper cach techniqu to support 37 fine-grain strong-consist access in high-latenc  environ, 2. an implement of the system prototyp that yield strong perform gain over the base system, 3. analyt and measur base perform  evalu of the cost and benefit of the new techniqu captur the domin perform cost, high  network latenc. 8. acknowledg we ar grate to jai lepreau and the staff of utah  experiment testb emulab.net [1], especi leigh stoller, for host the experi and the help with the testb. we also thank jeff chase, mauric herlihi, butler lampson and the oopsla review for the us comment that improv thi paper. 9. refer [1] "emulab.net", the utah network emul facil. http://www.emulab.net. [2] a. adya, m. castro, b. liskov, u. maheshwari, and l. shrira. fragment reconstruct: provid global cach coher in a transact storag system. proceed of the intern confer on distribut comput system, mai 1997. [3] a. adya, r. gruber, b. liskov, and u. maheshwari. effici optimist concurrencti control us loos synchron clock. in proceed of the acm sigmod intern confer on manag of data, mai 1995. [4] c. amza, a.l. cox, s. dwarkada, p. keleh, h. lu, r. rajamoni, w. yu, and w. zwaenepoel. treadmark: share memori comput on network of workstat. ieee comput, 29(2), februari 1996. [5] c. anderson and a. karlin. two adapt hybrid cach coher protocol. in proceed of the 2nd ieee symposium on high-perform comput architectur (hpca "96), februari 1996. [6] m. baker. fast crash recoveri in distribut file system. phd thesi, univers of california at berkelei, 1994. [7] p. cao and c. liu. maintain strong cach consist in the world wide web. in 17th intern confer on distribut comput system., april 1998. [8] m. carei, d. j. dewitt, c. kant, and j. f. naughton. a statu report on the oo7 oodbm benchmark effort. in proceed of oopsla, octob 1994. [9] a. chankhunthod, m. schwartz, p. danzig, k. worrel, and c. neerdael. a hierarch internet object cach. in usenix annual technic confer, januari 1995. [10] j. chase, s. gadd, and m. rabinovich. directori structur for scalabl internet cach. technic report cs-1997-18, dept. of comput scienc, duke univers, novemb 1997. [11] j. chase, s. gadd, and m. rabinovich. not all hit ar creat equal: cooper proxi cach over a wide-area network. in third intern www cach workshop, june 1998. [12] d. r. cheriton and d. li. scalabl web cach of frequent updat object us reliabl multicast. 2nd usenix symposium on internet technolog and system, octob 1999. [13] m. d. dahlin, r. y. wang, t. e. anderson, and d. a. patterson. cooper cach: us remot client memori to improv file system perform. proceed of the usenix confer on oper system design and implement, novemb 1994. [14] s. dwarkada, h. lu, a.l. cox, r. rajamoni, and w. zwaenepoel. combin compil-time and run-time support for effici softwar distribut share memori. in proceed of ieee, special issu on distribut share memori, march 1999. [15] li fan, pei cao, jussara almeida, and andrei broder. summari cach: a scalabl wide-area web cach share protocol. in proceed of acm sigcomm, septemb 1998. [16] m. feelei, w. morgan, f. pighin, a. karlin, and h. levi. implement global memori manag in a workstat cluster. proceed of the 15th acm symposium on oper system principl, decemb 1995. [17] m. j. feelei, j. s. chase, v. r. narasayya, and h. m. levi. integr coher and recoverabl in distribut system. in proceed of the first usenix symposium on oper sustem design and implement, mai 1994. [18] p. ferreira and m. shapiro et al. perdi: design, implement, and us of a persist distribut store. in recent advanc in distribut system, lnc 1752, springer-verlag, 1999. [19] m. j. franklin, m. carei, and m. livni. transact client-server cach consist: altern and perform. in acm transact on databas system, volum 22, page 315-363, septemb 1997. [20] michael franklin, michael carei, and miron livni. global memori manag for client-server dbm architectur. in proceed of the 19th intl. confer on veri larg data base (vldb), august 1992. [21] s. ghemawat. the modifi object buffer: a storag manag techniqu for object-orient databas. phd thesi, massachusett institut of technolog, 1997. [22] l. kawel, s. beckhardt, t. halvorsen, r. ozzi, and i. greif. replic document manag in a group commun system. in proceed of the acm cscw confer, septemb 1988. [23] b. liskov, m. castro, l. shrira, and a. adya. provid persist object in distribut system. in proceed of the 13th european confer on object-orient program (ecoop "99), june 1999. [24] a. muthitacharoen, b. chen, and d. mazier. a low-bandwidth network file system. in 18th acm symposium on oper system principl, octob 2001. [25] b. oki and b. liskov. viewstamp replic: a new primari copi method to support highli-avail distribut system. in proc. of acm symposium on principl of distribut 38 comput, august 1988. [26] j. o"tool and l. shrira. opportunist log: effici instal read in a reliabl object server. in usenix symposium on oper system design and implement, novemb 1994. [27] d. pendaraki, s. shi, and d. verma. almi: an applic level multicast infrastructur. in 3rd usenix symposium on internet technolog and system, march 2001. [28] p. sarkar and j. hartman. effici cooper cach us hint. in usenix symposium on oper system design and implement, octob 1996. [29] a. m. vahdat, p. c. eastham, and t. e anderson. webf: a global cach coher file system. technic report, univers of california, berkelei, 1996. [30] a. wolman, g. voelker, n. sharma, n. cardwel, a. karlin, and h. levi. on the scale and perform of cooper web proxi cach. in 17th acm symposium on oper system principl, decemb 1999. [31] j. yin, l. alvisi, m. dahlin, and c. lin. hierarch cach consist in a wan. in usenix symposium on internet technolog and system, octob 1999. [32] j. yin, l. alvisi, m. dahlin, and c. lin. volum leas for consist in larg-scale system. ieee transact on knowledg and data engin, 11(4), juli/august 1999. [33] m. zaharioudaki, m. j. carei, and m. j. franklin. adapt, fine-grain share in a client-server oodbm: a callback-base approach. acm transact on databas system, 22:570-627, decemb 1997. 10. appendix thi appendix outlin the buddycach failov protocol. to accommod heterogen client includ  resourcepoor hand-held we do not requir the avail of  persist storag in the buddycach peer group. the  buddycach design assum that the client cach and the  redirector data structur do not surviv node failur. a failur of a client or a redirector is detect by a  membership protocol that exchang period i am aliv  messag between group member and initi a failov  protocol. the failov determin the activ group particip, re-elect a redirector if need, reiniti the buddycach data structur in the new configur and restart the protocol. the group reconfigur protocol is similar to the on present in [25]. here we describ how the failov manag the buddycach state. to restart the buddycach protocol, the failov need to resynchron the redirector page directori and  clientserv request forward so that activ client can continu run transact us their cach. in the case of a client failur, the failov remov the crash client page from the directori. ani respons to an earlier request  initi by the fail client is ignor except a commit repli, in which case the redirector distribut the retain commit updat to activ client cach the modifi page. in the case of a redirector failur, the failov protocol reiniti session with the server and client, and  rebuild the page directori us a protocol similar to on in [6]. the newli restart redirector ask the activ group member for the list of page thei ar cach and the  statu of these page, i.e. whether the page ar complet or incomplet. request outstand at the redirector at the time of the crash mai be lost. a lost fetch request will time out at the client and will be retransmit. a transact run at the client dure a failov and commit after the failov is treat as a regular transact, a transact try to commit dure a failov is abort by the failov  protocol. a client will restart the transact and the commit request will be retransmit after the failov.  invalid, updat or collect updat acknowledg lost at the crash redirector could prevent the garbag collect of pend invalid at the server or the vcach in the client. therefor, server detect a redirector crash  retransmit unacknowledg invalid and commit repli. uniqu version number in invalid and updat ensur that duplic retransmit request ar detect and  discard. sinc the transact valid procedur depend on the cach coher protocol to ensur that transact do not read stale data, we now need to argu that buddycach failov protocol doe not compromis the correct of the valid procedur. recal that buddycach transact valid us two complementari mechan, page  version number and invalid acknowledg from the client, to check that a transact ha read up-to-date data. the redirector-base invalid (and updat)  acknowledg propag ensur the follow invari. when a server receiv an acknowledg for an object o  modif (invalid or updat) from a client group, ani client in the group cach the object o ha either instal the latest valu of object o, or ha invalid o.  therefor, if a server receiv a commit request from a client for a transact t read an object o after a failov in the client group, and the server ha no unacknowledg invalid for o pend for thi group, the version of the object read by the transact t is up-to-date independ of client or redirector failur. now consid the valid us version number. the transact commit record contain a version number for each object read by the transact. the version number protocol maintain the invari v p that ensur that the valu of object o read by the transact correspond to the highest version number for o receiv by the client. the  invari hold sinc the client never appli an earlier  modif after a later modif ha been receiv.  retransmit of invalid and updat maintain thi invari. the valid procedur check that the version number in the commit record match the version number in the  unacknowledg outstand invalid. it is straightforward to see that sinc thi check is an end-to-end client-server check it is unaffect by client or redirector failur. the failov protocol ha not been implement yet. 39 