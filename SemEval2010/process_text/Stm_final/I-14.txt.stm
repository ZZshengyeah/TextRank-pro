a reinforc learn base distribut search algorithm for hierarch peer-to-peer inform retriev system haizheng zhang colleg of inform scienc and technolog pennsylvania state univers univers park, pa 16803 hzhang@ist.psu.edu victor lesser depart of comput scienc univers of massachusett amherst, ma 01003 lesser@cs.umass.edu abstract the domin exist rout strategi emploi in  peerto-peer(p2p) base inform retriev(ir) system ar similar-base approach. in these approach, agent depend on the content similar between incom queri and their direct neighbor agent to direct the distribut search session. howev, such a heurist is myopic in that the neighbor agent mai not be connect to more  relev agent. in thi paper, an onlin reinforc-learn base approach is develop to take advantag of the  dynam run-time characterist of p2p ir system as  repres by inform about past search session.  specif, agent maintain estim on the downstream agent" abil to provid relev document for incom queri. these estim ar updat gradual by learn from the feedback inform return from previou search session. base on thi inform, the agent deriv correspond rout polici. thereaft, these agent rout the queri base on the learn polici and updat the estim base on the new rout polici. experiment result  demonstr that the learn algorithm improv consider the rout perform on two test collect set that have been us in a varieti of distribut ir studi. categori and subject descriptor i.2.11 [distribut artifici intellig]: multiag system gener term algorithm, perform, experiment 1. introduct over the last few year there have been increas  interest in studi how to control the search process in peer-to-peer(p2p) base inform retriev(ir) system [6, 13, 14, 15]. in thi line of research, on of the core  problem that concern research is to effici rout user queri in the network to agent that ar in possess of appropri document. in the absenc of global  inform, the domin strategi in address thi problem ar content-similar base approach [6, 13, 14, 15]. while the content similar between queri and local node  appear to be a credit indic for the number of  relev document resid on each node, these approach ar limit by a number of factor. first of all,  similaritybas metric can be myopic sinc local relev node mai not be connect to other relev node. second, the similar-base approach do not take into account the run-time characterist of the p2p ir system, includ environment paramet, bandwidth usag, and the  histor inform of the past search session, that provid valuabl inform for the queri rout algorithm. in thi paper, we develop a reinforc learn base ir approach for improv the perform of distribut ir search algorithm. agent can acquir better search strategi by collect and analyz feedback inform from previou search session. particularli, agent  maintain estim, name expect util, on the downstream agent" capabl of provid relev document for  specif type of incom queri. these estim ar  updat gradual by learn from the feedback inform return from previou search session. base on the  updat expect util inform, the agent deriv  correspond rout polici. thereaft, these agent rout the queri base on the learn polici and updat the estim on the expect util base on the new rout polici. thi process is conduct in an iter manner. the goal of the learn algorithm, even though it consum some network bandwidth, is to shorten the rout time so that more queri ar process per time unit while at the same time find more relev document. thi contrast with the content-similar base approach where similar oper ar repeat for everi incom queri and the process time keep larg constant over time. anoth wai of view thi paper is that our basic  approach to distribut ir search is to construct a hierarch overlai network(agent organ) base on the  contentsimilar measur among agent" document collect in a bottom-up fashion. in the past work, we have shown that thi organ improv search perform significantli. howev, thi organiz structur doe not take into account the arriv pattern of queri, includ their  frequenc, type, and where thei enter the system, nor the avail commun bandwidth of the network and  process capabl of individu agent. the intent of the reinforc learn is to adapt the agent" rout decis to the dynam network situat and learn from past search session. specif, the contribut of thi paper includ: (1) a reinforc learn base approach for agent to acquir satisfactori rout polici base on estim of the potenti contribut of their neighbor agent; (2) two strategi to speed up the learn process. to our best knowledg, thi is on of the first reinforc learn applic in address distribut content  share problem and it is indic of some of the issu in appli reinforc in a complex applic. the remaind of thi paper is organ as follow:  section 2 review the hierarch content share system and the two-phase search algorithm base on such topolog.  section 3 describ a reinforc learn base approach to direct the rout process; section 4 detail the experiment set and analyz the result. section 5 discuss relat studi and section 6 conclud the paper. 2. search in hierarch p2p ir system thi section briefli review our basic approach to  hierarch p2p ir system. in a hierarch p2p ir  system illustr in fig.1, agent ar connect to each other through three type of link: upward link, downward link, and later link. in the follow section, we denot the set of agent that ar directli connect to agent ai as directconn(ai), which is defin as directconn(ai) = nei(ai) ∪ par(ai) ∪ chl(ai) , where nei(ai) is the set of neighbor agent connect to ai through later link; par(ai) is the set of agent whom agent ai is connect to through upward link and chl(ai) is the set of agent that agent ai connect to through downward link. these link ar establish through a bottom-up content-similar base distribut cluster process[15]. these link ar then us by agent to locat other agent that contain document relev to the given queri. a typic agent ai in our system us two queue: a local search queue, lsi, and a messag forward queue mfi. the state of the two queue constitut the intern state of an agent. the local search queue lsi store search session that ar schedul for local process. it is a prioriti queue and agent ai alwai select the most promis queri to process in order to maxim the global util. mfi  consist of a set of queri to forward on and is process in a fifo (first in first out) fashion. for the first queri in mfi, agent ai determin which subset of it neighbor agent to forward it to base on the agent"s rout polici πi. these rout decis determin how the search  process is conduct in the network. in thi paper, we call ai as aj"s upstream agent and aj as ai"s downstream agent if a4 a5 a6 a7 a2 a3 a9 nei(a2)={a3} par(a2)={a1} chl(a2)={a4,a5} a1 a8 figur 1: a fraction of a hierarch p2pir system an agent ai rout a queri to agent aj. the distribut search protocol of our hierarch agent organ is compos of two step. in the first step, upon receipt of a queri qk at time tl from a user, agent ai  initi a search session si by probe it neighbor agent aj ∈ nei(ai) with the messag probe for the similar valu sim(qk, aj) between qk and aj. here, ai is defin as the queri initi of search session si. in the second step, ai select a group of the most promis agent to start the actual search process with the messag search. these search messag contain a ttl (time to live)  paramet in addit to the queri. the ttl valu decreas by 1 after each hop. in the search process, agent discard those queri that either have been previous process or whose ttl drop to 0, which prevent queri from loop in the system forev. the search session end when all the agent that receiv the queri drop it or ttl decreas to 0. upon receipt of search messag for qk, agent schedul local activ includ local search, forward qk to their neighbor, and return search result to the queri  initi. thi process and relat algorithm ar detail in [15, 14]. 3. a basic reinforcementlearn base search approach in the aforement distribut search algorithm, the rout decis of an agent ai reli on the similar  comparison between incom queri and ai"s neighbor agent in order to forward those queri to relev agent  without flood the network with unnecessari queri messag. howev, thi heurist is myopic becaus a relev  direct neighbor is not necessarili connect to other relev agent. in thi section, we propos a more gener approach by frame thi problem as a reinforc learn task. in pursuit of greater flexibl, agent can switch between two mode: learn mode and non-learn mode. in the non-learn mode, agent oper in the same wai as thei do in the normal distribut search process describ in [14, 15]. on the other hand, in the learn mode, in  parallel with distribut search session, agent also particip in a learn process which will be detail in thi section. note that in the learn protocol, the learn process doe not interfer with the distribut search process. agent can choos to initi and stop learn process without  affect the system perform. in particular, sinc the learn process consum network resourc (especi bandwidth), agent can choos to initi learn onli when the network load is rel low, thu minim the extra  commun cost incur by the learn algorithm. the section is structur as follow, section 3.1 describ 232 the sixth intl. joint conf. on autonom agent and multi-agent system (aama 07) a reinforc learn base model. section 3.2 describ a protocol to deploi the learn algorithm in the network. section 3.3 discuss the converg of the learn  algorithm. 3.1 the model an agent"s rout polici take the state of a search  session as input and output the rout action for that queri. in our work, the state of a search session sj is stipul as: qsj = (qk, ttlj) where ttlj is the number of hop that remain for the search session sj , qk is the specif queri. ql is an attribut of qk that indic which type of queri qk most like belong to. the set of ql can be gener by run a simpl onlin classif algorithm on all the queri that have been process by the agent, or an oﬄin algorithm on a pre-design train set. the assumpt here is that the set of queri type is learn ahead of time and belong to the common knowledg of the agent in the network. futur work includ explor how learn can be accomplish when thi assumpt doe not hold. given the queri type set, an incom queri qi can be classifi to on queri class q(qi) by the formula: q(qi) = arg max qj p(qi|qj) (1) where p(qi|qj ) indic the likelihood that the queri qi is gener by the queri class qj [8]. the set of atom rout action of an agent ai is denot as {αi}, where {αi} is defin as αi = {αi0 , αi1 , ..., αin }. an element αij repres an action to rout a given queri to the neighbor agent aij ∈ directconn(ai). the rout polici πi of agent ai is stochast and it outcom for a search session with state qsj is defin as: πi(qsj) = {(αi0 , πi(qsi, αi0 )), (αi1 , πi(qsi, αi1 )), ...} (2) note that oper πi is overload to repres either the probabilist polici for a search session with state qsj,  denot as πi(qsj); or the probabl of forward the queri to a specif neighbor agent aik ∈ directconn(ai)  under the polici πi(qsj), denot as πi(qsj, αik ).  therefor, equat (2) mean that the probabl of  forward the search session to agent ai0 is πi(qsi, αi0 ) and so on. under thi stochast polici, the rout action is  nondeterminist. the advantag of such a strategi is that the best neighbor agent will not be select repeatedli, therebi mitig the potenti hot spot situat. the expect util, un i (qsj), is us to estim the  potenti util gain of rout queri type qsj to agent ai under polici πn i . the superscript n indic the valu at the nth iter in an iter learn process. the expect util provid rout guidanc for futur search session. in the search process, each agent ai maintain partial  observ of it neighbor" state, as shown in fig. 2. the partial observ includ non-local inform such as the potenti util estim of it neighbor am for queri state qsj, denot as um(qsj), as well as the load  inform, lm. these observ ar updat period by the neighbor. the estim util inform will be us to updat ai"s expect util for it rout polici. load inform expect util for differ queri type neighbor agent ... a0 a1 a3 a2 un 0 (qs0) ... ... ... ... ...... un 0 (qs1) un 1 (qs1) un 2 (qs1) un 3 (qs1) un 1 (qs0) un 2 (qs0) un 3 (qs0) ln 0 ln 1 ln 2 ln 3 ... qs0 qs1 ... figur 2: agent ai"s partial observ about it neighbor(a0, a1...) the load inform of am, lm, is defin as lm = |mfm| cm , where |mfm| is the length of the messag-forward queue and cm is the servic rate of agent am"s messag-forward queue. therefor lm character the util of an agent"s commun channel, and thu provid non-local inform for am"s neighbor to adjust the paramet of their rout polici to avoid inund their downstream agent. note that base on the characterist of the queri enter the system and agent" capabl, the load of agent mai not be uniform. after collect the util rate inform from all it neighbor, agent ai comput li as a singl measur for assess the averag load  condit of it neighborhood: li = p k lk |directconn(ai)| agent exploit li valu in determin the rout  probabl in it rout polici. note that, as describ in section 3.2, inform about neighbor agent is piggyback with the queri messag propag among the agent whenev possibl to reduc the traffic overhead. 3.1.1 updat the polici an iter updat process is introduc for agent to learn a satisfactori stochast rout polici. in thi  iter process, agent updat their estim on the potenti util of their current rout polici and then propag the updat estim to their neighbor. their neighbor then gener a new rout polici base on the updat observ and in turn thei calcul the expect util base on the new polici and continu thi iter process. in particular, at time n, given a set of expect  util, an agent ai, whose directli connect agent set is directconn(ai) = {ai0 , ..., aim }, determin it  correspond stochast rout polici for a search session of state qsj base on the follow step: (1) ai first select a subset of agent as the potenti downstream agent from set directconn(ai), denot as pdn(ai, qsj). the size of the potenti downstream agent is specifi as |pdn(ai, qsj)| = min(|nei(ai), dn i + k)| where k is a constant and is set to 3 in thi paper; dn i , the forward width, is defin as the expect number of the sixth intl. joint conf. on autonom agent and multi-agent system (aama 07) 233 neighbor agent that agent ai can forward to at time n. thi formula specifi that the potenti downstream agent set pdn(ai, qsj) is either the subset of neighbor agent with dn i + k highest expect util valu for state qsj among all the agent in directconn(ai), or all their neighbor agent. the k is introduc base on the idea of a stochast rout polici and it make the forward probabl of the dn i +k highest agent less than 100%. note that if we want to limit the number of downstream agent for search session sj as 5, the probabl of forward the queri to all neighbor agent should add up to 5.  set up dn i valu properli can improv the util rate of the network bandwidth when much of the network is idl while mitig the traffic load when the network is highli load. the dn+1 i valu is updat base on dn i , the  previou and current observ on the traffic situat in the neighborhood. specif, the updat formula for dn+1 i is dn+1 i = dn i ∗ (1 + 1 − li |directconn(ai)| ) in thi formula, the forward width is updat base on the traffic condit of agent ai"s neighborhood, i.e li, and it previou valu. (2) for each agent aik in the pdn(ai, qsj), the  probabl of forward the queri to aik is determin in the follow wai in order to assign higher forward  probabl to the neighbor agent with higher expect util valu: πn+1 i (qsj, αik ) = dn+1 i |pdn(ai, qsj)| + β ∗ ` uik (qsj) − pdu(ai, qsj) |pdn(ai, qsj)| ´ (3) where pdun(ai, qsj) = x o∈p dn(ai,qsj ) uo(qsj) and qsj is the subsequ state of agent aik after agent ai forward the search session with state qsj to it  neighbor agent aik ; if qsj = (qk, ttl0), then qsj = (qk, ttl0 − 1). in formula 3, the first term on the right of the equat, dn+1 i |p dn(ai,qsj )| , is us to to determin the forward  probabl by equal distribut the forward width, dn+1 i , to the agent in pdn(ai, qsj) set. the second term is us to adjust the probabl of be chosen so that agent with higher expect util valu will be favor. β is  determin accord to: β = min ` m − dn+1 i m ∗ umax − pdun(ai, qsj) , dn+1 i pdun(ai, qsj) − m ∗ umin ´ (4) where m = |pdn(ai, qsj)|, umax = max o∈p dn(ao,qsj ) uo(qsj) and umin = min o∈p dn(ao,qsj ) uo(qsj) thi formula guarante that the final πn+1 i (qsj, αik ) valu is well defin, i.e, 0 ≤ πn+1 i (qsj, αik ) ≤ 1 and x i πn+1 i (qsj, αik ) = dn+1 i howev, such a solut doe not explor all the  possibl. in order to balanc between exploit and  explor, a λ-greedi approach is taken. in the λ-greedi  approach, in addit to assign higher probabl to those agent with higher expect util valu, as in the equat (3). agent that appear to be not-so-good choic will also be sent queri base on a dynam explor rate. in particular, for agent in the set pdn(ai, qsj), πn+1 i1 (qsj) is determin in the same wai as the abov, with the onli differ be that dn+1 i is replac with dn+1 i ∗ (1 − λn). the remain search bandwidth is us for learn by assign probabl λn evenli to agent ai2 in the set directconn(ai) − pdn(ai, qsj). πn+1 i2 (qsj, αik ) = dn+1 i ∗ λn |directconn(ai) − pdn(ai, qsj)| (5) where pdn(ai, qsj) ⊂ directconn(ai). note that the explor rate λ is not a constant and it decreas  overtim. the λ is determin accord to the follow  equat: λn+1 = λ0 ∗ e−c1n (6) where λ0 is the initi explor rate, which is a  constant; c1 is also a constant to adjust the decreas rate of the explor rate; n is the current time unit. 3.1.2 updat expect util onc the rout polici at step n+1, πn+1 i , is determin base on the abov formula, agent ai can updat it own  expect util, un+1 i (qsi), base on the the updat rout polici result from the formula 5 and the updat u valu of it neighbor agent. under the assumpt that after a queri is forward to ai"s neighbor the subsequ search session ar independ, the updat formula is similar to the bellman updat formula in q-learn: un+1 i (qsj) = (1 − θi) ∗ un i (qsj) + θi ∗ (rn+1 i (qsj) + x k πn+1 i (qsj, αik )un k (qsj)) (7) where qsj = (qj, ttl − 1) is the next state of qsj = (qj, ttl); rn+1 i (qsj) is the expect local reward for queri class qk at agent ai under the rout polici πn+1 i ; θi is the coeffici for decid how much weight is given to the old valu dure the updat process: the smaller θi valu is, the faster the agent is expect to learn the real valu, while the greater volatil of the algorithm, and vice versa. rn+1 (s) is updat accord to the follow equat: 234 the sixth intl. joint conf. on autonom agent and multi-agent system (aama 07) rn+1 i (qsj) = rn i (qsj) +γi ∗ (r(qsj) − rn i (qsj)) ∗ p(qj|qj ) (8) where r(qsj) is the local reward associ with the search session. p(qj|qj ) indic how relev the queri qj is to the queri type qj, and γi is the learn rate for agent ai. depend on the similar between a specif queri qi and it correspond queri type qi, the local reward associ with the search session ha differ impact on the rn i (qsj) estim. in the abov formula, thi impact is reflect by the coeffici, the p(qj|qj) valu. 3.1.3 reward function after a search session stop when it ttl valu expir, all search result ar return back to the user and ar  compar against the relev judgment. assum the set of search result is sr, the reward rew(sr) is defin as: rew(sr) = j 1 if |rel(sr)| > c |rel(sr)| c otherwis. where sr is the set of return search result, rel(sr) is the set of relev document in the search result. thi equat specifi that user give 1.0 reward if the number of return relev document reach a predefin number c. otherwis, the reward is in proport to the number of relev document return. thi rational for set up such a cut-off valu is that the import of recal ratio decreas with the abund of relev document in real world, therefor user tend to focu on onli a limit number of search result. the detail of the actual rout protocol will be  introduc in section 3.2 when we introduc how the learn algorithm is deploi in real system. 3.2 deploy of the learn algorithm thi section describ how the learn algorithm can be us in either a singl-phase or a two-phase search process. in the singl-phase search algorithm, search session start from the initi of the queri. in contrast, in the two-step search algorithm, the queri initi first attempt to seek a more appropri start point for the queri by introduc an exploratori step as describ in section 2. despit the differ in the qualiti of start point, the major part of the learn process for the two algorithm is larg the same as describ in the follow paragraph. befor learn start, each agent initi the expect util valu for all possibl state as 0. thereaft, upon receipt of a queri, in addit to the normal oper  describ in the previou section, an agent ai also set up a timer to wait for the search result return from it  downstream agent. onc the timer expir or it ha receiv respons from all it downstream agent, ai merg and  forward the search result accru from it downstream agent to it upstream agent. set up the timer speed up the learn becaus agent can avoid wait too long for the downstream agent to return search result. note that these detail result and correspond agent inform will still be store at ai until the feedback inform is pass from it upstream agent and the perform of it  downstream agent can be evalu. the durat of the timer is relat to the ttl valu. in thi paper, we set the timer to ttimer = ttli ∗ 2 + tf , where ttli ∗ 2 is the sum of the travel time of the queri in the network, and tf is the expect time period that user would like to wait. the search result will eventu be return to the search session initi a0. thei will be compar to the relev judgment that is provid by the final user (as describ in the experi section, the relev judgement for the queri set is provid along with the data collect). the reward will be calcul and propag backward to the agent along the wai that search result were pass. thi is a revers process of the search result propag. in the process of propag reward backward, agent updat  estim of their own potenti util valu, gener an  upto-date polici and pass their updat result to the  neighbor agent base on the algorithm describ in section 3. upon chang of expect util valu, agent ai send out it updat util estim to it neighbor so that thei can act upon the chang expect util and correspond state. thi updat messag includ the potenti reward as well as the correspond state qsi = (qk, ttll) of agent ai. each neighbor agent, aj, react to thi kind of  updat messag by updat the expect util valu for state qsj(qk, ttll + 1) accord to the newli-announc chang expect util valu. onc thei complet the updat, the agent would again in turn inform relat neighbor to  updat their valu. thi process goe on until the ttl valu in the updat messag increas to the ttl limit. to speed up the learn process, while updat the  expect util valu of an agent ai"s neighbor agent we specifi that um(qk, ttl0) >= um(qk, ttl1) iff ttl0 > ttl1 thu, when agent ai receiv an updat expect util valu with ttl1, it also updat the expect util valu with ani ttl0 > ttl1 if um(qk, ttl0) < um(qk, ttl1) to speed up converg. thi heurist is base on the fact that the util of a search session is a non-decreas function of time t. 3.3 discuss in formal the content rout system as a learn task, mani assumpt ar made. in real system, these assumpt mai not hold, and thu the learn algorithm mai not converg. two problem ar of particular note, (1) thi content rout problem doe not have markov properti. in contrast to ip-level base packet rout, the rout decis of each agent for a particular search  session sj depend on the rout histori of sj. therefor, the assumpt that all subsequ search session ar  independ doe not hold in realiti. thi mai lead to  doubl count problem that the relev document of some agent will be count more than onc for the state where the ttl valu is more than 1. howev, in the context of the hierarch agent organ, two factor mitig thi problem: first, the agent in each content group form a tree-like structur. with the absens of the cycl, the  estim insid the tree would be close to the accur valu. secondli, the stochast natur of the rout polici partli remedi thi problem. the sixth intl. joint conf. on autonom agent and multi-agent system (aama 07) 235 (2) anoth challeng for thi learn algorithm is that in a real network environ observ on neighbor agent mai not be abl to be updat in time due to the commun delai or other situat. in addit, when neighbor agent updat their estim at the same time, oscil mai aris dure the learn process[1]. thi paper explor sever approach to speed up the learn process. besid the aforement strategi of updat the expect util valu, we also emploi an activ updat strategi where agent notifi their  neighbor whenev it expect util is updat. thu a faster converg speed can be achiev. thi strategi contrast to the lazi updat, where agent onli echo their  neighbor agent with their expect util chang when thei exchang inform. the trade off between the two  approach is the network load versu learn speed. the advantag of thi learn algorithm is that onc a rout polici is learn, agent do not have to repeatedli compar the similar of queri as long as the network topolog remain unchang. instead, agent just have to determin the classif of the queri properli and follow the learn polici. the disadvantag of thi learn-base approach is that the learn process need to be conduct whenev the network structur chang. there ar mani potenti extens for thi learn model. for exampl, a singl measur is current us to indic the traffic load for an agent"s neighborhood. a simpl extens would be to keep track of individu load for each neighbor of the agent. 4. experimentssettingsand result the experi ar conduct on trano simul toolkit with two set of dataset, trec-vlc-921 and  trec123-100. the follow sub-section introduc the trano testb, the dataset, and the experiment result. 4.1 trano testb trano (task rout on agent network organ) is a multi-agent base network base inform retriev testb. trano is built on top of the farm [4], a time base distribut simul that provid a data  dissemin framework for larg scale distribut agent network base organ. trano support import and  export of agent organ profil includ topolog connect and other featur. each trano agent is  compos of an agent view structur and a control unit. in  simul, each agent is puls regularli and the agent check the incom messag queue, perform local oper and then forward messag to other agent . 4.2 experiment set in our experi, we us two standard dataset,  trecvlc-921 and trec-123-100 dataset, to simul the  collect host on agent. the trec-vlc-921 and  trec123-100 dataset were creat by the u.s. nation institut for standard technolog(nist) for it trec confer. in distribut inform retriev domain, the two data collect ar split to 921 and 100 sub-collect. it is  observ that dataset trec-vlc-921 is more heterogen than trec-123-100 in term of sourc, document length, and relev document distribut from the statist of the two data collect list in [13]. henc, trec-vlc-921 is much closer to real document distribut in p2p  environ. furthermor, trec-123-100 is split into two set of 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 500 1000 1500 2000 2500 3000 arss queri number arss versu the number of incom queri for trec-vlc-921 ssla-921 ssna-921 figur 3: arss(averag reward per search  session) versu the number of search session for 1phase search in trec-vlc-921 0 0.1 0.2 0.3 0.4 0.5 0.6 0 500 1000 1500 2000 2500 3000 arss queri number arss versu queri number for trec-vlc-921 tsla-921 tsna-921 figur 4: arss(averag reward per search  session) versu the number of search session for 2phase search in trec-vlc-921 sub-collect in two wai: randomli and by sourc. the two partit ar denot as trec-123-100-random and trec-123-100-sourc respect. the document in each subcollect in dataset trec-123-100-sourc ar more  coher than those in trec-123-100-random. the two  differ set of partit allow us to observ how the  distribut learn algorithm is affect by the homogen of the collect. the hierarch agent organ is gener by the algorithm describ in our previou algorithm [15]. dure the topolog gener process, degre inform of each agent is estim by the algorithm introduc by palmer et al. [9] with paramet α = 0.5 and β = 0.6. in our experi, we estim the upward limit and downward degre limit us linear discount factor 0.5, 0.8 and 1.0. onc the topolog is built, queri randomli select from the queri set 301−350 on trec-vlc-921 and queri set 1− 50 on trec-123-100-random and trec-123-100-sourc ar inject to the system base on a poisson distribut p(n(t) = n) = (λt)n n! e−λ 236 the sixth intl. joint conf. on autonom agent and multi-agent system (aama 07) 0 50 100 150 200 250 300 350 400 0 500 1000 1500 2000 2500 3000 cumulativeutil queri number cumul util over the number of incom queri tsla-921 ssna-921 ssla-921 tsna-921 figur 5: the cumul util versu the number of search session trec-vlc-921 in addit, we assum that all agent have an equal chanc of get queri from the environ, i.e, λ is the same for everi agent. in our experi, λ is set as 0.0543 so that the mean of the incom queri from the environ to the agent network is 50 per time unit. the servic time for the commun queue and local search queue, i.e tqij and tr, is set as 0.01 time unit and 0.05 time unit  respect. in our experi, there ar ten type of queri acquir by cluster the queri set 301 − 350 and 1 − 50. 4.3 result analysi and evalu figur 3 demonstr the arss(averag reward per search session) versu the number of incom queri over time for the the singl-step base non-learn algorithm (ssna), and the singl-step learn algorithm(ssla) for data collect trec-vlc-921. it show that the averag reward for ssna algorithm rang from 0.02 − 0.06 and the perform chang littl over time. the averag reward for ssla approach start at the same level with the ssna algorithm. but the perform increas over time and the averag perform gain stabil at about 25% after queri rang 2000 − 3000. figur 4 show the arss(averag reward per search  session) versu the number of incom queri over time for the the two-step base non-learn algorithm(tsna), and the two-step learn algorithm(tsla) for data  collect trec-vlc-921. the tsna approach ha a rel consist perform with the averag reward rang from 0.05 − 0.15. the averag reward for tsla approach, where learn algorithm is exploit, start at the same level with the tsna algorithm and improv the averag reward over time until 2000−2500 queri join the system. the result show that the averag perform gain for tsla approach over tnla approach is 35% after stabil. figur 5 show the cumul util versu the number of incom queri over time for ssna, ssla,tsna, and tsla respect. it illustr that the cumul  util of non-learn algorithm increas larg linearli over time, while the gain of learn-base algorithm  acceler when more queri enter the system. these experiment result demonstr that learn-base approach  consist perform better than non-learn base rout  algorithm. moreov, two-phase learn base algorithm is better than singl-phase base learn algorithm becaus the maxim reward an agent can receiv from search it neighborhood within ttl hop is relat to the total  number of the relev document in that area. thu, even the optim rout polici can do littl beyond reach these relev document faster. on the contrari, the  two-stepbas learn algorithm can reloc the search session to a neighborhood with more relev document. the tsla combin the merit of both approach and outperform them. tabl 1 list the cumul util for dataset  trec123-100-random and trec-123-100-sourc with  hierarch organ. the five column show the result for four differ approach. in particular, column tsna-random show the result for dataset trec-123-100-random with the tsna approach. the column tsla-random show the result for dataset trec-123-100-random with the tsla approach. there ar two number in each cell in the  column tsla-random. the first number is the actual  cumul util while the second number is the percentag gain in term of the util over tsna approach. column tsna-sourc and tsla-sourc show the result for dataset trec-123-100-sourc with tsna and tsla approach respect. tabl 1 show that the perform  improv for trec-123-100-random is not as signific as the other dataset. thi is becaus that the document in the sub-collect of trec-123-100-random ar select  randomli which make the collect model, the signatur of the collect, less meaning. sinc both algorithm ar design base on the assumpt that document collect can be well repres by their collect model, thi result is not surpris. overal, figur 4, 5, and tabl 1 demonstr that the reinforc learn base approach can consider  enhanc the system perform for both data collect. howev, it remain as futur work to discov the  correl between the magnitud of the perform gain and the size of the data collect and/or the extent of the  heterogen between the sub-collect. 5. relat work the content rout problem differ from the  networklevel rout in packet-switch commun network in that content-base rout occur in applic-level  network. in addit, the destin agent in our  contentrout algorithm ar multipl and the address ar not known in the rout process. ip-level rout problem have been attack from the reinforc learn  perspect[2, 5, 11, 12]. these studi have explor fulli distribut algorithm that ar abl, without central  coordin to dissemin knowledg about the network, to find the shortest path robustli and effici in the face of chang network topolog and chang link cost. there ar two major class of adapt, distribut packet  rout algorithm in the literatur: distanc-vector algorithm and link-state algorithm. while thi line of studi carri a certain similar with our work, it ha mainli focus on packet-switch commun network. in thi domain, the destin of a packet is determinist and uniqu. each agent maintain estim, probabilist or  determinist, on the distanc to a certain destin through it neighbor. a variant of q-learn techniqu is deploi the sixth intl. joint conf. on autonom agent and multi-agent system (aama 07) 237 tabl 1: cumul util for dataset trec-123-100-random and trec-123-100-sourc with hierarch organ; the percentag number in the column tsla-random and tsla-sourc demonstr the perform gain over the algorithm without learn queri number tsna-random tsla-random tsna-sourc tsla-sourc 500 25.15 28.45 13% 24.00 21.05 -13% 1000 104.99 126.74 20% 93.95 96.44 2.6% 1250 149.79 168.40 12% 122.64 134.05 9.3% 1500 188.94 211.05 12% 155.30 189.60 22% 1750 235.49 261.60 11% 189.14 243.90 28% 2000 275.09 319.25 16% 219.0 278.80 26% to updat the estim to converg to the real distanc. it ha been discov that the local properti is an  import featur of inform retriev system in user  model studi[3]. in p2p base content share system, thi properti is exemplifi by the phenomenon that user tend to send queri that repres onli a limit number of  topic and convers, user in the same neighborhood ar like to share common interest and send similar queri [10]. the learn base approach is perceiv to be more benefici for real distribut inform retriev system which  exhibit local properti. thi is becaus the user" traffic and queri pattern can reduc the state space and speed up the learn process. relat work in take advantag of thi properti includ [7], where the author attempt to address thi problem by user model techniqu. 6. conclus in thi paper, a reinforc-learn base approach is develop to improv the perform of distribut ir search algorithm. particularli, agent maintain estim, name expect util, on the downstream agent" abil to provid relev document for incom queri. these  estim ar updat gradual by learn from the feedback inform return from previou search session. base on the updat expect util inform, the agent  modifi their rout polici. thereaft, these agent rout the queri base on the learn polici and updat the  estim on the expect util base on the new rout polici. the experi on two differ distribut ir dataset illustr that the reinforc learn approach improv consider the cumul util over time. 7. refer [1] s. abdallah and v. lesser. learn the task alloc game. in aama "06: proceed of the fifth intern joint confer on autonom agent and multiag system, page 850-857, new york, ny, usa, 2006. acm press. [2] j. a. boyan and m. l. littman. packet rout in dynam chang network: a reinforc learn approach. in advanc in neural inform process system, volum 6, page 671-678. morgan kaufmann publish, inc., 1994. [3] j. c. french, a. l. powel, j. p. callan, c. l. vile, t. emmitt, k. j. prei, and y. mou. compar the perform of databas select algorithm. in research and develop in inform retriev, page 238-245, 1999. [4] b. horl, r. mailler, and v. lesser. farm: a scalabl environ for multi-agent develop and evalu. in advanc in softwar engin for multi-agent system, page 220-237, berlin, 2004. springer-verlag. [5] m. littman and j. boyan. a distribut reinforc learn scheme for network rout. in proceed of the intern workshop on applic of neural network to telecommun, 1993. [6] j. lu and j. callan. feder search of text-base digit librari in hierarch peer-to-peer network. in in ecir"05, 2005. [7] j. lu and j. callan. user model for full-text feder search in peer-to-peer network. in acm sigir 2006. acm press, 2006. [8] c. d. man and h. sch¨utz. foundat of statist natur languag process. the mit press, cambridg, massachusett, 1999. [9] c. r. palmer and j. g. steffan. gener network topolog that obei power law. in proceed of globecom "2000, novemb 2000. [10] k. sripanidkulchai, b. magg, and h. zhang. effici content locat us interest-base local in peer-topeer system. in infocom, 2003. [11] d. subramanian, p. druschel, and j. chen. ant and reinforc learn: a case studi in rout in dynam network. in in proceed of the fifteenth intern joint confer on artifici intellig, page 832-839, 1997. [12] j. n. tao and l. weaver. a multi-agent, polici gradient approach to network rout. in in proceed of the eighteenth intern confer on machin learn, 2001. [13] h. zhang, w. b. croft, b. levin, and v. lesser. a multi-agent approach for peer-to-peer inform retriev. in proceed of third intern joint confer on autonom agent and multi-agent system, juli 2004. [14] h. zhang and v. lesser. multi-agent base peer-to-peer inform retriev system with concurr search session. in proceed of the fifth intern joint confer on autonom agent and multi-agent system, mai 2006. [15] h. zhang and v. r. lesser. a dynam form hierarch agent organ for a distribut content share system. in 2004 ieee/wic/acm intern confer on intellig agent technolog (iat 2004), 20-24 septemb 2004, beij, china, page 169-175. ieee comput societi, 2004. 238 the sixth intl. joint conf. on autonom agent and multi-agent system (aama 07) 