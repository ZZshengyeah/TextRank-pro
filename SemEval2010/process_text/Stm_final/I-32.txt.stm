an adversari environ model for bound ration agent in zero-sum interact inon zuckerman1 , sarit krau1 , jeffrei s. rosenschein2 , gal kaminka1 1 depart of comput scienc 2 the school of engin bar-ilan univers and comput scienc ramat-gan, israel hebrew univers, jerusalem, israel {zukermi,sarit,galk}@cs.biu.ac.il jeff@cs.huji.ac.il abstract multiag environ ar often not cooper nor  collabor; in mani case, agent have conflict interest, lead to adversari interact. thi paper present a formal adversari environ model for bound  ration agent oper in a zero-sum environ. in such environ, attempt to us classic util-base search method can rais a varieti of difficulti (e.g., implicitli model the oppon as an omnisci util maxim, rather than leverag a more nuanc, explicit oppon model). we defin an adversari environ by describ the mental state of an agent in such an environ. we then present behavior axiom that ar intend to serv as  design principl for build such adversari agent. we  explor the applic of our approach by analyz log file of complet connect-four game, and present an empir analysi of the axiom" appropri. categori and subject descriptor i.2.11 [artifici intellig]: distribut artifici  intellig-intellig agent,multiag system; i.2.4 [artifici intellig]: knowledg represent formal and method -modal logic gener term design, theori 1. introduct earli research in multiag system (ma) consid cooper group of agent; becaus individu agent had limit resourc, or limit access to inform (e.g.,  limit process power, limit sensor coverag), thei work togeth by design to solv problem that individu thei could not solv, or at least could not solv as effici. ma research, howev, soon began to consid  interact agent with individu interest, as repres of differ human or organ with non-ident  interest. when interact ar guid by divers interest,  particip mai have to overcom disagr,  uncoop interact, and even intent attempt to damag on anoth. when these type of interact occur,  environ requir appropri behavior from the agent situat in them. we call these environ adversari environ, and call the clash agent adversari. model of cooper and teamwork have been  extens explor in ma through the axiomat of  mental state (e.g., [8, 4, 5]). howev, none of thi research dealt with adversari domain and their implic for agent behavior. our paper address thi issu by  provid a formal, axiomat mental state model for a subset of adversari domain, name simpl zero-sum adversari environ. simpl zero-sum encount exist of cours in variou  twoplay game (e.g., chess, checker), but thei also exist in n-player game (e.g., risk, diplomaci), auction for a  singl good, and elsewher. in these latter environ  especi, us a util-base adversari search (such as the min-max algorithm) doe not alwai provid an adequ solut; the payoff function might be quit complex or  difficult to quantifi, and there ar natur comput  limit on bound ration agent. in addit, tradit search method (like min-max) do not make us of a model of the oppon, which ha proven to be a valuabl addit to adversari plan [9, 3, 11]. in thi paper, we develop a formal, axiomat model for bound ration agent that ar situat in a zero-sum adversari environ. the model us differ modal oper, and it main foundat ar the sharedplan [4] model for collabor behavior. we explor environ properti and the mental state of agent to deriv  behavior axiom; these behavior axiom constitut a formal model that serv as a specif and design guidelin for agent design in such set. we then investig the behavior of our model empir us the connect-four board game. we show that thi game conform to our environ definit, and analyz player" behavior us a larg set of complet match log 550 978-81-904262-7-5 (rp) c 2007 ifaama file. in addit, we us the result present in [9] to  discuss the import of oppon model in our  connectfour adversari domain. the paper proce as follow. section 2 present the model"s formal. section 3 present the empir  analysi and it result. we discuss relat work in section 4, and conclud and present futur direct in section 5. 2. adversari environ the adversari environ model (denot as ae) is  intend to guid the design of agent by provid a  specif of the capabl and mental attitud of an agent in an adversari environ. we focu here on specif type of adversari environ, specifi as follow: 1. zero-sum interact: posit and neg util of all agent sum to zero; 2. simpl ae: all agent in the environ ar  adversari agent; 3. bilater ae: ae"s with exactli two agent; 4. multilater ae": ae"s of three or more agent. we will work on both bilater and multilater  instanti of zero-sum and simpl environ. in particular, our adversari environ model will deal with  interact that consist of n agent (n ≥ 2), where all agent ar adversari, and onli on agent can succe. exampl of such environ rang from board game (e.g., chess, connect-four, and diplomaci) to certain econom  environ (e.g., n-bidder auction over a singl good). 2.1 model overview our approach is to formal the mental attitud and behavior of a singl adversari agent; we consid how a singl agent perceiv the ae. the follow list specifi the condit and mental state of an agent in a simpl, zero-sum ae: 1. the agent ha an individu intent that it own goal will be complet; 2. the agent ha an individu belief that it and it  adversari ar pursu full conflict goal (defin  below)there can be onli on winner; 3. the agent ha an individu belief that each adversari ha an intent to complet it own full conflict goal; 4. the agent ha an individu belief in the (partial) profil of it adversari. item 3 is requir, sinc it might be the case that some agent ha a full conflict goal, and is current consid adopt the intent to complet it, but is, as of yet, not commit to achiev it. thi might occur becaus the agent ha not yet deliber about the effect that  adopt that intent might have on the other intent it is current hold. in such case, it might not consid itself to even be in an adversari environ. item 4 state that the agent should hold some belief about the profil of it adversari. the profil repres all the knowledg the agent ha about it adversari: it weak, strateg capabl, goal, intent, trustworthi, and more. it can be given explicitli or can be learn from observ of past encount. 2.2 model definit for mental state we us grosz and krau"s definit of the modal  oper, predic, and meta-predic, as defin in their sharedplan formal [4]. we recal here some of the predic and oper that ar us in that  formal: int.to(ai, α, tn, tα, c) repres ai"s intent at time tn to do an action α at time tα in the context of c. int.th(ai, prop, tn, tprop, c) repres ai"s intent at time tn that a certain proposit prop hold at time tprop in the context of c. the potenti intent  oper, pot.int.to(...) and pot.int.th(...), ar us to  repres the mental state when an agent consid adopt an intent, but ha not deliber about the interact of the other intent it hold. the oper bel(ai, f, tf ) repres agent ai believ in the statement express in formula f, at time tf . mb(a, f, tf ) repres mutual  belief for a group of agent a. a snapshot of the system find our environ to be in some state e ∈ e of environment variabl state, and each adversari in ani lai ∈ l of possibl local state. at ani given time step, the system will be in some world w of the set of all possibl world w ∈ w, where w = e×la1 ×la2 × ...lan , and n is the number of adversari. for exampl, in a texa hold"em poker game, an agent"s local state might be it own set of card (which is unknown to it adversari) while the environ will consist of the bet pot and the commun card (which ar visibl to both player). a util function under thi formal is defin as a map from a possibl world w ∈ w to an element in , which express the desir of the world, from a singl agent perspect. we usual normal the rang to [0,1], where 0 repres the least desir possibl world, and 1 is the most desir world. the implement of the util function is depend on the domain in question. the follow list specifi new predic, function,  variabl, and constant us in conjunct with the origin definit for the adversari environ formal: 1. φ is a null action (the agent doe not do anyth). 2. gai is the set of agent ai"s goal. each goal is a set of predic whose satisfact make the goal complet (we us g∗ ai ∈ gai to repres an arbitrari goal of agent ai). 3. gai is the set of agent ai"s subgoal. subgoal ar  predic whose satisfact repres an import mileston toward achiev of the full goal. gg∗ ai ⊆ gai is the set of subgoal that ar import to the complet of goal g∗ ai (we will us g∗ g∗ ai ∈ gg∗ ai to repres an arbitrari subgoal). 4. p aj ai is the profil object agent ai hold about agent aj. 5. ca is a gener set of action for all agent in a which ar deriv from the environ"s constraint. cai ⊆ ca is the set of agent ai"s possibl action. 6. do(ai, α, tα, w) hold when ai perform action α over time interv tα in world w. 7. achiev(g∗ ai , α, w) is true when goal g∗ ai is achiev  follow the complet of action α in world w ∈ w, where α ∈ cai . 8. profil(ai, pai ai ) is true when agent ai hold an object profil for agent aj. definit 1. full conflict (fulconf ) describ a  zerosum interact where onli a singl goal of the goal in  conflict can be complet. fulconf(g∗ ai , g∗ aj ) ⇒ (∃α ∈ cai , ∀w, β ∈ caj ) (achiev(g∗ ai , α, w) ⇒ ¬achiev(g∗ aj , β, w)) ∨ (∃β ∈ caj , ∀w, α ∈ cai )(achiev(g∗ aj , β, w) ⇒ ¬achiev(g∗ ai , α, w)) definit 2. adversari knowledg (advknow) is a function return a valu which repres the amount of the sixth intl. joint conf. on autonom agent and multi-agent system (aama 07) 551 knowledg agent ai ha on the profil of agent aj, at time tn. the higher the valu, the more knowledg agent ai ha. advknow : p aj ai × tn → definit 3. eval - thi evalu function return an estim expect util valu for an agent in a, after complet an action from ca in some world state w. eval : a × ca × w → definit 4. trh - (threshold) is a numer constant in the [0,1] rang that repres an evalu function (eval) threshold valu. an action that yield an estim util evalu abov the trh is regard as a highli  benefici action. the eval valu is an estim and not the real util function, which is usual unknown. us the real util valu for a ration agent would easili yield the best outcom for that agent. howev, agent usual do not have the real util function, but rather a heurist estim of it. there ar two import properti that should hold for the evalu function: properti 1. the evalu function should state that the most desir world state is on in which the goal is achiev. therefor, after the goal ha been satisfi, there can be no futur action that can put the agent in a world state with higher eval valu. (∀ai, g∗ ai , α, β ∈ cai , w ∈ w) achiev(g∗ ai , α, w) ⇒ eval(ai, α, w) ≥ eval(ai, β, w) properti 2. the evalu function should project an  action that caus a complet of a goal or a subgoal to a valu which is greater than trh (a highli benefici action). (∀ai, g∗ ai ∈ gai , α ∈ cai , w ∈ w, g∗ gai ∈ ggai ) achiev(g∗ ai , α, w) ∨ achiev(g∗ gai , α, w) ⇒ eval(ai, α, w) ≥ trh. definit 5. setact we defin a set action  (setact) as a set of action oper (either complex or basic action) from some action set cai and caj which,  accord to agent ai"s belief, ar attach togeth by a tempor and consequenti relationship, form a chain of event (action, and it follow consequ action). (∀α1 , . . . , αu ∈ cai , β1 , . . . , βv ∈ caj , w ∈ w) setact(α1 , . . . , αu , β1 , . . . , βv , w) ⇒ ((do(ai, α1 , tα1 , w) ⇒ do(aj, β1 , tβ1 , w)) ⇒ do(ai, α2 , tα2 , w) ⇒ . . . ⇒ do(ai, αu , tαu , w)) the consequenti relat might exist due to variou  environment constraint (when on action forc the  adversari to respond with a specif action) or due to the agent"s knowledg about the profil of it adversari. properti 3. as the knowledg we have about our  adversari increas we will have addit belief about it  behavior in differ situat which in turn creat new set action. formal, if our advknow at time tn+1 is greater than advknow at time tn, then everi setact known at time tn is also known at time tn+1. advknow(p aj ai , tn+1) > advknow(p aj ai , tn) ⇒ (∀α1 , . . . , αu ∈ cai , β1 , . . . , βv ∈ caj ) bel(aag, setact(α1 , . . . , αu , β1 , . . . , βv ), tn) ⇒ bel(aag, setact(α1 , . . . , αu , β1 , . . . , βv ), tn+1) 2.3 the environ formul the follow axiom provid the formal definit for a simpl, zero-sum adversari environ (ae).  satisfact of these axiom mean that the agent is situat in such an environ. it provid specif for agent aag to interact with it set of adversari a with respect to goal g∗ aag and g∗ a at time tco at some world state w. ae(aag, a, g∗ aag , a1, . . . , ak, g∗ a1 , . . . , g∗ ak , tn, w) 1. aag ha an int.th hi goal would be complet: (∃α ∈ caag , tα) int.th(aag, achiev(g∗ aag , α), tn, tα, ae) 2. aag believ that it and each of it adversari ao ar pursu full conflict goal: (∀ao ∈ {a1, . . . , ak}) bel(aag, fulconf(g∗ aag , g∗ ao ), tn) 3. aag believ that each of hi adversari in ao ha the int.th hi conflict goal g∗ aoi will be complet: (∀ao ∈ {a1, . . . , ak})(∃β ∈ cao , tβ) bel(aag, int.th(ao, achiev(g∗ ao , β), tco, tβ, ae), tn) 4. aag ha belief about the (partial) profil of it  adversari (∀ao ∈ {a1, . . . , ak}) (∃pao aag ∈ paag )bel(aag, profil(ao, pao aag ), tn) to build an agent that will be abl to oper successfulli within such an ae, we must specifi behavior guidelin for it interact. us a naiv eval maxim strategi to a certain search depth will not alwai yield satisfactori result for sever reason: (1) the search horizon problem when search for a fix depth; (2) the strong assumpt of an optim ration, unbound resourc adversari; (3) us an estim evalu function which will not give optim result in all world state, and can be exploit [9]. the follow axiom specifi the behavior principl that can be us to differenti between success and less success agent in the abov adversari environ. those axiom should be us as specif principl when design and implement agent that should be abl to perform well in such adversari environ. the  behavior axiom repres situat in which the agent will adopt potenti intent to (pot.int.to(...)) perform an action, which will typic requir some mean-end  reason to select a possibl cours of action. thi reason will lead to the adopt of an int.to(...) (see [4]). a1. goal achiev axiom. the first axiom is the  simplest case; when the agent aag believ that it is on action (α) awai from achiev hi conflict goal g∗ aag , it should adopt the potenti intent to do α and complet it goal. (∀aag, α ∈ caag , tn, tα, w ∈ w) (bel(aag, do(aag, α, tα, w) ⇒ achiev(g∗ aag , α, w)) ⇒ pot.int.to(aag, α, tn, tα, w) thi somewhat trivial behavior is the first and strongest axiom. in ani situat, when the agent is an action awai from complet the goal, it should complet the action. ani fair eval function would natur classifi α as the maxim valu action (properti 1). howev, without  explicit axiomat of such behavior there might be  situat where the agent will decid on take anoth action for variou reason, due to it bound decis resourc. a2. prevent act axiom. be in an adversari  situat, agent aag might decid to take action that will damag on of it adversari"s plan to complet it goal, even if those action do not explicitli advanc aag toward it conflict goal g∗ aag . such prevent action will take place when agent aag ha a belief about the possibl of it adversari ao do an action β that will give it a high 552 the sixth intl. joint conf. on autonom agent and multi-agent system (aama 07) util evalu valu (> trh). believ that take  action α will prevent the oppon from do it β, it will adopt a potenti intent to do α. (∀aag, ao ∈ a, α ∈ caag , β ∈ cao , tn, tβ, w ∈ w) (bel(aag, do(ao, β, tβ, w) ∧ eval(ao, β, w) > trh, tn) ∧ bel(aag, do(aag, α, tα, w) ⇒ ¬do(ao, β, tβ, w), tn) ⇒ pot.int.to(aag, α, tn, tα, w) thi axiom is a basic compon of ani adversari  environ. for exampl, look at a chess board game, a player could realiz that it is about to be checkmat by it oppon, thu make a prevent move. anoth  exampl is a connect four game: when a player ha a row of three chip, it oppon must block it, or lose. a specif instanc of a1 occur when the adversari is on action awai from achiev it goal, and immedi  prevent action need to be taken by the agent. formal, we have the same belief as state abov, with a chang belief that do action β will caus agent ao to achiev it goal. proposit 1: prevent or lose case. (∀aag, ao ∈ a, α ∈ caag , β ∈ cao , g∗ ao , tn, tα, tβ, w ∈ w) bel(aag, do(ao, β, tβ, w) ⇒ achiev(g∗ ao , β, w), tn) ∧ bel(aag, do(aag, α, tα, w) ⇒ ¬do(ao, β, tβ, w)) ⇒ pot.int.to(aag, α, tn, tα, w) sketch of proof: proposit 1 can be easili deriv from axiom a1 and the properti 2 of the eval function, which state that ani action that caus a complet of a goal is a highli benefici action. the prevent act behavior will occur implicitli when the eval function is equal to the real world util function. howev, be bound ration agent and deal with an estim evalu function we need to explicitli  axiomat such behavior, for it will not alwai occur implicitli from the evalu function. a3. suboptim tactic move axiom. in mani  scenario a situat mai occur where an agent will decid not to take the current most benefici action it can take (the action with the maxim util evalu valu), becaus it believ that take anoth action (with lower util  evalu valu) might yield (depend on the adversari"s  respons) a futur possibl for a highli benefici action. thi will occur most often when the eval function is  inaccur and differ by a larg extent from the util function. put formal, agent aag believ in a certain setact that will evolv accord to it initi action and will yield a high benefici valu (> trh) sole for it. (∀aag, ao ∈ a, tn, w ∈ w) (∃α1 , . . . , αu ∈ cai , β1 , . . . , βv ∈ caj , tα1 ) bel(aag, setact(α1 , . . . , αu , β1 , . . . , βv ), tn) ∧ bel(aag, eval(ao, βv , w) < trh < eval(aag, αu , w), tn) ⇒ pot.int.to(aag, α1 , tn, tα1 , w) an agent might believ that a chain of event will  occur for variou reason due to the inevit natur of the domain. for exampl, in chess, we often observ the  follow: a move caus a check posit, which in turn limit the oppon"s move to avoid the check, to which the first player might react with anoth check, and so on. the agent might also believ in a chain of event base on it  knowledg of it adversari"s profil, which allow it to forese the adversari"s movement with high accuraci. a4. profil detect axiom. the agent can adjust it adversari"s profil by observ and pattern studi (specif, if there ar repeat encount with the same adversari). howev, instead of wait for profil  inform to be reveal, an agent can also initi action that will forc it adversari to react in a wai that will reveal profil knowledg about it. formal, the axiom state that if all action (γ) ar not highli benefici action (< trh), the agent can do action α in time tα if it believ that it will result in a non-highli benefici action β from it adversari, which in turn teach it about the adversari"s profil, i.e., give a higher advknow(p aj ai , tβ). (∀aag, ao ∈ a, α ∈ caag , β ∈ cao , tn, tα, tβ, w ∈ w) bel(aag, (∀γ ∈ caag )eval(aag, γ, w) < trh, tn) ∧ bel(aag, do(aag, α, tα, w) ⇒ do(ao, β, tβ, w), tn) ∧ bel(aag, eval(ao, β, w) < trh) ∧ bel(aag, advknow(p aj ai , tβ) > advknow(p aj ai , tn), tn) ⇒ pot.int.to(aag, α, tn, tα, w) for exampl, go back to the chess board game  scenario, consid start a game versu an oppon about whom we know noth, not even if it is a human or a  computer oppon. we might start plai a strategi that will be suitabl versu an averag oppon, and adjust our game accord to it level of plai. a5. allianc format axiom the follow  behavior axiom is relev onli in a multilater instanti of the adversari environ (obvious, an allianc  cannot be form in a bilater, zero-sum encount). in  differ situat dure a multilater interact, a group of agent might believ that it is in their best interest to form a temporari allianc. such an allianc is an agreement that constrain it member" behavior, but is believ by it member to enabl them to achiev a higher util valu than the on achiev outsid of the allianc. as an exampl, we can look at the classic risk board game, where each player ha an individu goal of be the sole conquer of the world, a zero-sum game. howev, in order to achiev thi goal, it might be strateg wise to make short-term ceasefir agreement with other player, or to join forc and attack an oppon who is stronger than the rest. an allianc"s term defin the wai it member should act. it is a set of predic, denot as term, that is agre upon by the allianc member, and should remain true for the durat of the allianc. for exampl, the set term in the risk scenario, could contain the follow predic: 1. allianc member will not attack each other on territori x, y and z; 2. allianc member will contribut c unit per turn for attack adversari ao; 3. member ar oblig to stai as part of the allianc until time tk or until adversari"s ao armi is smaller than q. the set term specifi inter-group constraint on each of the allianc member"s (∀aal i ∈ aal ⊆ a) set of action cal i ⊆ c. definit 6. al val - the total evalu valu that agent ai will achiev while be part of aal is the sum of evali (eval for ai) of each of aal j eval valu after take their own α action (via the agent(α) predic): al val(ai, cal , aal , w) = α∈cal evali(aal j , agent(α), w) definit 7. al trh - is a number repres an al val the sixth intl. joint conf. on autonom agent and multi-agent system (aama 07) 553 threshold; abov it, the allianc can be said to be a highli benefici allianc. the valu of al trh will be calcul dynam  accord to the progress of the interact, as can be seen in [7]. after an allianc is form, it member ar now work in their normal adversari environ, as well as accord to the mental state and axiom requir for their  interact as part of the allianc. the follow allianc model (al) specifi the condit under which the group aal can be said to be in an allianc and work with a new and  constrain set of action cal , at time tn. al(aal , cal , w, tn) 1. aal ha a mb that all member ar part of aal : mb(aal , (∀aal i ∈ aal )member(aal i , aal ), tn) 2. aal ha a mb that the group be maintain: mb(aal , (∀aal i ∈ aal )int.th (ai, member(ai, aal ), tn, tn+1, co), tn) 3. aal ha a mb that be member give them high util valu: mb(aal , (∀aal i ∈ aal )al val(aal i , cal , aal , w) ≥ al trh, tn) member" profil ar a crucial part of success allianc. we assum that agent that have more accur profil of their adversari will be more success in such  environ. such agent will be abl to predict when a  member is about to breach the allianc"s contract (item 2 in the abov model), and take counter measur (when item 3 will falsifi). the robust of the allianc is in part a function of it member" trust measur, object posit  estim, and other profil properti. we should note that an agent can simultan be part of more than on allianc. such a temporari allianc, where the group member do not have a joint goal but act collabor for the interest of their own individu goal, is classifi as a treatment group by modern psychologist [12] (in contrast to a task group, where it member have a joint goal). the share activ model as present in [5] model treatment group behavior us the same sharedplan formal. when compar both definit of an allianc and a treatment group we found an unsurpris resembl  between both model: the environ model"s definit ar almost ident (see sa"s definit in [5]), and their selfish-act and cooper act axiom conform to our  adversari agent"s behavior. the main distinct between both model is the integr of a help-behavior act  axiom, in the share activ which cannot be part of our. thi axiom state that an agent will consid take action that will lower it eval valu (to a certain lower bound), if it believ that a group partner will gain a signific benefit. such behavior cannot occur in a pure adversari  environ (as a zero-sum game is), where the allianc member ar constantli on watch to manipul their allianc to their own advantag. a6. evalu maxim axiom. in a case when all other axiom ar inapplic, we will proce with the action that maxim the heurist valu as comput in the eval function. (∀aag, ao ∈ a, α ∈ cag, tn, w ∈ w) bel(aag, (∀γ ∈ cag)eval(aag, α, w) ≥ eval(aag, γ, w), tn) ⇒ pot.int.to(aag, α, tn, tα, w) t1. optim on eval = util the abov axiomat model handl situat where the util is unknown and the agent ar bound ration agent. the follow  theorem show that in bilater interact, where the agent have the real util function (i.e., eval = util) and ar ration agent, the axiom provid the same optim result as classic adversari search (e.g., min-max). theorem 1. let ae ag be an unbound ration ae agent us the eval heurist evalu function, au ag be the same agent us the true util function, and ao be a sole  unbound util-base ration adversari. given that eval = util: (∀α ∈ cau ag , α ∈ cae ag , tn, w ∈ w) pot.int.to(au ag, α, tn, tα, w) → pot.int.to(ae ag, α , tn, tα, w) ∧ ((α = α ) ∨ (util(au ag, α, w) = eval(ae ag, α , w))) sketch of proof - given that au ag ha the real util function and unbound resourc, it can gener the full game tree and run the optim minmax algorithm to choos the highest util valu action, which we denot by, α. the proof will show that ae ag, us the ae axiom, will select the same or equal util α (when there is more than on action with the same max util) when eval = util. (a1) goal achiev axiom - suppos there is an α such that it complet will achiev au ag"s goal. it will obtain the highest util by min-max for au ag. the ae ag agent will select α or anoth action with the same util valu via a1. if such α doe not exist, ae ag cannot appli thi axiom, and proce to a2. (a2) prevent act axiom - (1) look at the basic case (see prop1), if there is a β which lead ao to achiev it goal, then a prevent action α will yield the highest  util for au ag. au ag will choos it through the util, while ae ag will choos it through a2. (2) in the gener case, β is a highli benefici action for ao, thu yield low util for au ag, which will guid it to select an α that will prevent β, while ae ag will choos it through a2.1 if such β doe not exist for ao, then a2 is not applic, and ae ag can proce to a3. (a3) suboptim tactic move axiom - when us a heurist eval function, ae ag ha a partial belief in the profil of it adversari (item 4 in ae model), which mai lead it to believ in setact (prop1). in our case, ae ag is  hold a full profil on it optim adversari and know that ao will behav optim accord to the real util  valu on the complet search tree, therefor, ani belief about suboptim setact cannot exist, yield thi axiom  inapplic. ae ag will proce to a4. (a4) profil detect axiom - given that ae ag ha the full profil of ao, none of ae ag"s action can increas it  knowledg. that axiom will not be appli, and the agent will proce with a6 (a5 will be disregard becaus the  interact is bilater). (a6) evalu maxim axiom - thi axiom will  select the max eval for ae ag. given that eval = util, the same α that wa select by au ag will be select. 3. evalu the main purpos of our experiment analysi is to  evalu the model"s behavior and perform in a real  adversari environ. thi section investig whether bound 1 a case where follow the complet of β there exist a γ which give high util for agent au ag, cannot occur becaus ao us the same util, and γ"s exist will caus it to classifi β as a low util action. 554 the sixth intl. joint conf. on autonom agent and multi-agent system (aama 07) ration agent situat in such adversari environ will be better off appli our suggest behavior axiom. 3.1 the domain to explor the us of the abov model and it behavior axiom, we decid to us the connect-four game as our adversari environ. connect-four is a 2-player,  zerosum game which is plai us a 6x7 matrix-like board. each turn, a player drop a disc into on of the 7 column (the set of 21 disc is usual color yellow for player 1 and red for player 2; we will us white and black respect to avoid confus). the winner is the first player to complet a horizont, vertic, or diagon set of four disc with it color. on veri rare occas, the game might end in a tie if all the empti grid ar fill, but no player manag to creat a 4-disc set. the connect-four game wa solv in [1], where it is shown that the first player (plai with the white disc) can forc a win by start in the middl column (column 4) and plai optim howev, the optim strategi is veri complex, and difficult to follow even for complex bound ration agent, such as human player. befor we can proce check agent behavior, we must first verifi that the domain conform to the adversari  environ"s definit as given abov (which the behavior axiom ar base on). first, when plai a connect-four game, the agent ha an intent to win the game (item 1). second (item 2), our agent believ that in connect-four there can onli be on winner (or no winner at all in the rare occurr of a tie). in addit, our agent believ that it oppon to the game will try to win (item 3), and we hope it ha some partial knowledg (item 4) about it adversari (thi knowledg can vari from noth, through simpl fact such as ag, to strategi and weak). of cours, not all connect-four encount ar  adversari. for exampl, when a parent is plai the game with it child, the follow situat might occur: the child, have a strong incent to win, treat the environ as  adversari (it intend to win, understand that there can onli be on winner, and believ that it parent is try to beat him). howev, the parent"s point of view might see the environ as an educ on, where it goal is not to win the game, but to caus enjoy or practic strateg reason. in such an educ environ, a new set of behavior axiom might be more benefici to the parent"s goal than our suggest adversari behavior axiom. 3.2 axiom analysi after show that the connect-four game is inde a zero-sum, bilater adversari environ, the next step is to look at player" behavior dure the game and check whether behav accord to our model doe improv  perform. to do so we have collect log file from  complet connect-four game that were plai by human  player over the internet. our collect log file data came from plai by email (pbem) site. these ar web site that host email game, where each move is taken by an email  exchang between the server and the player. mani such site" archiv contain real competit interact, and also maintain a rank system for their member. most of the data we us can be found in [6]. as can be learn from [1], connect-four ha an optim strategi and a consider advantag for the player who start the game (which we call the white player). we will concentr in our analysi on the second player"s move (to be call black). the white player, be the first to act, ha the so-call initi advantag. have the advantag and a good strategi will keep the black player busi react to it move, instead of initi threat. a threat is a combin of three disc of the same color, with an empti spot for the fourth win disk. an open threat is a threat that can be realiz in the oppon"s next move. in order for the black player to win, it must somehow turn the tide, take the advantag and start present threat to the white player. we will explor black player" behavior and their conform to our axiom. to do so, we built an applic that read log file and analyz the black player"s move. the applic contain two main compon: (1) a min-max algorithm for  evalu of move; (2) open threat detector for the discov of open threat. the min-max algorithm will work to a given depth, d and for each move α will output the heurist valu for the next action taken by the player as written in the log file, h(α), alongsid the maximum heurist valu, maxh(α), that could be achiev prior to take the move (obvious, if h(α) = maxh(α), then the player did not do the optim move heurist). the threat detector"s job is to notifi if some action wa taken in order to block an open threat (not block an open threat will probabl caus the player to lose in the oppon"s next move). the heurist function us by min-max to evalu the player"s util is the follow function, which is simpl to comput, yet provid a reason challeng to human  oppon: definit 8. let group be an adjac set of four squar that ar horizont, vertic, or diagon. groupn b (groupn w) be a group with n piec of the black (white) color and 4−n empti squar. h = ((group1 b ∗α)+(group2 b ∗β)+(group3 b ∗γ)+(group4 b ∗∞)) − ((group1 w ∗α)+(group2 w ∗β)+(group3 w ∗γ)+(group4 w ∗∞)) the valu of α, β and δ can vari to form ani desir linear combin; howev, it is import to valu them with the α < β < δ order in mind (we us 1, 4, and 8 as their respect valu). group of 4 disc of the same color mean victori, thu discoveri of such a group will result in ∞ to ensur an extrem valu. we now us our estim evalu function to evalu the black player"s action dure the connect-four  adversari interact. each game from the log file wa input into the applic, which process and output a reformat log file contain the h valu of the current move, the maxh valu that could be achiev, and a notif if an open threat wa detect. a total of 123 game were analyz (57 with white win, and 66 with black win). a few addit game were manual ignor in the  experi, due to these problem: a player abandon the game while the outcom is not final, or a blunt irrat move in the earli stage of the game (e.g., not block an obviou win group in the first open move). in addit, a singl tie game wa also remov. the simul wa run to a search depth of 3 move. we now proce to analyz the game with respect to each behavior axiom. the sixth intl. joint conf. on autonom agent and multi-agent system (aama 07) 555 tabl 1: averag heurist differ analysi black loss black won avg" minh -17.62 -12.02 avg" 3 lowest h move (min3 h) -13.20 -8.70 3.2.1 affirm the suboptim tactic move axiom the follow section present the heurist evalu of the min-max algorithm for each action, and check the amount and extent of suboptim tactic action and their implic on perform. tabl 1 show result and insight from the game"  heurist analysi, when search depth equal 3 (thi search depth wa select for the result to be compar to [9], see  section 3.2.3). the tabl"s heurist data is the differ  between the present maxim heurist valu and the heurist valu of the action that wa eventu taken by the player (i.e., the closer the number is to 0, the closer the action wa to the maximum heurist action). the first row present the differ valu of the  action that had the maxim differ valu among all the black player"s action in a given game, as averag over all black"s win and lose game (see respect column). in game in which the black player lose, it averag  differ valu wa -17.62, while in game in which the black player won, it averag wa -12.02. the second row expand the analysi by consid the 3 highest heurist differ action, and averag them. in that case, we notic an  averag heurist differ of 5 point between game which the black player lose and game in which it win. nevertheless, the import of those number is that thei allow us to take an educ guess on a threshold number of 11.5, as the valu of the trh constant, which differenti between normal action and highli benefici on. after find an approxim trh constant, we can  proce with an analysi of the import of suboptim move. to do so we took the subset of game in which the minimum heurist differ valu for black"s action wa 11.5. as present in tabl 2, we can see the differ min3 h  averag of the 3 largest rang and the respect percentag of game won. the first row show that the black player won onli 12% of the game in which the averag of it 3 highest heurist differ action (min3 h) wa smaller than the suggest threshold, trh = 11.5. the second row show a surpris result: it seem that when min3 h > −4 the black player rare win. intuit would suggest that game in which the action evalu valu were closer to the maxim valu will result in more win game for black. howev, it seem that in the connect-four domain, mere respond with somewhat easili expect action, without initi a few surpris and suboptim move, doe not yield good result. the last row sum up the main insight from the analysi; most of black"s win (83%) came when it min3 h wa in the rang of -11.5 to -4. a close inspect of those black win game show the follow pattern behind the number:  after standard open move, black suddenli drop a disc into an isol column, which seem a wast of a move. white continu to build it threat, while usual  disregard black"s last move, which in turn us the isol disc as an anchor for a futur win threat. the result show that it wa benefici for the black player tabl 2: black"s win percentag % of game min3 h < −11.5 12% min3 h > −4 5% −11.5 ≤ min3 h ≤ −4 83% to take suboptim action and not give the current  highest possibl heurist valu, but will not be too harm for it posit (i.e., will not give high benefici valu to it adversari). as it turn out, learn the threshold is an import aspect of success: take wildli riski move (min3 h < −11.5) or try to avoid them (min3 h > −4)  reduc the black player"s win chanc by a larg margin. 3.2.2 affirm the profil monitor axiom in the task of show the import of monitor on"s adversari" profil, our log file could not be us becaus thei did not contain repeat interact between player, which ar need to infer the player" knowledg about their adversari. howev, the import of oppon  model and it us in attain tactic advantag wa alreadi studi in variou domain ([3, 9] ar good exampl). in a recent paper, markovitch and reger [9] explor the notion of learn and exploit of oppon weak in competit interact. thei appli simpl learn  strategi by analyz exampl from past interact in a  specif domain. thei also us the connect-four adversari domain, which can now be us to understand the  import of monitor the adversari"s profil. follow the present of their theoret model, thei describ an extens empir studi and check the agent"s perform after learn the weak model with past exampl. on of the domain us as a competit  environ wa the same connect-four game (checker wa the second domain). their heurist function wa ident to our with three differ variat (h1, h2, and h3) that ar distinguish from on anoth in their linear  combin coeffici valu. the search depth for the player wa 3 (as in our analysi). their extens experi check and compar variou learn strategi, risk factor,  predefin featur set and usag method. the bottom line is that the connect-four domain show an improv from a 0.556 win rate befor model to a 0.69 after  model (page 22). their conclus, show improv  perform when hold and us the adversari"s model, justifi the effort to monitor the adversari profil for  continu and repeat interact. an addit point that came up in their experi is the follow: after the oppon weak model ha been learn, the author describ differ method of  integr the oppon weak model into the agent"s decis strategi. nevertheless, regardless of the specif method thei chose to work with, all integr method might caus the agent to take suboptim decis; it might caus the agent to prefer action that ar suboptim at the present decis junction, but which might caus the oppon to react in accord with it weak model (as repres by our agent) which in turn will be benefici for us in the futur. the agent"s behavior, as demonstr in [9] further confirm and strengthen our suboptim tactic axiom as discuss in the previou section. 556 the sixth intl. joint conf. on autonom agent and multi-agent system (aama 07) 3.2.3 addit insight the need for the goal achiev, prevent act, and evalu maxim axiom ar obviou, and need no further verif. howev, even with respect to those  axiom, a few interest insight came up in the log analysi. the goal achiev and prevent act axiom, though  theoret trivial, seem to provid some challeng to a human player. in the initi inspect of the log, we encount few game2 where a player, for inexplic reason, did not block the other from win or fail to execut it own win move. we can blame those fault on the human"s lack of attent, or a type error in it move repli;  nevertheless, those error might occur in bound ration agent, and the appropri behavior need to be axiomat. a typic connect-four game revolv around gener threat and block them. in our analysi we look for explicit prevent action, i.e., move that block a group of 3 disc, or that remov a futur threat (in our limit search horizon). we found that in 83% of the total game there wa at least on prevent action taken by the black player. it wa also found that black averag 2.8 prevent action per game on the game in which it lost, while averag 1.5 prevent action per game when win. it seem that black requir 1 or 2 prevent action to build it initi take posit, befor start to present threat. if it did not manag to win, it will usual prevent an extra threat or two befor succumb to white. 4. relat work much research deal with the axiomat of teamwork and mental state of individu: some model us  knowledg and belief [10], other have model of goal and  intent [8, 4]. howev, all these formal theori deal with agent teamwork and cooper. as far as we know, our model is the first to provid a formal model for explicit adversari environ and agent" behavior in it. the classic min-max adversari search algorithm wa the first attempt to integr the oppon into the search space with a weak assumpt of an optim plai  oppon. sinc then, much effort ha gone into integr the oppon model into the decis procedur to predict futur behavior. the m∗ algorithm present by carmel and markovitch [2] show a method of incorpor  oppon model into adversari search, while in [3] thei us learn to provid a more accur oppon model in a  2player repeat game environ, where agent" strategi were model as finit automata. addit adversari plan work wa done by willmott et al. [13], which  provid an adversari plan approach to the game of go. the research mention abov dealt with adversari search and the integr of oppon model into classic  utilitybas search method. that work show the import of oppon model and the abil to exploit it to an agent"s advantag. howev, the basic limit of those search method still appli; our model tri to overcom those  limit by present a formal model for a new, mental state-base adversari specif. 5. conclus we present an adversari environ model for a 2 these were later remov from the final analysi. bound ration agent that is situat in an n-player,  zerosum environ. we us the sharedplan formal to defin the model and the axiom that agent can appli as behavior guidelin. the model is meant to be us as a guidelin for  design agent that need to oper in such adversari  environ. we present empir result, base on  connectfour log file analysi, that exemplifi the model and the axiom for a bilater instanc of the environ. the result we present ar a first step toward an  expand model that will cover all type of adversari  environ, for exampl, environ that ar non-zero-sum, and environ that contain natur agent that ar not part of the direct conflict. those challeng and more will be dealt with in futur research. 6. acknowledg thi research wa support in part by israel scienc foundat grant #1211/04 and #898/05. 7. refer [1] l. v. alli. a knowledg-base approach of connect-four - the game is solv: white win. master"s thesi, free univers, amsterdam, the netherland, 1988. [2] d. carmel and s. markovitch. incorpor oppon model into adversari search. in proceed of the thirteenth nation confer on artifici intellig, page 120-125, portland, or, 1996. [3] d. carmel and s. markovitch. oppon model in multi-agent system. in g. weiß and s. sen, editor, adapt and learn in multi-agent system, page 40-52. springer-verlag, 1996. [4] b. j. grosz and s. krau. collabor plan for complex group action. artifici intellig, 86(2):269-357, 1996. [5] m. hadad, g. kaminka, g. armon, and s. krau. support collabor activ. in proc. of aaai-2005, page 83-88, pittsburgh, 2005. [6] http://www.gamerz.net/˜pbmserv/. [7] s. krau and d. lehmann. design and build a negoti autom agent. comput intellig, 11:132-171, 1995. [8] h. j. levesqu, p. r. cohen, and j. h. t. nune. on act togeth. in proc. of aaai-90, page 94-99, boston, ma, 1990. [9] s. markovitch and r. reger. learn and exploit rel weak of oppon agent. autonom agent and multi-agent system, 10(2):103-130, 2005. [10] y. m. ronald fagin, joseph y. halpern and m. y. vardi. reason about knowledg. mit press, cambridg, mass., 1995. [11] p. thagard. adversari problem solv: model an opon us explanatori coher. cognit scienc, 16(1):123-149, 1992. [12] r. w. toseland and r. f. riva. an introduct to group work practic. prentic hall, englewood cliff, nj, 2nd edit edit, 1995. [13] s. willmott, j. richardson, a. bundi, and j. levin. an adversari plan approach to go. lectur note in comput scienc, 1558:93-112, 1999. the sixth intl. joint conf. on autonom agent and multi-agent system (aama 07) 557 