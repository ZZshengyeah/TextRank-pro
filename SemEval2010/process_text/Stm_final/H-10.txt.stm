regular cluster for document ∗ fei wang, changshui zhang state kei lab of intellig tech. and system depart of autom, tsinghua univers beij, china, 100084 feiwang03@gmail.com tao li school of comput scienc florida intern univers miami, fl 33199, u.s.a. taoli@cs.fiu.edu abstract in recent year, document cluster ha been receiv more and more attent as an import and fundament  techniqu for unsupervis document organ, automat topic extract, and fast inform retriev or filter. in thi paper, we propos a novel method for cluster  document us regular. unlik tradit global  regular cluster method, our method first construct a local regular linear label predictor for each document vector, and then combin all those local regular with a global smooth regular. so we call our algorithm cluster with local and global regular (clgr). we will show that the cluster membership of the  document can be achiev by eigenvalu decomposit of a spars symmetr matrix, which can be effici solv by iter method. final our experiment evalu on sever dataset ar present to show the superior of clgr over tradit document cluster method. categori and subject descriptor h.3.3 [inform storag and retriev]: inform search and retriev-cluster; i.2.6 [artifici  intellig]: learn-concept learn gener term algorithm 1. introduct document cluster ha been receiv more and more attent as an import and fundament techniqu for unsupervis document organ, automat topic  extract, and fast inform retriev or filter. a good document cluster approach can assist the comput to automat organ the document corpu into a  meaning cluster hierarchi for effici brows and navig, which is veri valuabl for complement the defici of tradit inform retriev technolog. as point out by [8], the inform retriev need can be express by a spectrum rang from narrow keyword-match base search to broad inform brows such as what ar the major intern event in recent month. tradit document retriev engin tend to fit well with the search end of the spectrum, i.e. thei usual provid specifi search for document match the user"s queri, howev, it is hard for them to meet the need from the rest of the spectrum in which a rather broad or vagu inform is need. in such case, effici brows through a good cluster hierarchi will be definit help. gener, document cluster method can be mainli categor into two class: hierarch method and  partit method. the hierarch method group the data point into a hierarch tree structur us bottom-up or top-down approach. for exampl, hierarch  agglom cluster (hac) [13] is a typic bottom-up  hierarch cluster method. it take each data point as a singl cluster to start off with and then build bigger and bigger cluster by group similar data point togeth until the entir dataset is encapsul into on final cluster. on the other hand, partit method decompos the dataset into a number of disjoint cluster which ar usual  optim in term of some predefin criterion function. for  instanc, k-mean [13] is a typic partit method which aim to minim the sum of the squar distanc between the data point and their correspond cluster center. in thi paper, we will focu on the partit method. as we know that there ar two main problem exist in partit method (like kmean and gaussian  mixtur model (gmm) [16]): (1) the predefin criterion is  usual non-convex which caus mani local optim solut; (2) the iter procedur (e.g. the expect  maxim (em) algorithm) for optim the criterion usual make the final solut heavili depend on the  initi. in the last decad, mani method have been  propos to overcom the abov problem of the partit method [19][28]. recent, anoth type of partit method base on cluster on data graph have arous consider  interest in the machin learn and data mine commun. the basic idea behind these method is to first model the whole dataset as a weight graph, in which the graph node repres the data point, and the weight on the edg  correspond to the similar between pairwis point. then the cluster assign of the dataset can be achiev by optim some criterion defin on the graph. for  exampl spectral cluster is on kind of the most repres graph-base cluster approach, it gener aim to  optim some cut valu (e.g. normal cut [22], ratio cut [7], min-max cut [11]) defin on an undirect graph. after some relax, these criterion can usual be optim via eigen-decomposit, which is guarante to be global optim. in thi wai, spectral cluster effici avoid the problem of the tradit partit method as we introduc in last paragraph. in thi paper, we propos a novel document cluster  algorithm that inherit the superior of spectral cluster, i.e. the final cluster result can also be obtain by exploit the eigen-structur of a symmetr matrix. howev, unlik spectral cluster, which just enforc a smooth  constraint on the data label over the whole data manifold [2], our method first construct a regular linear label  predictor for each data point from it neighborhood as in [25], and then combin the result of all these local label  predictor with a global label smooth regular. so we call our method cluster with local and global regular (clgr). the idea of incorpor both local and global inform into label predict is inspir by the recent work on semi-supervis learn [31], and our  experiment evalu on sever real document dataset show that clgr perform better than mani state-of-the-art cluster method. the rest of thi paper is organ as follow: in section 2 we will introduc our clgr algorithm in detail. the  experiment result on sever dataset ar present in section 3, follow by the conclus and discuss in section 4. 2. the propos algorithm in thi section, we will introduc our cluster with local and global regular (clgr) algorithm in detail. first let"s see the how the document ar repres throughout thi paper. 2.1 document represent in our work, all the document ar repres by the weight term-frequenc vector. let w = {w1, w2, · · · , wm} be the complet vocabulari set of the document corpu (which is preprocess by the stopword remov and word stem oper). the term-frequenc vector xi of  document di is defin as xi = [xi1, xi2, · · · , xim]t , xik = tik log n idfk , where tik is the term frequenc of wk ∈ w, n is the size of the document corpu, idfk is the number of document that contain word wk. in thi wai, xi is also call the  tfidf represent of document di. furthermor, we also normal each xi (1 i n) to have a unit length, so that each document is repres by a normal tf-idf vector. 2.2 local regular as it name suggest, clgr is compos of two part: local regular and global regular. in thi  subsect we will introduc the local regular part in detail. 2.2.1 motiv as we know that cluster is on type of learn  techniqu, it aim to organ the dataset in a reason wai. gener speak, learn can be pose as a problem of function estim, from which we can get a good  classif function that will assign label to the train dataset and even the unseen test dataset with some cost  minim [24]. for exampl, in the two-class classif  scenario1 (in which we exactli know the label of each  document), a linear classifi with least squar fit aim to learn a column vector w such that the squar cost j = 1 n (wt xi − yi)2 (1) is minim, where yi ∈ {+1, −1} is the label of xi. by take ∂j /∂w = 0, we get the solut w∗ = n i=1 xixt i −1 n i=1 xiyi , (2) which can further be written in it matrix form as w∗ = xxt −1 xy, (3) where x = [x1, x2, · · · , xn] is an m × n document matrix, y = [y1, y2, · · · , yn]t is the label vector. then for a test document t, we can determin it label by l = sign(w∗t u), (4) where sign(·) is the sign function. a natur problem in eq.(3) is that the matrix xxt mai be singular and thu not invert (e.g. when m n). to avoid such a problem, we can add a regular term and minim the follow criterion j = 1 n n i=1 (wt xi − yi)2 + λ w 2 , (5) where λ is a regular paramet. then the optim solut that minim j is given by w∗ = xxt + λni −1 xy, (6) where i is an m × m ident matrix. it ha been report that the regular linear classifi can achiev veri good result on text classif problem [29]. howev, despit it empir success, the regular  linear classifi is on earth a global classifi, i.e. w∗ is  estim us the whole train set. accord to [24], thi mai not be a smart idea, sinc a uniqu w∗ mai not be good enough for predict the label of the whole input space. in order to get better predict, [6] propos to train  classifi local and us them to classifi the test point. for exampl, a test point will be classifi by the local  classifi train us the train point locat in the vicin 1 in the follow discuss we all assum that the  document come from onli two class. the gener of our method to multi-class case will be discuss in section 2.5. of it. although thi method seem slow and stupid, it is report that it can get better perform than us a uniqu global classifi on certain task [6]. 2.2.2 construct the local regular predictor inspir by their success, we propos to appli the local learn algorithm for cluster. the basic idea is that, for each document vector xi (1 i n), we train a local label predictor base on it k-nearest neighborhood ni, and then us it to predict the label of xi. final we will combin all those local predictor by minim the sum of their predict error. in thi subsect we will introduc how to construct those local predictor. due to the simplic and effect of the regular linear classifi that we have introduc in section 2.2.1, we choos it to be our local label predictor, such that for each document xi, the follow criterion is minim ji = 1 ni xj ∈ni wt i xj − qj 2 + λi wi 2 , (7) where ni = |ni| is the cardin of ni, and qj is the  cluster membership of xj. then us eq.(6), we can get the optim solut is w∗ i = xixt i + λinii −1 xiqi, (8) where xi = [xi1, xi2, · · · , xini ], and we us xik to denot the k-th nearest neighbor of xi. qi = [qi1, qi2, · · · , qini ]t with qik repres the cluster assign of xik. the problem here is that xixt i is an m × m matrix with m ni, i.e. we should comput the invers of an m × m matrix for  everi document vector, which is computation prohibit. fortun, we have the follow theorem: theorem 1. w∗ i in eq.(8) can be rewritten as w∗ i = xi xt i xi + λiniii −1 qi, (9) where ii is an ni × ni ident matrix. proof. sinc w∗ i = xixt i + λinii −1 xiqi, then xixt i + λinii w∗ i = xiqi =⇒ xixt i w∗ i + λiniw∗ i = xiqi =⇒ w∗ i = (λini)−1 xi qi − xt i w∗ i . let β = (λini)−1 qi − xt i w∗ i , then w∗ i = xiβ =⇒ λiniβ = qi − xt i w∗ i = qi − xt i xiβ =⇒ qi = xt i xi + λiniii β =⇒ β = xt i xi + λiniii −1 qi. therefor w∗ i = xiβ = xi xt i xi + λiniii −1 qi 2 us theorem 1, we onli need to comput the invers of an ni × ni matrix for everi document to train a local label predictor. moreov, for a new test point u that fall into ni, we can classifi it by the sign of qu = w∗t i u = ut wi = ut xi xt i xi + λiniii −1 qi. thi is an attract express sinc we can determin the cluster assign of u by us the inner-product between the point in {u ∪ ni}, which suggest that such a local regular can easili be kernel [21] as long as we defin a proper kernel function. 2.2.3 combin the local regular predictor after all the local predictor have been construct, we will combin them togeth by minim jl = n i=1 w∗t i xi − qi 2 , (10) which stand for the sum of the predict error for all the local predictor. combin eq.(10) with eq.(6), we can get jl = n i=1 w∗t i xi − qi 2 = n i=1 xt i xi xt i xi + λiniii −1 qi − qi 2 = pq − q 2 , (11) where q = [q1, q2, · · · , qn]t , and the p is an n × n matrix construct in the follow wai. let αi = xt i xi xt i xi + λiniii −1 , then pij = αi j, if xj ∈ ni 0, otherwis , (12) where pij is the (i, j)-th entri of p, and αi j repres the j-th entri of αi . till now we can write the criterion of cluster by  combin local regular linear label predictor jl in an explicit mathemat form, and we can minim it directli us some standard optim techniqu. howev, the result mai not be good enough sinc we onli exploit the local inform of the dataset. in the next subsect, we will introduc a global regular criterion and combin it with jl, which aim to find a good cluster result in a local-global wai. 2.3 global regular in data cluster, we usual requir that the cluster  assign of the data point should be suffici smooth with respect to the underli data manifold, which impli (1) the nearbi point tend to have the same cluster  assign; (2) the point on the same structur (e.g.  submanifold or cluster) tend to have the same cluster assign [31]. without the loss of gener, we assum that the data point resid (roughli) on a low-dimension manifold m2 , and q is the cluster assign function defin on m, i.e. 2 we believ that the text data ar also sampl from some low dimension manifold, sinc it is imposs for them to for ∀x ∈ m, q(x) return the cluster membership of x. the smooth of q over m can be calcul by the follow dirichlet integr [2] d[q] = 1 2 m q(x) 2 dm, (13) where the gradient q is a vector in the tangent space t mx, and the integr is taken with respect to the standard  measur on m. if we restrict the scale of q by q, q m = 1 (where ·, · m is the inner product induc on m), then it turn out that find the smoothest function  minim d[q] reduc to find the eigenfunct of the laplac beltrami oper l, which is defin as lq −div q, (14) where div is the diverg of a vector field. gener, the graph can be view as the discret form of manifold. we can model the dataset as an weight  undirect graph as in spectral cluster [22], where the graph node ar just the data point, and the weight on the edg repres the similar between pairwis point. then it can be shown that minim eq.(13) correspond to  minim jg = qt lq = n i=1 (qi − qj)2 wij, (15) where q = [q1, q2, · · · , qn]t with qi = q(xi), l is the graph laplacian with it (i, j)-th entri lij =    di − wii, if i = j −wij, if xi and xj ar adjac 0, otherwis, (16) where di = j wij is the degre of xi, wij is the similar between xi and xj. if xi and xj ar adjac3 , wij is usual comput in the follow wai wij = e − xi−xj 2 2σ2 , (17) where σ is a dataset depend paramet. it is prove that under certain condit, such a form of wij to determin the weight on graph edg lead to the converg of graph laplacian to the laplac beltrami oper [3][18]. in summari, us eq.(15) with exponenti weight can effect measur the smooth of the data assign with respect to the intrins data manifold. thu we adopt it as a global regular to punish the smooth of the predict data assign. 2.4 cluster with local and global regular combin the content we have introduc in section 2.2 and section 2.3 we can deriv the cluster criterion is minq j = jl + λjg = pq − q 2 + λqt lq s.t. qi ∈ {−1, +1}, (18) where p is defin as in eq.(12), and λ is a regular paramet to trade off jl and jg. howev, the discret fill in the whole high-dimension sampl space. and it ha been shown that the manifold base method can achiev good result on text classif task [31]. 3 in thi paper, we defin xi and xj to be adjac if xi ∈ n(xj) or xj ∈ n(xi). constraint of pi make the problem an np hard integ  program problem. a natur wai for make the problem solvabl is to remov the constraint and relax qi to be  continu, then the object that we aim to minim becom j = pq − q 2 + λqt lq = qt (p − i)t (p − i)q + λqt lq = qt (p − i)t (p − i) + λl q, (19) and we further add a constraint qt q = 1 to restrict the scale of q. then our object becom minq j = qt (p − i)t (p − i) + λl q s.t. qt q = 1 (20) us the lagrangian method, we can deriv that the  optim solut q correspond to the smallest eigenvector of the matrix m = (p − i)t (p − i) + λl, and the cluster  assign of xi can be determin by the sign of qi, i.e. xi will be classifi as class on if qi > 0, otherwis it will be classifi as class 2. 2.5 multi-class clgr in the abov we have introduc the basic framework of cluster with local and global regular (clgr) for the two-class cluster problem, and we will extend it to multi-class cluster in thi subsect. first we assum that all the document belong to c class index by l = {1, 2, · · · , c}. qc is the classif  function for class c (1 c c), such that qc (xi) return the confid that xi belong to class c. our goal is to obtain the valu of qc (xi) (1 c c, 1 i n), and the cluster assign of xi can be determin by {qc (xi)}c c=1 us some proper discret method that we will introduc later. therefor, in thi multi-class case, for each document xi (1 i n), we will construct c local linear regular label predictor whose normal vector ar wc∗ i = xi xt i xi + λiniii −1 qc i (1 c c), (21) where xi = [xi1, xi2, · · · , xini ] with xik be the k-th  neighbor of xi, and qc i = [qc i1, qc i2, · · · , qc ini ]t with qc ik = qc (xik). then (wc∗ i )t xi return the predict confid of xi  belong to class c. henc the local predict error for class c can be defin as j c l = n i=1 (wc∗ i ) t xi − qc i 2 , (22) and the total local predict error becom jl = c c=1 j c l = c c=1 n i=1 (wc∗ i ) t xi − qc i 2 . (23) as in eq.(11), we can defin an n×n matrix p (see eq.(12)) and rewrit jl as jl = c c=1 j c l = c c=1 pqc − qc 2 . (24) similarli we can defin the global smooth regular in multi-class case as jg = c c=1 n i=1 (qc i − qc j )2 wij = c c=1 (qc )t lqc . (25) then the criterion to be minim for clgr in multi-class case becom j = jl + λjg = c c=1 pqc − qc 2 + λ(qc )t lqc = c c=1 (qc )t (p − i)t (p − i) + λl qc = trace qt (p − i)t (p − i) + λl q , (26) where q = [q1 , q2 , · · · , qc ] is an n × c matrix, and trace(·) return the trace of a matrix. the same as in eq.(20), we also add the constraint that qt q = i to restrict the scale of q. then our optim problem becom minq j = trace qt (p − i)t (p − i) + λl q s.t. qt q = i, (27) from the ky fan theorem [28], we know the optim solut of the abov problem is q∗ = [q∗ 1, q∗ 2, · · · , q∗ c ]r, (28) where q∗ k (1 k c) is the eigenvector correspond to the k-th smallest eigenvalu of matrix (p − i)t (p − i) + λl, and r is an arbitrari c × c matrix. sinc the valu of the entri in q∗ is continu, we need to further discret q∗ to get the cluster assign of all the data point. there ar mainli two approach to achiev thi goal: 1. as in [20], we can treat the i-th row of q as the  embed of xi in a c-dimension space, and appli some tradit cluster method like kmean to  cluster these embed into c cluster. 2. sinc the optim q∗ is not uniqu (becaus of the exist of an arbitrari matrix r), we can pursu an optim r that will rotat q∗ to an indic matrix4 . the detail algorithm can be refer to [26]. the detail algorithm procedur for clgr is summar in tabl 1. 3. experi in thi section, experi ar conduct to empir compar the cluster result of clgr with other 8  representit document cluster algorithm on 5 dataset. first we will introduc the basic inform of those dataset. 3.1 dataset we us a varieti of dataset, most of which ar frequent us in the inform retriev research. tabl 2  summar the characterist of the dataset. 4 here an indic matrix t is a n×c matrix with it (i,  j)th entri tij ∈ {0, 1} such that for each row of q∗ there is onli on 1. then the xi can be assign to the j-th cluster such that j = argjq∗ ij = 1. tabl 1: cluster with local and global  regular (clgr) input: 1. dataset x = {xi}n i=1; 2. number of cluster c; 3. size of the neighborhood k; 4. local regular paramet {λi}n i=1; 5. global regular paramet λ; output: the cluster membership of each data point. procedur: 1. construct the k nearest neighborhood for each data point; 2. construct the matrix p us eq.(12); 3. construct the laplacian matrix l us eq.(16); 4. construct the matrix m = (p − i)t (p − i) + λl; 5. do eigenvalu decomposit on m, and construct the matrix q∗ accord to eq.(28); 6. output the cluster assign of each data point by properli discret q∗ . tabl 2: descript of the document dataset dataset number of document number of class cstr 476 4 webkb4 4199 4 reuter 2900 10 webac 2340 20 newsgroup4 3970 4 cstr. thi is the dataset of the abstract of technic report publish in the depart of comput scienc at a univers. the dataset contain 476 abstract, which were divid into four research area: natur languag process(nlp), robot/vision, system, and theori. webkb. the webkb dataset contain webpag  gather from univers comput scienc depart. there ar about 8280 document and thei ar divid into 7  categori: student, faculti, staff, cours, project, depart and other. the raw text is about 27mb. among these 7 categori, student, faculti, cours and project ar four most popul entiti-repres categori. the  associ subset is typic call webkb4. reuter. the reuter-21578 text categor test collect contain document collect from the reuter newswir in 1987. it is a standard text categor  benchmark and contain 135 categori. in our experi, we us a subset of the data collect which includ the 10 most frequent categori among the 135 topic and we call it reuter-top 10. webac. the webac dataset wa from webac project and ha been us for document cluster [17][5]. the  webac dataset contain 2340 document consist new  articl from reuter new servic via the web in octob 1997. these document ar divid into 20 class. new4. the new4 dataset us in our experi ar select from the famou 20-newsgroup dataset5 . the topic rec contain auto, motorcycl, basebal and hockei wa select from the version 20new-18828. the new4 dataset contain 3970 document vector. 5 http://peopl.csail.mit.edu/jrenni/20newsgroup/ to pre-process the dataset, we remov the stop word us a standard stop list, all html tag ar skip and all header field except subject and organ of the post articl ar ignor. in all our experi, we first select the top 1000 word by mutual inform with class label. 3.2 evalu metric in the experi, we set the number of cluster equal to the true number of class c for all the cluster  algorithm. to evalu their perform, we compar the cluster gener by these algorithm with the true class by comput the follow two perform measur. cluster accuraci (acc). the first perform  measur is the cluster accuraci, which discov the  on-toon relationship between cluster and class and measur the extent to which each cluster contain data point from the correspond class. it sum up the whole match  degre between all pair class-cluster. cluster accuraci can be comput as: acc = 1 n max   ck,lm t(ck, lm)   , (29) where ck denot the k-th cluster in the final result, and lm is the true m-th class. t(ck, lm) is the number of entiti which belong to class m ar assign to cluster k.  accuraci comput the maximum sum of t(ck, lm) for all pair of cluster and class, and these pair have no overlap. the greater cluster accuraci mean the better cluster perform. normal mutual inform (nmi). anoth  evalu metric we adopt here is the normal mutual  inform nmi [23], which is wide us for determin the qualiti of cluster. for two random variabl x and y, the nmi is defin as: nmi(x, y) = i(x, y) h(x)h(y) , (30) where i(x, y) is the mutual inform between x and y, while h(x) and h(y) ar the entropi of x and y respect. on can see that nmi(x, x) = 1, which is the maxim possibl valu of nmi. given a cluster result, the nmi in eq.(30) is estim as nmi = c k=1 c m=1 nk,mlog n·nk,m nk ˆnm c k=1 nklog nk n c m=1 ˆnmlog ˆnm n , (31) where nk denot the number of data contain in the cluster ck (1 k c), ˆnm is the number of data belong to the m-th class (1 m c), and nk,m denot the number of data that ar in the intersect between the cluster ck and the m-th class. the valu calcul in eq.(31) is us as a perform measur for the given cluster result. the larger thi valu, the better the cluster perform. 3.3 comparison we have conduct comprehens perform  evalu by test our method and compar it with 8 other repres data cluster method us the same data corpora. the algorithm that we evalu ar list below. 1. tradit k-mean (km). 2. spheric k-mean (skm). the implement is base on [9]. 3. gaussian mixtur model (gmm). the implement is base on [16]. 4. spectral cluster with normal cut (ncut). the implement is base on [26], and the varianc of the gaussian similar is determin by local scale [30]. note that the criterion that ncut aim to  minim is just the global regular in our clgr  algorithm except that ncut us the normal laplacian. 5. cluster us pure local regular (cplr). in thi method we just minim jl (defin in eq.(24)), and the cluster result can be obtain by do eigenvalu decomposit on matrix (i − p)t (i − p) with some proper discret method. 6. adapt subspac iter (asi). the  implement is base on [14]. 7. nonneg matrix factor (nmf). the  implement is base on [27]. 8. tri-factor nonneg matrix factor (tnmf) [12]. the implement is base on [15]. for comput effici, in the implement of cplr and our clgr algorithm, we have set all the local regular paramet {λi}n i=1 to be ident, which is set by grid search from {0.1, 1, 10}. the size of the k-nearest neighborhood is set by grid search from {20, 40, 80}. for the clgr method, it global regular paramet is set by grid search from {0.1, 1, 10}. when construct the global regular, we have adopt the local scale method [30] to construct the laplacian matrix. the final  discret method adopt in these two method is the same as in [26], sinc our experi show that us such method can achiev better result than us kmean base method as in [20]. 3.4 experiment result the cluster accuraci comparison result ar shown in tabl 3, and the normal mutual inform comparison result ar summar in tabl 4. from the two tabl we mainli observ that: 1. our clgr method outperform all other document cluster method in most of the dataset; 2. for document cluster, the spheric k-mean method usual outperform the tradit k-mean cluster method, and the gmm method can achiev  competit result compar to the spheric k-mean method; 3. the result achiev from the k-mean and gmm type algorithm ar usual wors than the result achiev from spectral cluster. sinc spectral cluster can be view as a weight version of kernel k-mean, it can obtain good result the data cluster ar arbitrarili shape. thi corrobor that the document vector ar not regularli distribut (spheric or ellipt). 4. the experiment comparison empir verifi the equival between nmf and spectral cluster, which tabl 3: cluster accuraci of the variou  method cstr webkb4 reuter webac new4 km 0.4256 0.3888 0.4448 0.4001 0.3527 skm 0.4690 0.4318 0.5025 0.4458 0.3912 gmm 0.4487 0.4271 0.4897 0.4521 0.3844 nmf 0.5713 0.4418 0.4947 0.4761 0.4213 ncut 0.5435 0.4521 0.4896 0.4513 0.4189 asi 0.5621 0.4752 0.5235 0.4823 0.4335 tnmf 0.6040 0.4832 0.5541 0.5102 0.4613 cplr 0.5974 0.5020 0.4832 0.5213 0.4890 clgr 0.6235 0.5228 0.5341 0.5376 0.5102 tabl 4: normal mutual inform result of the variou method cstr webkb4 reuter webac new4 km 0.3675 0.3023 0.4012 0.3864 0.3318 skm 0.4027 0.4155 0.4587 0.4003 0.4085 gmm 0.4034 0.4093 0.4356 0.4209 0.3994 nmf 0.5235 0.4517 0.4402 0.4359 0.4130 ncut 0.4833 0.4497 0.4392 0.4289 0.4231 asi 0.5008 0.4833 0.4769 0.4817 0.4503 tnmf 0.5724 0.5011 0.5132 0.5328 0.4749 cplr 0.5695 0.5231 0.4402 0.5543 0.4690 clgr 0.6012 0.5434 0.4935 0.5390 0.4908 ha been prove theoret in [10]. it can be  observ from the tabl that nmf and spectral  cluster usual lead to similar cluster result. 5. the co-cluster base method (tnmf and asi) can usual achiev better result than tradit pure document vector base method. sinc these method perform an implicit featur select at each iter, provid an adapt metric for measur the  neighborhood, and thu tend to yield better cluster result. 6. the result achiev from cplr ar usual better than the result achiev from spectral cluster, which support vapnik"s theori [24] that sometim local learn algorithm can obtain better result than global learn algorithm. besid the abov comparison experi, we also test the paramet sensibl of our method. there ar mainli two set of paramet in our clgr algorithm, the local and global regular paramet ({λi}n i=1 and λ, as we have said in section 3.3, we have set all λi"s to be ident to λ∗ in our experi), and the size of the neighborhood. therefor we have also done two set of experi: 1. fix the size of the neighborhood, and test the cluster perform with vari λ∗ and λ. in thi set of experi, we find that our clgr algorithm can achiev good result when the two regular paramet ar neither too larg nor too small.  typic our method can achiev good result when λ∗ and λ ar around 0.1. figur 1 show us such a  test exampl on the webac dataset. 2. fix the local and global regular paramet, and test the cluster perform with differ −5 −4.5 −4 −3.5 −3 −5 −4.5 −4 −3.5 −3 0.35 0.4 0.45 0.5 0.55 local regular para (log 2 valu) global regular para (log 2 valu) clusteringaccuraci figur 1: paramet sensibl test result on the webac dataset with the neighborhood size fix to 20, and the x-axi and y-axi repres the log2 valu of λ∗ and λ. size of neighborhood. in thi set of experi, we find that the neighborhood with a too larg or too small size will all deterior the final cluster result. thi can be easili understood sinc when the neighborhood size is veri small, then the data point us for train the local classifi mai not be suffici; when the neighborhood size is veri larg, the train classifi will tend to be global and  cannot captur the typic local characterist. figur 2 show us a test exampl on the webac dataset. therefor, we can see that our clgr algorithm (1) can achiev satisfactori result and (2) is not veri sensit to the choic of paramet, which make it practic in real world applic. 4. conclus and futur work in thi paper, we deriv a new cluster algorithm call cluster with local and global regular. our method preserv the merit of local learn algorithm and spectral cluster. our experi show that the propos  algorithm outperform most of the state of the art algorithm on mani benchmark dataset. in the futur, we will focu on the paramet select and acceler issu of the clgr algorithm. 5. refer [1] l. baker and a. mccallum. distribut cluster of word for text classif. in proceed of the intern acm sigir confer on research and develop in inform retriev, 1998. [2] m. belkin and p. niyogi. laplacian eigenmap for dimension reduct and data represent. neural comput, 15 (6):1373-1396. june 2003. [3] m. belkin and p. niyogi. toward a theoret foundat for laplacian-base manifold method. in proceed of the 18th confer on learn theori (colt). 2005. 10 20 30 40 50 60 70 80 90 100 0.35 0.4 0.45 0.5 0.55 size of the neighborhood clusteringaccuraci figur 2: paramet sensibl test result on the webac dataset with the regular  paramet be fix to 0.1, and the neighborhood size vare from 10 to 100. [4] m. belkin, p. niyogi and v. sindhwani. manifold regular: a geometr framework for learn from exampl. journal of machin learn research 7, 1-48, 2006. [5] d. bolei. princip direct divis partit. data mine and knowledg discoveri, 2:325-344, 1998. [6] l. bottou and v. vapnik. local learn algorithm. neural comput, 4:888-900, 1992. [7] p. k. chan, d. f. schlag and j. y. zien. spectral k-wai ratio-cut partit and cluster. ieee tran. comput-aid design, 13:1088-1096, sep. 1994. [8] d. r. cut, d. r. karger, j. o. pederson and j. w. tukei. scatter/gather: a cluster-base approach to brows larg document collect. in proceed of the intern acm sigir confer on research and develop in inform retriev, 1992. [9] i. s. dhillon and d. s. modha. concept decomposit for larg spars text data us cluster. machin learn, vol. 42(1), page 143-175, januari 2001. [10] c. ding, x. he, and h. simon. on the equival of nonneg matrix factor and spectral cluster. in proceed of the siam data mine confer, 2005. [11] c. ding, x. he, h. zha, m. gu, and h. d. simon. a min-max cut algorithm for graph partit and data cluster. in proc. of the 1st intern confer on data mine (icdm), page 107-114, 2001. [12] c. ding, t. li, w. peng, and h. park. orthogon nonneg matrix tri-factor for cluster. in proceed of the twelfth acm sigkdd intern confer on knowledg discoveri and data mine, 2006. [13] r. o. duda, p. e. hart, and d. g. stork. pattern classif. john wilei & son, inc., 2001. [14] t. li, s. ma, and m. ogihara. document cluster via adapt subspac iter. in proceed of the intern acm sigir confer on research and develop in inform retriev, 2004. [15] t. li and c. ding. the relationship among variou nonneg matrix factor method for cluster. in proceed of the 6th intern confer on data mine (icdm). 2006. [16] x. liu and y. gong. document cluster with cluster refin and model select capabl. in proc. of the intern acm sigir confer on research and develop in inform retriev, 2002. [17] e. han, d. bolei, m. gini, r. gross, k. hast, g. karypi, v. kumar, b. mobash, and j. moor. webac: a web agent for document categor and explor. in proceed of the 2nd intern confer on autonom agent (agent98). acm press, 1998. [18] m. hein, j. y. audibert, and u. von luxburg. from graph to manifold - weak and strong pointwis consist of graph laplacian. in proceed of the 18th confer on learn theori (colt), 470-485. 2005. [19] j. he, m. lan, c.-l. tan, s.-y. sung, and h.-b. low. initi of cluster refin algorithm: a review and compar studi. in proc. of inter. joint confer on neural network, 2004. [20] a. y. ng, m. i. jordan, y. weiss. on spectral cluster: analysi and an algorithm. in advanc in neural inform process system 14. 2002. [21] b. sch¨olkopf and a. smola. learn with kernel. the mit press. cambridg, massachusett. 2002. [22] j. shi and j. malik. normal cut and imag segment. ieee tran. on pattern analysi and machin intellig, 22(8):888-905, 2000. [23] a. strehl and j. ghosh. cluster ensembl - a knowledg reus framework for combin multipl partit. journal of machin learn research, 3:583-617, 2002. [24] v. n. vapnik. the natur of statist learn theori. berlin: springer-verlag, 1995. [25] wu, m. and sch¨olkopf, b. a local learn approach for cluster. in advanc in neural inform process system 18. 2006. [26] s. x. yu, j. shi. multiclass spectral cluster. in proceed of the intern confer on comput vision, 2003. [27] w. xu, x. liu and y. gong. document cluster base on non-neg matrix factor. in proceed of the intern acm sigir confer on research and develop in inform retriev, 2003. [28] h. zha, x. he, c. ding, m. gu and h. simon. spectral relax for k-mean cluster. in nip 14. 2001. [29] t. zhang and f. j. ol. text categor base on regular linear classif method. journal of inform retriev, 4:5-31, 2001. [30] l. zelnik-manor and p. perona. self-tune spectral cluster. in nip 17. 2005. [31] d. zhou, o. bousquet, t. n. lal, j. weston and b. sch¨olkopf. learn with local and global consist. nip 17, 2005. 