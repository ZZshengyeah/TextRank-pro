term feedback for inform retriev with languag model bin tan† , atulya velivelli‡ , hui fang† , chengxiang zhai† dept. of comput scienc† , dept. of electr and comput engin‡ univers of illinoi at urbana-champaign bintan@cs.uiuc.edu, velivel@ifp.uiuc.edu, hfang@cs.uiuc.edu, czhai@cs.uiuc.edu abstract in thi paper we studi term-base feedback for inform  retriev in the languag model approach. with term feedback a user directli judg the relev of individu term without  interact with feedback document, take full control of the queri expans process. we propos a cluster-base method for  select term to present to the user for judgment, as well as effect algorithm for construct refin queri languag model from user term feedback. our algorithm ar shown to bring signific improv in retriev accuraci over a non-feedback baselin, and achiev compar perform to relev feedback. thei ar help even when there ar no relev document in the top. categori and subject descriptor h.3.3 [inform search and retriev]: retriev model gener term algorithm 1. introduct in the languag model approach to inform retriev,  feedback is often model as estim an improv queri model or relev model base on a set of feedback document [25, 13]. thi is in line with the tradit wai of do relev feedback - present a user with document/passag for relev  judgment and then extract term from the judg document or  passag to expand the initi queri. it is an indirect wai of seek user"s assist for queri model construct, in the sens that the refin queri model (base on term) is learn through feedback document/passag, which ar high-level structur of term. it ha the disadvantag that irrelev term, which occur along with relev on in the judg content, mai be erron us for queri expans, caus undesir effect. for exampl, for the trec queri hubbl telescop achiev, when a relev document talk more about the telescop"s repair than it  discoveri, irrelev term such as spacewalk can be ad into the modifi queri. we can consid a more direct wai to involv a user in queri model improv, without an intermediari step of document feedback that can introduc nois. the idea is to present a  (reason) number of individu term to the user and ask him/her to judg the relev of each term or directli specifi their  probabl in the queri model. thi strategi ha been discuss in [15], but to our knowledg, it ha not been serious studi in exist languag model literatur. compar to tradit relev feedback, thi term-base approach to interact queri model  refin ha sever advantag. first, the user ha better  control of the final queri model through direct manipul of term: he/she can dictat which term ar relev, irrelev, and  possibl, to what degre. thi avoid the risk of bring unwant term into the queri model, although sometim the user introduc low-qualiti term. second, becaus a term take less time to judg than a document"s full text or summari, and as few as around 20 present term can bring signific improv in retriev  perform (as we will show later), term feedback make it faster to gather user feedback. thi is especi help for interact  adhoc search. third, sometim there ar no relev document in the top n of the initi retriev result if the topic is hard. thi is often true when n is constrain to be small, which aris from the fact that the user is unwil to judg too mani document. in thi case, relev feedback is useless, as no relev document can be leverag on, but term feedback is still often help, by allow relev term to be pick from irrelev document. dure our particip in the trec 2005 hard track and continu studi afterward, we explor how to exploit term  feedback from the user to construct improv queri model for  inform retriev in the languag model approach. we identifi two kei subtask of term-base feedback, i.e., pre-feedback  present term select and post-feedback queri model  construct, with effect algorithm develop for both. we impos a secondari cluster structur on term and found that a cluster view shed addit insight into the user"s inform need, and  provid a good wai of util term feedback. through experi we found that term feedback improv significantli over the  nonfeedback baselin, even though the user often make mistak in relev judgment. among our algorithm, the on with best  retriev perform is tcfb, the combin of tfb, the direct term feedback algorithm, and cfb, the cluster-base feedback  algorithm. we also vari the number of feedback term and  observ reason improv even at low number. final, by compar term feedback with document-level feedback, we found it to be a viabl altern to the latter with competit retriev perform. the rest of the paper is organ as follow. section 2 discuss some relat work. section 4 outlin our gener approach to term feedback. we present our method for present term select in section 3 and algorithm for queri model construct in section 5. the experi result ar given in section 6. section 7 conclud thi paper. 2. relat work relev feedback[17, 19] ha long been recogn as an  effect method for improv retriev perform. normal, the top n document retriev us the origin queri ar present to the user for judgment, after which term ar extract from the judg relev document, weight by their potenti of  attract more relev document, and ad into the queri model. the expand queri usual repres the user"s inform need  better than the origin on, which is often just a short keyword queri. a second iter of retriev us thi modifi queri usual produc signific increas in retriev accuraci. in case where true relev judgment is unavail and all top n document ar assum to be relev, it is call blind or pseudo feedback[5, 16] and usual still bring perform improv. becaus document is a larg text unit, when it is us for  relev feedback mani irrelev term can be introduc into the feedback process. to overcom thi, passag feedback is propos and shown to improv feedback perform[1, 23]. a more direct solut is to ask the user for their relev judgment of feedback term. for exampl, in some relev feedback system such as [12], there is an interact step that allow the user to add or  remov expans term after thei ar automat extract from relev document. thi is categor as interact queri  expans, where the origin queri is augment with user-provid term, which can come from direct user input (free-form text or keyword)[22, 7, 10] or user select of system-suggest term (us thesauri[6, 22] or extract from feedback document[6, 22, 12, 4, 7]). in mani case term relev feedback ha been found to  effect improv retriev perform[6, 22, 12, 4, 10]. for  exampl, the studi in [12] show that the user prefer to have explicit knowledg and direct control of which term ar us for queri  expans, and the penetr interfac that provid thi freedom is shown to perform better than other interfac. howev, in some other case there is no signific benefit[3, 14], even if the user like interact with expans term. in a simul studi  carri out in [18], the author compar the retriev perform of interact queri expans and automat queri expans with a simul studi, and suggest that the potenti benefit of the  former can be hard to achiev. the user is found to be not good at identifi us term for queri expans, when a simpl term present interfac is unabl to provid suffici semant  context of the feedback term. our work differ from the previou on in two import  aspect. first, when we choos term to present to the user for  relev judgment, we not onli consid singl-term valu (e.g., the rel frequenc of a term in the top document, which can be measur by metric such as robertson select valu and  simplifi kullback-leibler distanc as list in [24]), but also  examin the cluster structur of the term, so as to produc a balanc coverag of the differ topic aspect. second, with the languag model framework, we allow an elabor construct of the updat queri model, by set differ probabl for differ term base on whether it is a queri term, it signific in the top document, and it cluster membership. although techniqu for adjust queri term weight exist for vector space model[17] and probablist relev model[9], most of the aforement work do not us them, choos to just append feedback term to the origin queri (thu us equal weight for them), which can lead to poorer retriev perform. the combin of the two aspect allow our method to perform much better than the  baselin. the usual wai for feedback term present is just to displai the term in a list. there have been some work on altern user interfac. [8] arrang term in a hierarchi, and [11] compar three differ interfac, includ term + checkbox, term + context (sentenc) + checkbox, sentenc + input text box. in both studi, howev, there is no signific perform  differ. in our work we adopt the simplest approach of term +  checkbox. we focu on term present and queri model  construct from feedback term, and believ us context to improv feedback term qualiti should be orthogon to our method. 3. gener approach we follow the languag model approach, and base our method on the kl-diverg retriev model propos in [25]. with thi model, the retriev task involv estim a queri languag model θq from a given queri, a document languag model θd from each document, and calcul their kl-diverg d(θq||θd), which is then us to score the document. [25] treat relev feedback as a queri model re-estim problem, i.e., comput an updat queri model θq given the origin queri text and the extra evid carri by the judg relev document. we adopt thi view, and cast our task as updat the queri model from user term feedback. there ar two kei subtask here: first, how to choos the best term to present to the user for judgment, in order to gather maxim  evid about the user"s inform need. second, how to comput an updat queri model base on thi term feedback evid, so that it captur the user"s inform need and translat into good retriev perform. 4. present term select proper select of term to be present to the user for  judgment is crucial to the success of term feedback. if the term ar poorli chosen and there ar few relev on, the user will have a hard time look for us term to help clarifi hi/her  inform need. if the relev term ar plenti, but all concentr on a singl aspect of the queri topic, then we will onli be abl to get feedback on that aspect and miss other, result in a breadth loss in retriev result. therefor, it is import to carefulli select present term to maxim expect gain from user feedback, i.e., those that can potenti reveal most evid of the user"s inform need. thi is similar to activ feedback[21], which suggest that a retriev system should activ probe the user"s  inform need, and in the case of relev feedback, the feedback document should be chosen to maxim learn benefit (e.g.  divers so as to increas coverag). in our approach, the top n document from an initi retriev us the origin queri form the sourc of feedback term: all term that appear in them ar consid candid to present to the user. these document serv as pseudo-feedback, sinc thei provid a much richer context than the origin queri (usual veri short), while the user is not ask to judg their relev. due to the latter reason, it is possibl to make n quit larg (e.g., in our experi we set n = 60) to increas it coverag of differ aspect in the topic. the simplest wai of select feedback term is to choos the most frequent m term from the n document. thi method,  howev, ha two drawback. first, a lot of common noisi term will be select due to their high frequenc in the document collect, unless a stop-word list is us for filter. second, the  present list will tend to be fill by term from major aspect of the topic; those from a minor aspect ar like to be miss due to their rel low frequenc. we solv the abov problem by two correspond measur. first, we introduc a background model θb that is estim from collect statist and explain the common term, so that thei ar much less like to appear in the present list. second, the term ar select from multipl cluster in the pseudo-feedback document, to ensur suffici represent of differ aspect of the topic. we reli on the mixtur multinomi model, which is us for theme discoveri in [26]. specif, we assum the n document contain k cluster {ci| i = 1, 2, · · · k}, each character by a multinomi word distribut (also known as unigram languag model) θi and correspond to an aspect of the topic. the  document ar regard as sampl from a mixtur of k + 1  compon, includ the k cluster and the background model: p(w|d) = λbp(w|θb) + (1 − λb) k i=1 πd,ip(w|θi) where w is a word, λb is the mixtur weight for the background model θb, and πd,i is the document-specif mixtur weight for the i-th cluster model θi. we then estim the cluster model by  maxim the probabl of the pseudo-feedback document be gener from the multinomi mixtur model: log p(d|Λ) = d∈d w∈v c(w; d) log p(w|d) where d = {di| i = 1, 2, · · · n} is the set of the n document, v is the vocabulari, c(w; d) is w"s frequenc in d and Λ = {θi| i = 1, 2, · · · k} ∪ {πdij | i = 1, 2, · · · n, j = 1, 2, · · · k} is the set of model paramet to estim. the cluster model can be  effici estim us the expect-maxim (em)  algorithm. for it detail, we refer the reader to [26]. tabl 1 show the cluster model for trec queri transport tunnel disast (k = 3). note that onli the middl cluster is relev. tabl 1: cluster model for topic 363 transport tunnel disast cluster 1 cluster 2 cluster 3 tunnel 0.0768 tunnel 0.0935 tunnel 0.0454 transport 0.0364 fire 0.0295 transport 0.0406 traffic 0.0206 truck 0.0236 toll 0.0166 railwai 0.0186 french 0.0220 amtrak 0.0153 harbor 0.0146 smoke 0.0157 train 0.0129 rail 0.0140 car 0.0154 airport 0.0122 bridg 0.0139 italian 0.0152 turnpik 0.0105 kilomet 0.0136 firefight 0.0144 lui 0.0095 truck 0.0133 blaze 0.0127 jersei 0.0093 construct 0.0131 blanc 0.0121 pass 0.0087 · · · · · · · · · from each of the k estim cluster, we choos the l = m/k term with highest probabl to form a total of m  present term. if a term happen to be in top l in multipl cluster, we assign it to the cluster where it ha highest probabl and let the other cluster take on more term as compens. we also filter out term in the origin queri text becaus thei tend to alwai be relev when the queri is short. the select term ar then  present to the user for judgment. a sampl (complet) feedback form is shown in figur 1. in thi studi we onli deal with binari judgment: a present term is by default uncheck, and a user mai check it to  indic relev. we also do not explicitli exploit neg feedback (i.e., penal irrelev term), becaus with binari feedback an uncheck term is not necessarili irrelev (mayb the user is  unsur about it relev). we could ask the user for finer  judgment (e.g., choos from highli relev, somewhat relev, do not know, somewhat irrelev and highli irrelev), but binari feedback is more compact, take less space to displai and less user effort to make judgment. 5. estim queri model from term feedback in thi section, we present sever algorithm for exploit term feedback. the algorithm take as input the origin queri q, the cluster {θi} as gener by the theme discoveri algorithm, the set of feedback term t and their relev judgment r, and output an updat queri languag model θq that make best us of the feedback evid to captur the user"s inform need. first we describ our notat: • θq: the origin queri model, deriv from queri term onli: p(w|θq) = c(w; q) |q| where c(w; q) is the count of w in q, and |q| = w∈q c(w; q) is the queri length. • θq : the updat queri model which we need to estim from term feedback. • θi (i = 1, 2, . . . k): the unigram languag model of cluster ci, as estim us the theme discoveri algorithm. • t = {ti,j} (i = 1 . . . k, j = 1 . . . l): the set of term  present to the user for judgment. ti,j is the j-th term chosen from cluster ci. • r = {δw|w ∈ t}: δw is an indic variabl that is 1 if w is judg relev or 0 otherwis. 5.1 tfb (direct term feedback) thi is a straight-forward form of term feedback that doe not involv ani secondari structur. we give a weight of 1 to term judg relev by the user, a weight of μ to queri term, zero weight to other term, and then appli normal: p(w|θq ) = δw + μ c(w; q) w ∈t δw + μ|q| where w ∈t δw is the total number of term that ar judg  relev. we call thi method tfb (direct term feedback). if we let μ = 1, thi approach is equival to append the relev term after the origin queri, which is what standard queri expans (without term reweight) doe. if we set μ > 1, we ar put more emphasi on the queri term than the check on. note that the result model will be more bias toward θq if the origin queri is long or the user feedback is weak, which make sens, as we can trust more on the origin queri in either case. figur 1: fill clarif form for topic 363 363 transport tunnel disast pleas select all term that ar relev to the topic. traffic railwai harbor rail bridg kilomet construct swiss cross link kong hong river project meter shanghai fire truck french smoke car italian firefight blaze blanc mont victim franc rescu driver chamonix emerg toll amtrak train airport turnpik lui jersei pass rome z center electron road boston speed bu submit 5.2 cfb (cluster feedback) here we exploit the cluster structur that plai an import role when we select the present term. the cluster  repres differ aspect of the queri topic, each of which mai or mai not be relev. if we ar abl to identifi the relev cluster, we can combin them to gener a queri model that is good at discov document belong to these cluster (instead of the irrelev on). we could ask the user to directli judg the  relev of a cluster after view repres term in that cluster, but thi would sometim be a difficult task for the user, who ha to guess the semant of a cluster via it set of term, which mai not be well connect to on anoth due to a lack of context.  therefor, we propos to learn cluster feedback indirectli, infer the relev of a cluster through the relev of it feedback term. becaus each cluster ha an equal number of term present to the user, the simplest measur of a cluster"s relev is the number of term that ar judg relev in it. intuit, the more term ar mark relev in a cluster, the closer the cluster is to the queri topic, and the more the cluster should particip in queri  modif. if we combin the cluster model us weight determin thi wai and then interpol with the origin queri model, we get the follow formula for queri updat, which we call cfb (cluster feedback): p(w|θq ) = λp(w|θq) + (1 − λ) k i=1 l j=1 δti,j k k=1 l j=1 δtk,j p(w|θi) where l j=1 δti,j is the number of relev term in cluster ci, and k k=1 l j=1 δtk,j is the total number of relev term. we note that when there is onli on cluster (k = 1), the abov formula degener to p(w|θq ) = λp(w|θq) + (1 − λ)p(w|θ1) which is mere pseudo-feedback of the form propos in [25]. 5.3 tcfb (term-cluster feedback) tfb and cfb both have their drawback. tfb assign non-zero probabl to the present term that ar mark relev, but complet ignor (a lot more) other, which mai be left uncheck due to the user"s ignor, or simpli not includ in the  present list, but we should be abl to infer their relev from the check on. for exampl, in figur 1, sinc as mani as 5 term in the middl cluster (the third and fourth column) ar check, we should have high confid in the relev of other term in that cluster. cfb remedi tfb"s problem by treat the term in a cluster collect, so that uncheck/unpres term  receiv weight when present term in their cluster ar judg as relev, but it doe not distinguish which term in a cluster ar present or judg. intuit, the judg relev term should receiv larger weight becaus thei ar explicitli indic as  relev by the user. therefor, we try to combin the two method, hope to get the best out of both. we do thi by interpol the tfb model with the cfb model, and call it tcfb: p(w|θq ) = αp(w|θqt f b ) + (1 − α)p(w|θqcf b ) 6. experi in thi section, we describ our experi result. we first  describ our experi setup and present an overview of variou method" perform. then we discuss the effect of vari the paramet set in the algorithm, as well as the number of present term. next we analyz user term feedback behavior and it relat to retriev perform. final we compar term feedback to relev feedback and show that it ha it particular advantag. 6.1 experi setup and basic result we took the opportun of trec 2005 hard track[2] for the evalu of our algorithm. the track us the aquaint  collect, a 3gb corpu of english newswir text. the topic  includ 50 on previous known to be hard, i.e. with low retriev perform. it is for these hard topic that user feedback is most help, as it can provid inform to disambigu the queri; with easi topic the user mai be unwil to spend effort for feedback if the automat retriev result ar good enough.  particip of the track were abl to submit custom-design clarif form (cf) to solicit feedback from human assessor provid by tabl 2: retriev perform for differ method and cf type. the last row is the percentag of map improv over the baselin. the paramet set μ = 4, λ = 0.1, α = 0.3 ar near optim. baselin tfb1c tfb3c tfb6c cfb1c cfb3c cfb6c tcfb1c tcfb3c tcfb6c map 0.219 0.288 0.288 0.278 0.254 0.305 0.301 0.274 0.309 0.304 pr@30 0.393 0.467 0.475 0.457 0.399 0.480 0.473 0.431 0.491 0.473 rr 4339 4753 4762 4740 4600 4907 4872 4767 4947 4906 % 0% 31.5% 31.5% 26.9% 16.0% 39.3% 37.4% 25.1% 41.1% 38.8% tabl 3: map variat with the number of present term. # term tfb1c tfb3c tfb6c cfb3c cfb6c tcfb3c tcfb6c 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 nist. we design three set of clarif form for term  feedback, differ in the choic of k, the number of cluster, and l, the number of present term from each cluster. thei ar: 1× 48, a big cluster with 48 term, 3 × 16, 3 cluster with 16 term each, and 6 × 8, 6 cluster with 8 term each. the total number of  present term (m) is fix at 48, so by compar the perform of differ type of clarif form we can know the effect of differ degre of cluster. for each topic, an assessor would complet the form order by 6 × 8, 1 × 48 and 3 × 16, spend up to three minut on each form. the sampl clarif form shown in figur 1 is of type 3 × 16. it is a simpl and compact interfac in which the user can check relev term. the form is self-explanatori; there is no need for extra user train on how to us it. our initin queri ar construct onli us the topic titl descript, which ar on averag 2.7 word in length. as our baselin we us the kl diverg retriev method implement in the lemur toolkit1 with 5 pseudo-feedback document. we stem the term, choos dirichlet smooth with a prior of 2000, and truncat queri languag model to 50 term (these set ar us throughout the experi). for all other paramet we us lemur"s default set. the baselin turn out to perform abov averag among the track particip. after an initi run us thi baselin retriev method, we take the top 60 document for each topic and appli the theme discoveri algorithm to output the  cluster (1, 3, or 6 of them), base on which we gener clarif form. after user feedback is receiv, we run the term feedback algorithm (tfb, cfb or tcfb) to estim updat queri  model, which ar then us for a second iter of retriev. we evalu the differ retriev method" perform on their rank of the top 1000 document. the evalu metric we adopt includ mean averag (non-interpol) precis (map), precis at top 30 (pr@30) and total relev retriev (rr). tabl 2 show the perform of variou method and configur of k × l. the suffix (1c, 3c, 6c) after tfb,cfb,tcfb stand for the number of cluster (k). for exampl, tcfb3c mean the tcfb method on the 3 × 16 clarif form. from tabl 2 we can make the follow observ: 1 http://www.lemurproject.com 1. all method perform consider better than the  pseudofeedback baselin, with tcfb3c achiev a highest 41.1% improv in map, indic signific contribut of term feedback for clarif of the user"s inform need. in other word, term feedback is truli help for improv retriev accuraci. 2. for tfb, the perform is almost equal on the 1 × 48 and 3 × 16 clarif form in term of map (although the latter is slightli better in pr@30 and rr), and a littl wors on the 6 × 8 on. 3. both cfb3c and cfb6c perform better than their tfb  counterpart in all three metric, suggest that feedback on a secondari cluster structur is inde benefici. cfb1c is actual wors becaus it cannot adjust the weight of it  (singl) cluster from term feedback and it is mere  pseudofeedback. 4. although tcfb is just a simpl mixtur of tfb and cfb by interpol, it is abl to outperform both. thi support our specul that tcfb overcom the drawback of tfb (pai attent onli to check term) and cfb (not  distinguish check and uncheck term in a cluster).  except for tcfb6c v.s. cfb6c, the perform advantag of tcfb over tfb/cfb is signific at p < 0.05 us the wilcoxon sign rank test. thi is not true in the case of tfb v.s. cfb, each of which is better than the other in nearli half of the topic. 6.2 reduct of present term in some situat we mai have to reduc the number of  present term due to limit in displai space or user feedback effort. it is interest to know whether our algorithm" perform  deterior when the user is present with fewer term. becaus the present term within each cluster ar gener in decreas order of their frequenc, the present list form a subset of the origin on if it size is reduc2 . therefor, we can easili  simul what happen when the number of present term decreas 2 there ar complex aris from term appear in top l of multipl cluster, but these ar except from m to m : we will keep all judgment of the top l = m /k term in each cluster and discard those of other. tabl 3 show the perform of variou algorithm as the number of present term rang from 6 to 48. we find that the perform of tfb is more suscept to  present term reduct than that of cfb or tcfb. for exampl, at 12 term the map of tfb3c is 90.6% of that at 48 term, while the number for cfb3c and tcfb3c ar 98.0% and 96.1%  respect. we conjectur the reason to be that while tfb"s  perform heavili depend on how mani good term ar chosen for queri expans, cfb onli need a rough estim of cluster weight to work. also, the 3 × 16 clarif form seem to be more robust than the 6 × 8 on: at 12 term the map of tfb6c is 87.1% of that at 48 term, lower than 90.6% for tfb3c. similarli, for cfb it is 95.0% against 98.0%. thi is natual, as for a larg cluster number of 6, it is easier to get into the situat where each cluster get too few present term to make topic diversif us. overal, we ar surpris to see that the algorithm ar still abl to perform reason well when the number of present term is small. for exampl, at onli 12 term cfb3c (the clarif form is of size 3 × 4) can still improv 36.5% over the baselin, drop slightli from 39.3% at 48 term. 6.3 user feedback analysi in thi part we studi sever aspect of user"s term feedback  behavior, and whether thei ar connect to retriev perform. figur 2: clarif form complet time distribut 0−30 30−60 60−90 90−120 120−150 150−180 0 5 10 15 20 25 30 35 complet time (second) #topic 1×48 3×16 6×8 figur 2 show the distribut of time need to complet a clarif form3 . we see that the user is usual abl to finish term feedback within a reason short amount of time: for more than half of the topic the clarif form is complet in just 1 minut, and onli a small fraction of topic (less than 10% for 1 × 48 and 3 × 16) take more than 2 minut. thi suggest that term feedback is suitabl for interact ad-hoc retriev, where a user usual doe not want to spend too much time on provid feedback. we find that a user often make mistak when judg term  relev. sometim a relev term mai be left out becaus it  connect to the queri topic is not obviou to the user. other time a dubiou term mai be includ but turn out to be irrelev. take the topic in figur 1 for exampl. there wa a fire disast in mont 3 the maxim time is 180 second, as the nist assessor would be forc to submit the form at that moment. tabl 4: term select statist (topic averag) cf type 1 × 48 3 × 16 6 × 8 # check term 14.8 13.3 11.2 # rel. term 15.0 12.6 11.2 # rel. check term 7.9 6.9 5.9 precis 0.534 0.519 0.527 recal 0.526 0.548 0.527 blanc tunnel between franc and itali in 1999, but the user fail to select such keyword as mont, blanc, french and italian due to hi/her ignor of the event. inde, without proper  context it would be hard to make perfect judgment. what is then, the extent to which the user is good at term  feedback? doe it have seriou impact on retriev perform? to  answer these question, we need a measur of individu term" true relev. we adopt the simplifi kl diverg metric us in [24] to decid queri expans term as our term relev  measur: σkld(w) = p(w|r) log p(w|r) p(w|¬r) where p(w|r) is the probabl that a relev document contain term w, and p(w|¬r) is the probabl that an irrelev document contain w, both of which can be easili comput via maximum likelihood estim given document-level relev judgment. if σkld(w) > 0, w is more like to appear in relev document than irrelev on. we consid a term relev if it simplifi kl diverg valu is greater than a certain threshold σ0. we can then defin precis and recal of user term judgment accordingli: precis is the fraction of term check by the user that ar relev; recal is the fraction of present relev term that ar check by the user. tabl 4 show the number of check term, relev term and relev check term when σ0 is set to 1.0, as well as the precis/recal of user term judgment. note that when the clarif form contain more cluster, fewer term ar check: 14.8 for 1 × 48, 13.3 for 3 × 16 and 11.2 for 6×8. similar pattern hold for relev term and relev check term. there seem to be a trade-off between increas topic divers by cluster and lose extra relev term: when there ar more cluster, each of them get fewer term to present, which can hurt a major relev cluster that contain mani relev term. therefor, it is not alwai help to have more cluster, e.g., tfb6c is actual wors than tfb1c. the major find we can make from tabl 4 is that the user is not particularli good at identifi relev term, which echo the discoveri in [18]. in the case of 3 × 16 clarif form, the averag number of term check as relev by the user is 13.3 per topic, and the averag number of relev term whose σkld valu exce 1.0 is 12.6. the user is abl to recogn onli 6.9 of these term on averag. inde, the precis and recal of user feedback term (as defin previous) ar far from perfect. on the other hand, if the user had correctli check all such relev term, the perform of our algorithm would have increas a lot, as shown in tabl 5. we see that tfb get big improv when there is an  oracl who check all relev term, while cfb meet a bottleneck around map of 0.325, sinc all it doe is adjust cluster weight, and when the learn weight ar close to be accur, it  cannot benefit more from term feedback. also note that tcfb fail to outperform tfb, probabl becaus tfb is suffici accur. tabl 5: chang of map when us all (and onli) relev term (σkld > 1.0) for feedback. origin term feedback relev term feedback tf1 0.288 0.354 tf3 0.288 0.354 tf6 0.278 0.346 cf3 0.305 0.325 cf6 0.301 0.326 tcf3 0.309 0.345 tcf6 0.304 0.341 6.4 comparison with relev feedback now we compar term feedback with document-level relev feedback, in which the user is present with the top n document from an initi retriev and ask to judg their relev. the feedback process is simul us document relev judgment from nist. we us the mixtur model base feedback method  propos in [25], with mixtur nois set to 0.95 and feedback  coeffici set to 0.9. compar evalu of relev feedback against other  method is complic by the fact that some document have alreadi been view dure feedback, so it make no sens to includ them in the retriev result of the second run. howev, thi doe not hold for term feedback. thu, to make it fair w.r.t. user"s  inform gain, if the feedback document ar relev, thei should be kept in the top of the rank; if thei ar irrelev, thei should be left out. therefor, we us relev feedback to produc a rank of top 1000 retriev document but with everi feedback document exclud, and then prepend the relev feedback document at the front. tabl 6 show the perform of relev feedback for differ valu of n and compar it with tcfb3c. tabl 6: perform of relev feedback for differ  number of feedback document (n). n map pr@30 rr 5 0.302 0.586 4779 10 0.345 0.670 4916 20 0.389 0.772 5004 tcfb3c 0.309 0.491 4947 we see that the perform of tcfb3c is compar to that of relev feedback us 5 document. although it is poorer than when there ar 10 feedback document in term of map and pr@30, it doe retriev more document (4947) when go down the rank list. we try to compar the qualiti of automat insert term in relev feedback with that of manual select term in term feedback. thi is done by truncat the relev feedback  modifi queri model to a size equal to the number of check term for the same topic. we can then compar the term in the truncat model with the check term. figur 3 show the distribut of the term" σkld score. we find that term feedback tend to produc expans term of higher qualiti(those with σkld > 1) compar to relev feedback (with 10 feedback document). thi doe not contradict the fact that the latter yield higher retriev perform. actual, when we us the truncat queri model instead of the intact on refin from relev feedback, the map is onli 0.304. the truth figur 3: comparison of expans term qualiti between  relev feedback (with 10 feedback document) and term  feedback (with 3 × 16 cf) −1−0 0−1 1−2 2−3 3−4 4−5 5−6 0 50 100 150 200 250 300 350 σkld #term relev feedback term feedback is, although there ar mani unwant term in the expand queri model from feedback document, there ar also more relev term than what the user can possibl select from the list of present term gener with pseudo-feedback document, and the posit effect often outweight the neg on. we ar interest to know under what circumst term  feedback ha advantag over relev feedback. on such situat is when none of the top n feedback document is relev, render relev feedback useless. thi is not infrequ, as on might have thought: out of the 50 topic, there ar 13 such case when n = 5, 10 when n = 10, and still 3 when n = 20. when thi happen, on can onli back off to the origin retriev method; the power of relev feedback is lost. surprisingli, in 11 out of 13 such case where relev  feedback seem imposs, the user is abl to check at least 2  relev term from the 3 × 16 clarif form (we consid term t to be relev if σkld(t) > 1.0). furthermor, in 10 out of them tcfb3c outperform the pseudo-feedback baselin,  increas map from 0.076 to 0.146 on averag (these ar particularli hard topic). we think that there ar two possibl explan for thi phenomenon of term feedback be activ even when  relev feedback doe not work: first, even if none of the top n (suppos it is a small number) document ar relev, we mai still find relev document in top 60, which is more inclus but usual unreach when peopl ar do relev feedback in interact ad-hoc search, from which we can draw feedback term. thi is true for topic 367 piraci, where the top 10 feedback  document ar all about softwar piraci, yet there ar document  between 10-60 that ar about piraci on the sea (which is about the real inform need), contribut term such as pirat, ship for select in the clarif form. second, for some topic, a document need to meet some special condit in order to be relev. the top n document mai be relat to the topic, but nonetheless irrelev. in thi case, we mai still extract us term from these document, even if thei do not qualifi as  relev on. for exampl, in topic 639 consum onlin shop, a document need to mention what contribut to shop growth to realli match the specifi inform need, henc none of the top 10 feedback document ar regard as relev. but  nevertheless, the feedback term such as retail, commerc ar good for queri expans. 7. conclus in thi paper we studi the us of term feedback for  interact inform retriev in the languag model approach. we propos a cluster-base method for select present term as well as algorithm to estim refin queri model from user term feedback. we saw signific improv in retriev  accuraci brought by term feedback, in spite of the fact that a user often make mistak in relev judgment that hurt it perform. we found the best-perform algorithm to be tcfb, which  benefit from the combin of directli observ term evid with tfb and indirectli learn cluster relev with cfb. when we reduc the number of present term, term feedback is still abl to keep much of it perform gain over the baselin.  final, we compar term feedback to document-level relev  feedback, and found that tcfb3c"s perform is on a par with the latter with 5 feedback document. we regard term feedback as a viabl altern to tradit relev feedback, especi when there ar no relev document in the top. we propos to extend our work in sever wai. first, we want to studi whether the us of variou context can help the user to better identifi term relev, while not sacrif the simplic and compact of term feedback. second, current all term ar present to the user in a singl batch. we could instead consid  iter term feedback, by present a small number of term first, and show more term after receiv user feedback or stop when the refin queri is good enough. the present term should be select dynam to maxim learn benefit at ani moment. third, we have plan to incorpor term feedback into our ucair toolbar[20], an internet explor plugin, to make it work for web search. we ar also interest in studi how to combin term feedback with relev feedback or implicit feedback. we could, for exampl, allow the user to dynam modifi term in a  languag model learn from feedback document. 8. acknowledg thi work is support in part by the nation scienc  foundat grant ii-0347933 and ii-0428472. 9. refer [1] j. allan. relev feedback with too much data. in proceed of the 18th annual intern acm sigir confer on research and develop in inform retriev, page 337-343, 1995. [2] j. allan. hard track overview in trec 2005 - high accuraci retriev from document. in the fourteenth text retriev confer, 2005. [3] p. anick. us terminolog feedback for web search refin: a log-base studi. in proceed of the 26th annual intern acm sigir confer on research and develop in informaion retriev, page 88-95, 2003. [4] p. g. anick and s. tipirneni. the paraphras search assist: terminolog feedback for iter inform seek. in proceed of the 22nd annual intern acm sigir confer on research and develop in inform retriev, page 153-159, 1999. [5] c. bucklei, g. salton, j. allan, and a. singhal. automat queri expans us smart. in proceed of the third text retriev confer, 1994. [6] d. harman. toward interact queri expans. in proceed of the 11th annual intern acm sigir confer on research and develop in inform retriev, page 321-331, 1988. [7] n. a. jaleel, a. corrada-emmanuel, q. li, x. liu, c. wade, and j. allan. umass at trec 2003: hard and qa. in trec, page 715-725, 2003. [8] h. joho, c. coverson, m. sanderson, and m. beaulieu. hierarch present of expans term. in proceed of the 2002 acm symposium on appli comput, page 645-649, 2002. [9] k. s. jone, s. walker, and s. e. robertson. a probabilist model of inform retriev: develop and statu. technic report 446, comput laboratori, univers of cambridg, 1998. [10] d. kelli, v. d. dollu, and x. fu. the loquaci user: a document-independ sourc of term for queri expans. in proceed of the 28th annual intern acm sigir confer on research and develop in inform retriev, page 457-464, 2005. [11] d. kelli and x. fu. elicit of term relev feedback: an investig of term sourc and context. in proceed of the 29th annual intern acm sigir confer on research and develop in inform retriev, 2006. [12] j. koenemann and n. belkin. a case for interact: a studi of interact inform retriev behavior and effect. in proceed of the sigchi confer on human factor in comput system, page 205-212, 1996. [13] v. lavrenko and w. b. croft. relev-base languag model. in research and develop in inform retriev, page 120-127, 2001. [14] y. nemeth, b. shapira, and m. taeib-maimon. evalu of the real and perceiv valu of automat and interact queri expans. in proceed of the 27th annual intern acm sigir confer on research and develop in inform retriev, page 526-527, 2004. [15] j. pont. a languag model approach to inform retriev. phd thesi, univers of massachusett at amherst, 1998. [16] s. e. robertson, s. walker, s. jone, m. beaulieu, and m. gatford. okapi at trec-3. in proceed of the third text retriev confer, 1994. [17] j. rocchio. relev feedback in inform retriev. in the smart retriev system, page 313-323. 1971. [18] i. ruthven. re-examin the potenti effect of interact queri expans. in proceed of the 26th annual intern acm sigir confer on research and develop in informaion retriev, page 213-220, 2003. [19] g. salton and c. bucklei. improv retriev perform by relev feedback. journal of the american societi for inform scienc, 41:288-297, 1990. [20] x. shen, b. tan, and c. zhai. implicit user model for person search. in proceed of the 14th acm intern confer on inform and knowledg manag, page 824-831, 2005. [21] x. shen and c. zhai. activ feedback in ad-hoc inform retriev. in proceed of the 28th annual intern acm sigir confer on research and develop in inform retriev, page 59-66, 2005. [22] a. spink. term relev feedback and queri expans: relat to design. in proceed of the 17th annual intern acm sigir confer on research and develop in inform retriev, page 81-90, 1994. [23] j. xu and w. b. croft. queri expans us local and global document analysi. in proceed of the 19th annual intern acm sigir confer on research and develop in inform retriev, page 4-11, 1996. [24] h. zaragoza, n. craswel, m. taylor, s. saria, and s. robertson. microsoft cambridg at trec-13: web and hard track. in proceed of the 13th text retriev confer, 2004. [25] c. zhai and j. lafferti. model-base feedback in the languag model approach to inform retriev. in proceed of the tenth intern confer on inform and knowledg manag, page 403-410, 2001. [26] c. zhai, a. velivelli, and b. yu. a cross-collect mixtur model for compar text mine. in proceed of the tenth acm sigkdd intern confer on knowledg discoveri and data mine, page 743-748, 2004. 