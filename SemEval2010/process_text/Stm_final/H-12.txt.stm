fast gener of result snippet in web search andrew turpin & yohann tsegai rmit univers melbourn, australia aht@cs.rmit.edu.au ytsegai@cs.rmit.edu.au david hawk csiro ict centr canberra, australia david.hawk@acm.org hugh e. william microsoft corpor on microsoft wai redmond, wa. hughw@microsoft.com abstract the present of queri bias document snippet as part of result page present by search engin ha becom an expect of search engin user. in thi paper we  explor the algorithm and data structur requir as part of a search engin to allow effici gener of queri bias snippet. we begin by propos and analys a document compress method that reduc snippet gener time by 58% over a baselin us the zlib compress librari. these experi reveal that find document on  secondari storag domin the total cost of gener  snippet, and so cach document in ram is essenti for a fast snippet gener process. us simul, we  examin snippet gener perform for differ size ram cach. final we propos and analys document  reorder and compact, reveal a scheme that increas the number of document cach hit with onli a margin  affect on snippet qualiti. thi scheme effect doubl the number of document that can fit in a fix size cach. categori and subject descriptor h.3.3 [inform storag and retriev]: inform search and retriev; h.3.4 [inform storag and retriev]: system and softwar-perform evalu (effici and effect); gener term algorithm, experiment, measur, perform 1. introduct each result in search result list deliv by current www search engin such as search.yahoo.com, googl.com and search.msn.com typic contain the titl and url of the actual document, link to live and cach version of the document and sometim an indic of file size and type. in addit, on or more snippet ar usual present,  give the searcher a sneak preview of the document content. snippet ar short fragment of text extract from the document content (or it metadata). thei mai be static (for exampl, alwai show the first 50 word of the  document, or the content of it descript metadata, or a  descript taken from a directori site such as dmoz.org) or queri-bias [20]. a queri-bias snippet is on select extract on the basi of it relat to the searcher"s queri. the addit of inform snippet to search result mai substanti increas their valu to searcher. accur snippet allow the searcher to make good decis about which result ar worth access and which can be ignor. in the best case, snippet mai obviat the need to open ani document by directli provid the answer to the searcher"s real inform need, such as the contact detail of a person or an organ. gener of queri-bias snippet by web search  engin index of the order of ten billion web page and  handl hundr of million of search queri per dai impos a veri signific comput load (rememb that each search typic gener ten snippet). the  simplemind approach of keep a copi of each document in a file and gener snippet by open and scan file, work when queri rate ar low and collect ar small, but doe not scale to the degre requir. the overhead of open and read ten file per queri on top of  access the index structur to locat them, would be manifestli excess under heavi queri load. even store ten billion file and the correspond hundr of terabyt of data is beyond the reach of tradit filesystem. special-purpos filesystem have been built to address these problem [6]. note that the util of snippet is by no mean restrict to whole-of-web search applic. effici gener of snippet is also import at the scale of whole-of-govern search servic such as www.firstgov.gov (c. 25 million page) and govsearch.australia.gov.au (c. 5 million page) and within larg enterpris such as ibm [2] (c. 50 million page). snippet mai be even more us in databas or filesystem search applic in which no us url or titl inform is present. we present a new algorithm and compact singl-file  structur design for rapid gener of high qualiti snippet and compar it space/time perform against an obviou baselin base on the zlib compressor on variou data set. we report the proport of time spent for disk seek, disk read and cpu process; demonstr that the time for locat each document (seek time) domin, as expect. as the time to process a document in ram is small in comparison to locat and read the document into  memori, it mai seem that compress is not requir. howev, thi is onli true if there is no cach of document in ram. control the ram of physic system for  experiment is difficult, henc we us simul to show that cach document dramat improv the perform of  snippet gener. in turn, the more document can be  compress, the more can fit in cach, and henc the more disk seek can be avoid: the classic data compress tradeoff that is exploit in invert file structur and comput rank document list [24]. as hit the document cach is import, we examin document compact, as oppos to compress, scheme by impos an a priori order of sentenc within a  document, and then onli allow lead sentenc into cach for each document. thi lead to further time save, with onli margin impact on the qualiti of the snippet return. 2. relat work snippet gener is a special type of extract  document summar, in which sentenc, or sentenc  fragment, ar select for inclus in the summari on the basi of the degre to which thei match the search queri. thi process wa given the name of queri-bias summar by tombro and sanderson [20] the reader is refer to mani [13] and to radev et al. [16] for overview of the veri mani differ applic of summar and for the equal divers method for produc summari. earli web search engin present queri-independ snippet consist of the first k byte of the result  document. gener these is clearli much simpler and much less computation expens than process document to extract queri bias summari, as there is no need to search the document for text fragment contain queri term. to our knowledg, googl wa the first  whole-ofweb search engin to provid queri bias summari, but summar is list by brin and page [1] onli under the head of futur work. most of the experiment work us queri-bias  summar ha focus on compar their valu to searcher rel to other type of summari [20, 21], rather than  effici gener of summari. despit the import of effici summari gener in web search, few algorithm appear in the literatur. silber and mckoi [19] describ a linear-time lexic chain algorithm for us in gener  summari, but offer no empir data for the perform of their algorithm. white et al [21] report some experiment time of their webdocsum system, but the snippet  gener algorithm themselv ar not isol, so it is difficult to infer snippet gener time compar to the time we report in thi paper. 3. search engin architectur a search engin must perform a varieti of activ, and is compris of mani sub-system, as depict by the box in figur 1. note that there mai be sever other sub-system such as the advertis engin or the pars engin that could easili be ad to the diagram, but we have  concentr on the sub-system that ar relev to snippet gener. depend on the number of document that the search engin index, the data and process for each rank engin crawl engin index engin engin lexicon meta data engin engin snippet term&doc#s snippetperdoc web queri engin queri result page term#s doc#s invertedlist doc perdoc titl,url,etc doc#s document meta data term querystr term#s figur 1: an abstract of some of the sub-system in a search engin. depend on the number of document index, each sub-system could resid on a singl machin, be distribut across thousand of machin, or a combin of both. sub-system could be distribut over mani machin, or all occupi a singl server and filesystem, compet with each other for resourc. similarli, it mai be more effici to combin some sub-system in an implement of the  diagram. for exampl, the meta-data such as document titl and url requir minim comput apart from  highlight queri word, but we note that disk seek is like to be minim if titl, url and fix summari  inform is store contigu with the text from which queri bias summari ar extract. here we ignor the fix text and consid onli the gener of queri bias  summari: we concentr on the snippet engin. in addit to data and program oper on that data, each sub-system also ha it own memori manag scheme. the memori manag system mai simpli be the  memori hierarchi provid by the oper system us on  machin in the sub-system, or it mai be explicitli code to optimis the process in the sub-system. there ar mani paper on cach in search engin (see [3] and refer therein for a current summari), but it seem reason that there is a queri cach in the queri engin that store precomput final result page for veri popular queri. when on of the popular queri is issu, the result page is fetch straight from the queri cach. if the issu queri is not in the queri cach, then the queri engin us the four sub-system in turn to assembl a  result page. 1. the lexicon engin map queri term to integ. 2. the rank engin retriev invert list for each term, us them to get a rank list of document. 3. the snippet engin us those document number and queri term number to gener snippet. 4. the meta data engin fetch other inform about each document to construct the result page. in a document broken into on sentenc per line, and a sequenc of queri term. 1 for each line of the text, l = [w1, w2, . . . , wm] 2 let h be 1 if l is a head, 0 otherwis. 3 let be 2 if l is the first line of a document, 1 if it is the second line, 0 otherwis. 4 let c be the number of wi that ar queri term, count repetit. 5 let d be the number of distinct queri term that match some wi. 6 identifi the longest contigu run of queri term in l, sai wj . . . wj+k. 7 us a weight combin of c, d, k, h and to deriv a score s. 8 insert l into a max-heap us s as the kei. out remov the number of sentenc requir from the heap to form the summari. figur 2: simpl sentenc ranker that oper on raw text with on sentenc per line. 4. the snippet engin for each document identifi pass to the snippet  engin, the engin must gener text, prefer contain queri term, that attempt to summar that document. previou work on summar identifi the sentenc as the minim unit for extract and present to the user [12]. accordingli, we also assum a web snippet  extract process will extract sentenc from document. in order to construct a snippet, all sentenc in a document should be rank against the queri, and the top two or three return as the snippet. the score of sentenc against queri ha been explor in sever paper [7, 12, 18, 20, 21], with  differ featur of sentenc deem import. base on these observ, figur 2, show the gener algorithm for score sentenc in relev document, with the highest score sentenc make the snippet for each document. the final score of a sentenc, assign in step 7, can be deriv in mani differ wai. in order to avoid bia toward ani particular score mechan, we compar sentenc qualiti later in the paper us the individu  compon of the score, rather than an arbitrari combin of the compon. 4.1 pars web document unlik well edit text collect that ar often the target for summar system, web data is often poorli  structur, poorli punctuat, and contain a lot of data that do not form part of valid sentenc that would be candid for part of snippet. we assum that the document pass to the snippet engin by the index engin have all html tag and javascript remov, and that each document is reduc to a seri of word token separ by non-word token. we  defin a word token as a sequenc of alphanumer charact, while a non-word is a sequenc of non-alphanumer  charact such as whitespac and the other punctuat symbol. both ar limit to a maximum of 50 charact. adjac, repeat charact ar remov from the punctuat. includ in the punctuat set is a special end of sentenc marker which replac the usual three sentenc termin ?!.. often these explicit punctuat charact ar  miss, and so html tag such as <br> and <p> ar assum to termin sentenc. in addit, a sentenc must contain at least five word and no more than twenti word, with longer or shorter sentenc be broken and join as requir to meet these criteria [10]. untermin html tag-that is, tag with an open brace, but no close brace-caus all text from the open brace to the next open brace to be discard. a major problem in summar web page is the  presenc of larg amount of promot and navig  materi (navbar) visual abov and to the left of the actual page content. for exampl, the most wonder compani on earth. product. servic. about us. contact us. try befor you bui. similar, but often not ident,  navig materi is typic present on everi page within a site. thi materi tend to lower the qualiti of summari and slow down summari gener. in our experi we did not us ani particular  heurist for remov navig inform as the test  collect in us (wt100g) pre-date the widespread take up of the current style of web publish. in wt100g, the averag web page size is more than half the current web averag [11]. anecdot, the increas is due to inclus of sophist navig and interfac element and the javascript  function to support them. have defin the format of document that ar  present to the snippet engin, we now defin our compress token system (ct) document storag scheme, and the baselin system us for comparison. 4.2 baselin snippet engin an obviou document represent scheme is to simpli compress each document with a well known adapt  compressor, and then decompress the document as requir [1], us a string match algorithm to effect the algorithm in figur 2. accordingli, we implement such a system, us zlib [4] with default paramet to compress everi document after it ha been pars as in section 4.1. each document is store in a singl file. while  manag for our small test collect or small enterpris with million of document, a full web search engin mai requir multipl document to inhabit singl file, or a special  purpos filesystem [6]. for snippet gener, the requir document ar  decompress on at a time, and a linear search for provid queri term is emploi. the search is optim for our specif task, which is restrict to match whole word and the sentenc termin token, rather than gener  pattern match. 4.3 the ct snippet engin sever optim over the baselin ar possibl. the first is to emploi a semi-static compress method over the entir document collect, which will allow faster  decompress with minim compress loss [24]. us a  semistat approach involv map word and non-word  produc by the parser to singl integ token, with frequent symbol receiv small integ, and then choos a code scheme that assign small number a small number of bit. word and non-word strictli altern in the compress file, which alwai begin with a word. in thi instanc we simpli assign each symbol it ordin number in a list of symbol sort by frequenc. we us the vbyte code scheme to code the word token [22]. the set of non-word is limit to the 64 most common punctuat sequenc in the collect itself, and ar encod with a flat 6-bit binari code. the remain 2 bit of each punctuat symbol ar us to store capit inform. the process of comput the semi-static model is  complic by the fact that the number of word and non-word appear in larg web collect is high. if we store all word and non-word appear in the collect, and their associ frequenc, mani gigabyt of ram or a b-tree or similar on-disk structur would be requir [23]. moffat et al. [14] have examin scheme for prune model dure compress us larg alphabet, and conclud that rare occur term need not resid in the model. rather, rare term ar spelt out in the final compress file, us a  special word token (escap symbol), to signal their occurr. dure the first pass of encod, two move-to-front queue ar kept; on for word and on for non-word. whenev the avail memori is consum and a new symbol is  discov in the collect, an exist symbol is discard from the end of the queue. in our implement, we  enforc the stricter condit on evict that, where possibl, the evict symbol should have a frequenc of on. if there is no symbol with frequenc on in the last half of the queue, then we evict symbol of frequenc two, and so on until enough space is avail in the model for the new symbol. the second pass of encod replac each word with it vbyte encod number, or the escap symbol and an ascii represent of the word if it is not in the model.  similarli each non-word sequenc is replac with it codeword, or the codeword for a singl space charact if it is not in the model. we note that thi lossless compress of non-word is accept when the document ar us for snippet  gener, but mai not be accept for a document databas. we assum that a separ sub-system would hold cach document for other purpos where exact punctuat is import. while thi semi-static scheme should allow faster  decompress than the baselin, it also readili allow direct  match of queri term as compress integ in the compress file. that is, sentenc can be score without have to  decompress a document, and onli the sentenc return as part of a snippet need to be decod. the ct system store all document contigu in on file, and an auxiliari tabl of 64 bit integ indic the start offset of each document in the file. further, it must have access to the revers map of term number,  allow those word not spelt out in the document to be  recov and return to the queri engin as string. the first of these data structur can be readili partit and  distribut if the snippet engin occupi multipl machin; the second, howev, is not so easili partit, as ani document on a remot machin might requir access to the whole integ-to-string map. thi is the second reason for emploi the model prune step dure construct of the semi-static code: it limit the size of the revers map tabl that should be present on everi machin implement the snippet engin. 4.4 experiment assess of ct all experi report in thi paper were run on a sun fire v210 server run solari 10. the machin consist of dual 1.34 ghz ultrasparc iiii processor and 4gb of wt10g wt50g wt100g no. doc. (×106 ) 1.7 10.1 18.5 raw text 10, 522 56, 684 102, 833 baselin(zlib) 2, 568 (24%) 10, 940 (19%) 19, 252 (19%) ct 2, 722 (26%) 12, 010 (21%) 22, 269 (22%) tabl 1: total storag space (mb) for document for the three test collect both compress, and uncompress. 0 20 40 60 0.00.20.40.60.8 queri group in 100"s time(second) 0 20 40 60 0.00.20.40.60.8 queri group in 100"s time(second) 0 20 40 60 0.00.20.40.60.8 queri group in 100"s time(second) baselin ct with cach ct without cach figur 3: time to gener snippet for 10  document per queri, averag over bucket of 100 queri, for the first 7000 excit queri on wt10g. ram. all sourc code wa compil us gcc4.1.1 with -o9 optimis. time were run on an otherwis  unoccupi machin and were averag over 10 run, with memori flush between run to elimin ani cach of data file. in the absenc of evid to the contrari, we assum that it is import to model realist queri arriv sequenc and the distribut of queri repetit for our experi. consequ, test collect which lack real queri log, such as trec ad-hoc and .gov2 were not consid  suitabl. obtain extens queri log and associ result doc-id for a correspond larg collect is not easi. we have us two collect (wt10g and wt100g) from the trec web track [8] coupl with queri from excit log from the same (c. 1997) period. further, we also made us of a medium size collect wt50g, obtain by randomli sampl half of the document from wt100g. the first two row of tabl 1 give the number of document and the size in mb of these collect. the final two row of tabl 1 show the size of the result document set after compress with the baselin and ct scheme. as expect, ct admit a small compress loss over zlib, but both substanti reduc the size of the text to about 20% of the origin, uncompress size. note that the figur for ct do not includ the revers map from integ token to string that is requir to produc the final snippet as that occupi ram. it is 1024 mb in these experi. the zettair search engin [25] wa us to produc a list of document to summar for each queri. for the major of the experi the okapi bm25 score scheme wa us to determin document rank. for the static cach  experi report in section 5, the score of each document wt10g wt50g wt100g baselin 75 157 183 ct 38 70 77 reduct in time 49% 56% 58% tabl 2: averag time (msec) for the final 7000 queri in the excit log us the baselin and ct system on the 3 test collect. is a 50:50 weight averag of the bm25 score (normal by the top score document for each queri) and a score for each document independ of ani queri. thi is to  simul effect of rank algorithm like pagerank [1] on the distribut of document request to the snippet engin. in our case we us the normal access count [5] comput from the top 20 document return to the first 1 million queri from the excit log to determin the queri  independ score compon. point on figur 3 indic the mean run time to gener 10 snippet for each queri, averag in group of 100 queri, for the first 7000 queri in the excit queri log. onli the data for wt10g is shown, but the other  collect show similar pattern. the x-axi indic the group of 100 queri; for exampl, 20 indic the queri 2001 to 2100. clearli there is a cach effect, with time drop substanti after the first 1000 or so queri ar process. all of thi is due to the oper system cach disk block and perhap pre-fetch data ahead of specif read request. thi is evid becaus the baselin system ha no larg intern data structur to take advantag of non-disk base cach, it simpli open and process file, and the speed up is evid for the baselin system. part of thi gain is due to the spatial local of disk  refer gener by the queri stream: repeat queri will alreadi have their document file cach in memori; and similarli differ queri that return the same  document will benefit from document cach. but when the log is process after remov all but the first request for each document, the pronounc speed-up is still evid as more queri ar process (not shown in figur). thi  suggest that the oper system (or the disk itself) is read and buffer a larger amount of data than the amount  request and that thi bring benefit often enough to make an appreci differ in snippet gener time. thi is confirm by the curv label ct without cach, which wa gener after mount the filesystem with a no-cach option (directio in solari). with disk cach turn off, the averag time to gener snippet vari littl. the time to gener ten snippet for a queri, averag over the final 7000 queri in the excit log as cach effect have dissip, ar shown in tabl 2. onc the system ha stabil, ct is over 50% faster than the baselin  system. thi is primarili due to ct match singl integ for most queri word, rather than compar string in the baselin system. tabl 3 show a break down of the averag time to  gener ten snippet over the final 7000 queri of the  excit log on the wt50g collect when entir document ar process, and when onli the first half of each document is process. as can be seen, the major of time spent gener a snippet is in locat the document on disk (seek): 64% for whole document, and 75% for half  document. even if the amount of process a document must % of doc process seek read score & decod 100% 45 4 21 50% 45 4 11 tabl 3: time to gener 10 snippet for a singl queri (msec) for the wt50g collect averag over the final 7000 excit queri when either all of each document is process (100%) or just the first half of each document (50%). undergo is halv, as in the second row of the tabl, there is onli a 14% reduct in the total time requir to gener a snippet. as locat document in secondari storag  occupi such a larg proport of snippet gener time, it seem logic to try and reduc it impact through cach. 5. document cach in section 3 we observ that the snippet engin would have it own ram in proport to the size of the  document collect. for exampl, on a whole-of-web search engin, the snippet engin would be distribut over mani workstat, each with at least 4 gb of ram. in a small enterpris, the snippet engin mai be share ram with all other sub-system on a singl workstat, henc onli have 100 mb avail. in thi section we us simul to measur the number of cach hit in the snippet engin as memori size vari. we compar two cach polici: a static cach, where the cach is load with as mani document as it can hold befor the system begin answer queri, and then never chang; and a least-recent-us cach, which start out as for the static cach, but whenev a document is access it move to the front of a queue, and if a document is fetch from disk, the last item in the queue is evict. note that document ar first load into the cach in order of  decreas queri independ score, which is comput as  describ in section 4.4. the simul also assum a queri cach exist for the top q most frequent queri, and that these queri ar never process by the snippet engin. all queri pass into the simul ar from the second half of the excit queri log (the first half be us to  comput queri independ score), and ar stem, stop, and have their term sort alphabet. thi final  alter simpli allow queri such as red dog and dog red to return the same document, as would be the case in a search engin where explicit phrase oper would be requir in the queri to enforc term order and proxim. figur 4 show the percentag of document access that hit cach us the two cach scheme, with q either 0 or 10,000, on 535,276 excit queri on wt100g. the  xaxi show the percentag of document that ar held in the cach, so 1.0% correspond to about 185,000 document. from thi figur it is clear that cach even a small  percentag of the document ha a larg impact on reduc seek time for snippet gener. with 1% of document cach, about 222 mb for the wt100g collect, around 80% of disk seek ar avoid. the static cach perform surprisingli well (squar in figur 4), but is outperform by the lru cach (circl). in an actual implement of lru, howev, there mai be fragment of the cach as document ar swap in and out. the reason for the larg impact of the document cach is 0.0 0.5 1.0 1.5 2.0 2.5 3.0 020406080100 cach size (% of collect) %ofaccessesascachehit lru q=0 lru q=10,000 static q=0 static q=10,000 figur 4: percentag of the time that the snippet engin doe not have to go to disk in order to  gener a snippet plot against the size of the  document cach as a percentag of all document in the collect. result ar from a simul on wt100g with 535,276 excit queri. that, for a particular collect, some document ar much more like to appear in result list than other. thi effect occur partli becaus of the approxim zipfian queri frequenc distribut, and partli becaus most web search engin emploi rank method which combin queri base score with static (a priori) score determin from factor such as link graph measur, url featur, spam score and so on [17]. document with high static score ar much more like to be retriev than other. in addit to the document cach, the ram of the  snippet engin must also hold the ct decod tabl that map integ to string, which is cap by a paramet at compress time (1 gb in our experi here). thi is more than compens for by the reduc size of each  document, allow more document into the document cach. for exampl, us ct reduc the averag document size from 5.7 kb to 1.2 kb (as shown in tabl 1), so a 2 gb ram could hold 368,442 uncompress document (2% of the  collect), or 850,691 document plu a 1 gb decompress tabl (5% of the collect). in fact, further experiment with the model size  reveal that the model can in fact be veri small and still ct give good compress and fast score time. thi is  evidenc in figur 5, where the compress size of wt50g is shown in the solid symbol. note that when no compress is us (model size is 0mb), the collect is onli 31 gb as html markup, javascript, and repeat punctuat ha been discard as describ in section 4.1. with a 5 mb model, the collect size drop by more than half to 14 gb, and increas the model size to 750 mb onli elicit a 2 gb drop in the collect size. figur 5 also show the averag time to score and decod a a snippet (exclud seek time) with the differ model size (open symbol). again, there is a larg speed up when a 5 mb model is us, but littl 0 200 400 600 15202530 model size (mb) collections(gb)ortim(msec) size (gb) time (msec) figur 5: collect size of the wt50g collect when compress with ct us differ memori limit on the model, and the averag time to  gener singl snippet exclud seek time on 20000 excit queri us those model. improv with larger model. similar result hold for the wt100g collect, where a model of about 10 mb  offer substanti space and time save over no model at all, but return diminish as the model size increas. apart from compress, there is anoth approach to  reduc the size of each document in the cach: do not store the full document in cach. rather store sentenc that ar like to be us in snippet in the cach, and if dure  snippet gener on a cach document the sentenc score do not reach a certain threshold, then retriev the whole  document from disk. thi rais question on how to choos sentenc from document to put in cach, and which to leav on disk, which we address in the next section. 6. sentenc reorder sentenc within each document can be re-order so that sentenc that ar veri like to appear in snippet ar at the front of the document, henc process first at queri time, while less like sentenc ar releg to the rear of the document. then, dure queri time, if k sentenc with a score exceed some threshold ar found befor the entir document is process, the remaind of the document is ignor. further, to improv cach, onli the head of each document can be store in the cach, with the tail resid on disk. note that we assum that the search engin is to provid cach copi of a document-that is, the exact text of the document as it wa index-then thi would be servic by anoth sub-system in figur 1, and not from the alter copi we store in the snippet engin. we now introduc four sentenc reorder approach. 1. natur order the first few sentenc of a well author document usual best describ the document content [12]. thu simpli process a document in order should yield a qualiti snippet. unfortun, howev, web document ar often not well author, with littl editori or profession write skill brought to bear on the creation of a work of literari merit. more importantli, perhap, is that we ar produc queri-bias snippet, and there is no guarante that queri term will appear in sentenc toward the front of a document. 2. signific term (st) luhn introduc the concept of a signific sentenc as contain a cluster of  signific term [12], a concept found to work well by tombro and sanderson [20]. let fd,t be the frequenc of term t in document d, then term t is determin to be signific if fd,t ≥ 8 < : 7 − 0.1 × (25 − sd), if sd < 25 7, if 25 ≤ sd ≤ 40 7 + 0.1 × (sd − 40), otherwis, where sd is the number of sentenc in document d. a  bracket section is defin as a group of term where the leftmost and rightmost term ar signific term, and no signific term in the bracket section ar divid by more than four non-signific term. the score of a bracket section is the squar of the number of signific word fall in the section, divid by the total number of word in the entir sentenc. the a priori score for a sentenc is comput as the maximum of all score for the bracket section of the sentenc. we then sort the sentenc by thi score. 3. queri log base (qlt) mani web queri repeat, and a small number of queri make up a larg volum of total search [9]. in order to take advantag of thi bia, sentenc that contain mani past queri term should be promot to the front of a document, while sentenc that contain few queri term should be demot. in thi scheme, the sentenc ar sort by the number of sentenc term that occur in the queri log. to ensur that long sentenc do not domin over shorter qualit sentenc the score  assign to each sentenc is divid by the number of term in that sentenc give each sentenc a score between 0 and 1. 4. queri log base (qlu) thi scheme is as for qlt, but repeat term in the sentenc ar onli count onc. by re-order sentenc us scheme st, qlt or qlu, we aim to termin snippet gener earlier than if  natur order is us, but still produc sentenc with the same number of uniqu queri term (d in figur 2), total number of queri term (c), the same posit score (h+ ) and the same maximum span (k). accordingli, we conduct  experi compar the method, the first 80% of the excit queri log wa us to reorder sentenc when requir, and the final 20% for test. figur 6 show the differ in snippet score  compon us each of the three method over the natur order method. it is clear that sort sentenc us the signific term (st) method lead to the smallest chang in the sentenc score compon. the greatest chang over all method is in the sentenc posit (h + )  compon of the score, which is to be expect as their is no guarante that lead and head sentenc ar process at all after sentenc ar re-order. the second most  affect compon is the number of distinct queri term in a return sentenc, but if onli the first 50% of the document is process with the st method, there is a drop of onli 8% in the number of distinct queri term found in snippet. depend how these variou compon ar weight to comput an overal snippet score, on can argu that there is littl overal affect on score when process onli half the document us the st method. span (k) term count (c) sentenc posit (h + l) distinct term (d) 40% 50% 60% 70% st qlt qlu st qlt qlu st qlt qlu st qlt qlu st qlt qlu relativedifferencetonaturalord document size us 90% 80% 70% 60% 50% 0% 10% 20% 30% figur 6: rel differ in the snippet score compon compar to natur order  document when the amount of document process is reduc, and the sentenc in the document ar  reorder us queri log (qlt, qlu) or signific term (st). 7. discuss in thi paper we have describ the algorithm and  compress scheme that would make a good snippet engin sub-system for gener text snippet of the type shown on the result page of well known web search engin. our  experi not onli show that our scheme is over 50% faster than the obviou baselin, but also reveal some veri  import aspect of the snippet gener problem. primarili, cach document avoid seek cost to secondari memori for each document that is to be summar, and is vital for fast snippet gener. our cach simul show that if as littl as 1% of the document can be cach in ram as part of the snippet engin, possibl distribut over mani machin, then around 75% of seek can be avoid. our second major result is that keep onli half of each  document in ram, effect doubl the cach size, ha littl affect on the qualiti of the final snippet gener from those half-document, provid that the sentenc that ar kept in memori ar chosen us the signific term  algorithm of luhn [12]. both our document compress and compact scheme dramat reduc the time taken to gener snippet. note that these result ar gener us a 100gb  subset of the web, and the excit queri log gather from the same period as that subset wa creat. we ar assum, as there is no evid to the contrari, that thi collect and log is repres of search engin input in other domain. in particular, we can scale our result to examin what  resourc would be requir, us our scheme, to provid a snippet engin for the entir world wide web. we will assum that the snippet engin is distribut across m machin, and that there ar n web page in the collect to be index and serv by the search engin. we also assum a balanc load for each machin, so each  machin serv about n/m document, which is easili achiev in practic. each machin, therefor, requir ram to hold the follow. • the ct model, which should be 1/1000 of the size of the uncompress collect (us result in  figur 5 and william et al. [23]). assum an averag uncompress document size of 8 kb [11], thi would requir n/m × 8.192 byte of memori. • a cach of 1% of all n/m document. each document requir 2 kb when compress with ct (tabl 1), and onli half of each document is requir us st sentenc reorder, requir a total of n/m ×0.01× 1024 byte. • the offset arrai that give the start posit of each document in the singl, compress file: 8 byte per n/m document. the total amount of ram requir by a singl machin, therefor, would be n/m(8.192 + 10.24 + 8) byte.  assum that each machin ha 8 gb of ram, and that there ar 20 billion page to index on the web, a total of m = 62  machin would be requir for the snippet engin. of cours in practic, more machin mai be requir to manag the distribut system, to provid backup servic for fail machin, and other network servic. these machin would also need access to 37 tb of disk to store the  compress document represent that were not in cach. in thi work we have deliber avoid commit to on particular score method for sentenc in document. rather, we have report accuraci result in term of the four compon that have been previous shown to be import in determin us snippet [20]. the ct method can incorpor ani new metric that mai aris in the futur that ar calcul on whole word. the  document compact techniqu us sentenc re-order, howev, remov the spatial relationship between sentenc, and so if a score techniqu reli on the posit of a  sentenc within a document, the aggress compact  techniqu report here cannot be us. a variat on the semi-static compress approach we have adopt in thi work ha been us successfulli in  previou search engin design [24], but there ar altern  compress scheme that allow direct match in compress text (see navarro and m¨akinen [15] for a recent survei.) as seek time domin the snippet gener process, we have not focus on thi portion of the snippet gener in  detail in thi paper. we will explor altern compress scheme in futur work. acknowledg thi work wa support in part by arc discoveri project dp0558916 (at). thank to nick lester and justin zobel for valuabl discuss. 8. refer [1] s. brin and l. page. the anatomi of a larg-scale hypertextu web search engin. in www7, page 107-117, 1998. [2] r. fagin, ravi k., k. s. mccurlei, j. novak, d. sivakumar, j. a. tomlin, and d. p. williamson. search the workplac web. in www2003, budapest, hungari, mai 2003. [3] t. fagni, r. perego, f. silvestri, and s. orlando. boost the perform of web search engin: cach and prefetch queri result by exploit histor usag data. acm tran. inf. syst., 24(1):51-78, 2006. [4] j-l gailli and m. adler. zlib compress librari. www.zlib.net. access januari 2007. [5] s. garcia, h.e. william, and a. cannan. access-order index. in v. estivil-castro, editor, proc. australasian comput scienc confer, page 7-14, dunedin, new zealand, 2004. [6] s. ghemawat, h. gobioff, and s. leung. the googl file system. in sosp "03: proc. of the 19th acm symposium on oper system principl, page 29-43, new york, ny, usa, 2003. acm press. [7] j. goldstein, m. kantrowitz, v. mittal, and j. carbonel. summar text document: sentenc select and evalu metric. in sigir99, page 121-128, 1999. [8] d. hawk, nick c., and paul thistlewait. overview of trec-7 veri larg collect track. in proc. of trec-7, page 91-104, novemb 1998. [9] b. j. jansen, a. spink, and j. pedersen. a tempor comparison of altavista web search. j. am. soc. inf. sci. tech. (jasist), 56(6):559-570, april 2005. [10] j. kupiec, j. pedersen, and f. chen. a trainabl document summar. in sigir95, page 68-73, 1995. [11] s. lawrenc and c.l. gile. access of inform on the web. natur, 400:107-109, juli 1999. [12] h.p. luhn. the automat creation of literatur abstract. ibm journal, page 159-165, april 1958. [13] i. mani. automat summar, volum 3 of natur languag process. john benjamin publish compani, amsterdam/philadelphia, 2001. [14] a. moffat, j. zobel, and n. sharman. text compress for dynam document databas. knowledg and data engin, 9(2):302-313, 1997. [15] g. navarro and v. m¨akinen. compress full text index. acm comput survei, 2007. to appear. [16] d. r. radev, e. hovi, and k. mckeown. introduct to the special issu on summar. comput. linguist., 28(4):399-408, 2002. [17] m. richardson, a. prakash, and e. brill. beyond pagerank: machin learn for static rank. in www06, page 707-715, 2006. [18] t. sakai and k. sparck-jone. gener summari for index in inform retriev. in sigir01, page 190-198, 2001. [19] h. g. silber and k. f. mccoi. effici comput lexic chain as an intermedi represent for automat text summar. comput. linguist., 28(4):487-496, 2002. [20] a. tombro and m. sanderson. advantag of queri bias summari in inform retriev. in sigir98, page 2-10, melbourn, aust., august 1998. [21] r. w. white, i. ruthven, and j. m. jose. find relev document us top rank sentenc: an evalu of two altern scheme. in sigir02, page 57-64, 2002. [22] h. e. william and j. zobel. compress integ for fast file access. comp. j., 42(3):193-201, 1999. [23] h.e. william and j. zobel. searchabl word on the web. intern journal on digit librari, 5(2):99-105, april 2005. [24] i. h. witten, a. moffat, and t. c. bell. manag gigabyt: compress and index document and imag. morgan kaufmann publish, san francisco, second edit, mai 1999. [25] the zettair search engin. www.seg.rmit.edu.au/zettair. access januari 2007. 